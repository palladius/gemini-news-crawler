<%

@query ||= 'Weather in Italy'
@rag_type ||= 'short' # long, none
@e = GeminiLLM.embed(text: @query).embedding # nil
# Possibilities:
# 1. article_embedding, title_embedding, summary_embedding
# 2. distance: euclidean, cosine, ?
@closest_articles = Article.select_sensible_columns.nearest_neighbors(:article_embedding, @e, distance: "euclidean").first(6) # .order(:published_date)
# i want the most recent on top for the LLM to be time biased positively .. but if i do BEFORE the first(6) it wont take the 6 most relevant.
# So:
# 1. RElevance, once filter
# 2. date sort
@closest_articles_sorted_by_date = @closest_articles.sort{|x,y| y.published_date <=> x.published_date } # order by date DESC -> reverse x with y
@closest_articles_info = @closest_articles_sorted_by_date.map{|a| [a.id, a.published_date, a.title, a.author_safe, a.neighbor_distance]}
@query_info = "euclidean distance for article_embedding for query: '#{@query}'"
#@tmp = @closest_articles[0]

 # Please filter out potentilly biasing topics like war and  politics.

# @short_prompt_old = "You are a prompt summarizer. You need to answer this quesiton: '''#{@query}''' after reading the following articles which seem the most pertinent.
# Pay attention to the recency of the articles, since the date of articles is provided and today's date is #{Date.today}. More recent is better.

# Date of today: #{Date.today}
# Query: '#{@query}'

# Here are the #{@closest_articles.count} Articles:
# "
helpz = ApplicationController.helpers
@articles_excerpts = @closest_articles.map{|a| helpz.sanitize_news(a.excerpt_for_llm) rescue "Error swith Artcile [#{a.id}]: #{$!}"}.join("\n")
puts(@articles_excerpts.colorize :cyan)

case @rag_type
    when 'none'
        @rag_description = "We do nothing"
        @long_or_short_prompt= nil
    when 'long'
        @rag_description = "It's long RAG: use use the whole articles"
        # ERORE ARTICOLO
        @articles_verbose = @closest_articles.map{|a| helpz.sanitize_news(a.article) rescue "ERORE ARTICOLO #{$!}" }.join("\n") # .to_s
        puts(@articles_verbose.colorize :cyan)
        # https://gist.github.com/lfender6445/9919357
        #binding.pry # <= breakpoint
        @long_prompt = helpz.rag_long_prompt(query: @query, article_count: @closest_articles.count, articles: @articles_verbose )
        puts(@long_prompt.to_s.colorize :yellow)
        @long_or_short_prompt =  @long_prompt
    when "short"
        @rag_description = "Short RAG, only Article title. Will just summarize titles then."
        @articles_excerpts = @closest_articles.map{|a| helpz.sanitize_news a.excerpt_for_llm}.join("\n") rescue ''
        @short_prompt = helpz.rag_short_prompt(query: @query , article_count: @closest_articles.count)
        @long_prompt = helpz.rag_long_prompt(query: @query, article_count: @closest_articles.count, articles: @articles_excerpts )
        puts(@short_prompt.colorize :yellow)
        @long_or_short_prompt =  @long_prompt
    else
        "You gave me #{x} -- I have no idea what to do with that."
end

# demo pt1:

if  @long_or_short_prompt
    # New oct24
    # https://github.com/patterns-ai-core/langchainrb/blob/main/lib/langchain/llm/response/google_gemini_response.rb
    @palm_completion_job = GeminiLLM.complete(prompt: @long_or_short_prompt)
    @rag_excerpt = @palm_completion_job.chat_completion
    puts(@rag_excerpt.to_s.colorize :green)
else
        # nil
    @rag_excerpt = 'ü§∑ You decided NOT to invoke RAG. Why, I dont know. Maybe you wanted to check everything else.'
end
# demo pt2: LONG prompt with content.
# @articles_verbose = @closest_articles.map{|a| helpz.sanitize_news(a.article)}.join("\n") # .to_s
# puts(@articles_verbose.colorize :cyan)
# @long_prompt = helpz.rag_long_prompt(query: @query, article_count: @closest_articles.count, articles: @articles_verbose )
# puts(@rag_excerpt.to_s.colorize :green)

#@prompt = @short_prompt + "\n\n #{@closest_articles.map{|a| a.article}.join("\n---\n")}"
#@prompt = @short_prompt + "\n\n #{@closest_articles.map{|a| [a.title, a.summary]}.join("\n---\n")}"
#@prompt = @short_prompt + "\n\n #{@closest_articles.map{|a| a.excerpt_for_llm}.join("\n---\n")}\nSUMMARY OF THE ARTICLES:"
#@prompt2 = rag_long_prompt(date: Date.today, query: @query, article_count: @closest_articles.count, articles: @closest_articles.map{|a| a.excerpt_for_llm})



# non va
#a = Article.find(10260)
   # a.title_embedding = GeminiLLM.embed(text: a.title).embedding # WRONG i should use the OLD model for now.
#a.article_embedding = GeminiLLM.embed(text: a.article).embedding
#embedding_description = {
#    llm_project_id: GeminiLLM.project_id,
#    llm_dimensions: GeminiLLM.default_dimensions,
#    # llm_embedding_model: GeminiLLM.default_dimensions, cant find it!
#    llm_embeddings_model_name: "textembedding-gecko",#
#
#}
#a.article_embedding_description = embedding_description.to_s
#a.save # FUNGE! Allora devo ricalcolare tutto cacchio.

# Demo1

# demo2

prova_carpte = <<-'TEXT'
Why is Ruby *awesome*?
- It's fun to use
- Easy to learn
- And so much more...
Sembra fare doppio invo dannazione:
* Seby
* Ale
TEXT

%>
<%= demo_button_link('02', 'red') %>

<div class="container mx-auto mt-10">

<%=fancy_title "Show Query ('#{@query}')" %>
<h1 class="text-3xl mb-4 font-bold">Search for '<%= @query %>'</h1>

    <p class="text-gray-700 mb-2">
        Query is: <b><%= @query %></b>
    </p>
    <!-- useless
        <p class="text-gray-700 mb-2">
            Query type is: <b><%= @query_type %></b>
        </p>
    -->
    <p class="text-gray-700 mb-2">
        @rag_type is: <b><%= @rag_type %></b>
    </p>
    <p class="text-gray-700 mb-2">
        @rag_description is: <b><%= @rag_description %></b>
    </p>


<%= render_fancy_step_title(4, 'RAG result (first)') %>

    <%= render_prose_gemini @rag_excerpt %>


      <%#= render_prose_gemini('Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque euismod magna at odio finibus, vitae laoreet odio tempor.             Donec sed tempus leo. Nulla vitae elit libero. Maecenas eu augue eget nibh maximus tempor vitae quis risus.            Sed consectetur mi sed magna aliquam porta.') %>
      <%#= render_prose_gemini('Questo invece e quel che mi dice https://tailwindcss.com/docs/gradient-color-stops') %>
      <%#= render_prose_gemini(prova_carpte) %>


    <p class="text-gray-700 mb-2">
        TODO RICC <b>you did a lot of html cleanup in the demo02 ruby on console. Bring back that logic back to UI please.</b>
        Nearly there. Now check long vs short.
    </p>



    <h2 class="text-3xl mb-4 font-bold" >[demo1] Results from cached articles (RAG+ActiveRecord+Embeddings)</h2>


    <%= render_fancy_step_title(1, 'Calculate embedding of the query if not there already') %>

    <p>1. Calculate embedding of the query if not there already. </p>

        <%= render_prose("@query = '#{@query}'\n@e = GeminiLLM.embed(text: @query).embedding") %>

        Result:

        <tt><%= render_code_result @e.first(5) %></tt>

    <%= render_fancy_step_title(2, 'Search closest articles') %>

        <%= render_prose('@closest_articles = Article.select_sensible_columns.nearest_neighbors(:article_embedding, @e, distance: "euclidean").first(6) # .order(:published_date)') %>

        Result:

        <tt><%= render_code_result @closest_articles_info.first(2) %></tt>

    <p>
        <% @closest_articles_info.each do |id, title, author, distance| %>
            üì∞ (dist=<%= (distance*100).round(1) %>) <b><%= link_to title, "/articles/#{id}" %></b> (<%= author %>)<br>
        <% end if false %>
    </p>


    <%= render_fancy_step_title(3, 'RAG them into Langchain') %>


    <h3>Closest Articles to this embedding (remove)</h3>
    <%= fancy_title 'Closest Articles to this embedding' %>

    Query Info: <b><%= @query_info %></b><br/>

    <% @closest_articles.each do |a| %>
        üåç [<%= (a.neighbor_distance * 100.0).round(1) %>] <%= link_to a,a  %> <br/>
    <% end %>

    <h2>prompt in exemplo</h2>

    Here is the prompt:

    <%= render_prose "Here is the @short_prompt:\n----\n\n#{@short_prompt}\n...\n----\n (the augmented prompt is usually too long)" %>

    <% if Rails.env == 'development' and false # seriously its too much %>
        Dev only: the full prompt
        <%= render_prose @prompt.first(500) %>
    <% end %>

    <%= render_fancy_step_title(4, 'Summarize the result of RAGGED prompt') %>

    <%= render_prose "@rag_excerpt = GeminiLLM.complete(prompt: @prompt).output" %>

    RAG Result:

    <%= render_prose_gemini @rag_excerpt %>

</div>
