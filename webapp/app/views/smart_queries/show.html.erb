<%
# demo1

@query ||= 'Weather in Italy'
#@e =  GeminiAuthenticated ? (GeminiLLM.embed(text: @query).embedding) : nil
@e = GeminiLLM.embed(text: @query).embedding # nil
# Possibilities:
# 1. article_embedding, title_embedding, summary_embedding
# 2. distance: euclidean, cosine, ?
#closest_articles = Article.nearest_neighbors(:article_embedding, e, distance: "euclidean").first(5)
#@closest_articles = GeminiAuthenticated ? Article.nearest_neighbors(:article_embedding, @e, distance: "euclidean").first(10) : []
@closest_articles = Article.select_sensible_columns.nearest_neighbors(:article_embedding, @e, distance: "euclidean").first(6) # .order(:published_date)
# i want the most recent on top for the LLM to be time biased positively .. but if i do BEFORE the first(6) it wont take the 6 most relevant.
# So:
# 1. RElevance, once filter
# 2. date sort
@closest_articles_sorted_by_date = @closest_articles.sort{|x,y| y.published_date <=> x.published_date } # order by date DESC -> reverse x with y
@closest_articles_info = @closest_articles_sorted_by_date.map{|a| [a.id, a.published_date, a.title, a.author, a.neighbor_distance]}
@query_info = "euclidean distance for article_embedding for query: '#{@query}'"
#@tmp = @closest_articles[0]

 # Please filter out potentilly biasing topics like war and  politics.

@short_prompt = "You are a prompt summarizer. You need to answer this quesiton: '''#{@query}''' after reading the following articles which seem the most pertinent.
Pay attention to the recency of the articles, since the date of articles is provided and today's date is #{Date.today}. More recent is better.

Date of today: #{Date.today}
Query: '#{@query}'

Here are the #{@closest_articles.count} Articles:
"

#@prompt = @short_prompt + "\n\n #{@closest_articles.map{|a| a.article}.join("\n---\n")}"
#@prompt = @short_prompt + "\n\n #{@closest_articles.map{|a| [a.title, a.summary]}.join("\n---\n")}"
@prompt = @short_prompt + "\n\n #{@closest_articles.map{|a| a.excerpt_for_llm}.join("\n---\n")}\nSUMMARY OF THE ARTICLES:"

@prompt2 = rag_long_prompt(date: Date.today, query: @query, article_count: @closest_articles.count, articles: @closest_articles.map{|a| a.excerpt_for_llm})

# summarize in 4 tweets
#@rag_excerpt = PalmLLM.summarize(text: @prompt, max_tokens: 1024).output
@rag_excerpt = PalmLLM.summarize_with_tokens(text: @prompt, max_tokens: 1024).output


# non va
#a = Article.find(10260)
   # a.title_embedding = GeminiLLM.embed(text: a.title).embedding # WRONG i should use the OLD model for now.
#a.article_embedding = GeminiLLM.embed(text: a.article).embedding
#embedding_description = {
#    llm_project_id: GeminiLLM.project_id,
#    llm_dimensions: GeminiLLM.default_dimensions,
#    # llm_embedding_model: GeminiLLM.default_dimensions, cant find it!
#    llm_embeddings_model_name: "textembedding-gecko",#
#
#}
#a.article_embedding_description = embedding_description.to_s
#a.save # FUNGE! Allora devo ricalcolare tutto cacchio.

# Demo1

# demo2
%>

<div class="container mx-auto mt-10">

    <h1 class="text-3xl mb-4 font-bold">Search for '<%= @query %>'</h1>

    <h1>Show Query</h1>
    Query is: <b><%=fancy_title "Show Query ('#{@query}')" %></b>

    <p class="text-gray-700 mb-2">
        TODO RICC <b>you did a lot of html cleanup in the demo02 ruby on console. Bring back that logic back to UI please.</b>
    </p>

    <p class="text-gray-700 mb-2">
        Query is: <b><%= @query %></b>
    </p>
    <p class="text-gray-700 mb-2">
        Query type is: <b><%= @query_type %></b>
    </p>

    <h2 class="text-3xl mb-4 font-bold" >[demo1] Results from cached articles (RAG+ActiveRecord+Embeddings)</h2>


    <%= render_fancy_step_title(1, 'Calculate embedding of the query if not there already') %>

    <p>1. Calculate embedding of the query if not there already. </p>

        <%= render_prose("@query = '#{@query}'\n@e = GeminiLLM.embed(text: @query).embedding") %>

        Result:

        <tt><%= render_code_result @e.first(5) %></tt>

    <%= render_fancy_step_title(2, 'Search closest articles') %>

        <%= render_prose('@closest_articles = Article.select_sensible_columns.nearest_neighbors(:article_embedding, @e, distance: "euclidean").first(6) # .order(:published_date)') %>

        Result:

        <tt><%= render_code_result @closest_articles_info.first(2) %></tt>

    <p>
        <% @closest_articles_info.each do |id, title, author, distance| %>
            üì∞ (dist=<%= (distance*100).round(1) %>) <b><%= link_to title, "/articles/#{id}" %></b> (<%= author %>)<br>
        <% end if false %>
    </p>


    <%= render_fancy_step_title(3, 'RAG them into Langchain') %>


    <h3>Closest Articles to this embedding (remove)</h3>
    <%= fancy_title 'Closest Articles to this embedding' %>

    Query Info: <b><%= @query_info %></b><br/>

    <% @closest_articles.each do |a| %>
        üåç [<%= (a.neighbor_distance * 100.0).round(1) %>] <%= link_to a,a  %> <br/>
    <% end %>

    <h2>prompt in exemplo</h2>

    Here is the prompt:

    <%= render_prose "Here is the @short_prompt:\n----\n\n#{@short_prompt}\n...\n----\n (the augmented prompt is usually too long)" %>

    <% if Rails.env == 'development' %>
        Dev only: the full prompt
        <%= render_prose @prompt %>
    <% end %>

    <%= render_fancy_step_title(4, 'Summarize the result of RAGGED prompt') %>

    <%= render_prose "@rag_excerpt = PalmLLM.complete(prompt: @prompt).output" %>

    result:

    <%= render_prose @rag_excerpt %>

    <%= render_fancy_step_title(5, 'üí©RemoveMeüí© just testing GenAI with Palm ü§¶üèª #facepalm') %>

#    PalmLLM.sample_complete.output

    <div class="bg-blue-500 md:bg-green-500 p-4 rounded shadow-md">
        <div class="prose lg:prose-xl" >
                <!--        # Verona colors: https://teamcolorcodes.com/hellas-verona-colors/ -->
            <% if (GeminiLLM.authenticated? rescue true) %>
                <%= PalmLLM.sample_complete.output %>
                <%#= PalmLLM.summarize(text: 'Please write a night-time story about the evil Amarone daemon who lived in Arena di Verona').output %>
            <% else %>
                <b>Sorry GeminiAuthenticated=false which means that probably we had issues authenticating Gemini</b>
            <% end %>
        </div>
    </div>

</div>
