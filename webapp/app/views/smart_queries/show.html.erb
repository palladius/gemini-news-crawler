<%
# demo1
@query ||= 'Riccardo Carlesso'
@e = GeminiLLM.embed(text: @query).embedding rescue nil
# Possibilities:
# 1. article_embedding, title_embedding, summary_embedding
# 2. distance: euclidean, cosine, ?
#closest_articles = Article.nearest_neighbors(:article_embedding, e, distance: "euclidean").first(5)
@closest_articles = Article.nearest_neighbors(:article_embedding, @e, distance: "euclidean").first(10)
@query_info = "euclidean distance for article_embedding for query: '#{@query}'"

# non va
#a = Article.find(10260)
   # a.title_embedding = GeminiLLM.embed(text: a.title).embedding # WRONG i should use the OLD model for now.
#a.article_embedding = GeminiLLM.embed(text: a.article).embedding
#embedding_description = {
#    llm_project_id: GeminiLLM.project_id,
#    llm_dimensions: GeminiLLM.default_dimensions,
#    # llm_embedding_model: GeminiLLM.default_dimensions, cant find it!
#    llm_embeddings_model_name: "textembedding-gecko",#
#
#}
#a.article_embedding_description = embedding_description.to_s
#a.save # FUNGE! Allora devo ricalcolare tutto cacchio.

# demo2
%>

<div class="container mx-auto mt-10">

    <h1 class="text-3xl mb-4 font-bold">Search for '<%= @query %>'</h1>

    <h1>Show Query</h1>

    <p class="text-gray-700 mb-2">
        Query is: <b><%= @query %></b>
    </p>
    <p class="text-gray-700 mb-2">
        Query type is: <b><%= @query_type %></b>
    </p>



    <h2 class="text-3xl mb-4 font-bold" >[demo1] Results from cached articles (RAG+ActiveRecord+Embeddings)</h2>


    Play with console and Langchainrb now!

    <pre>
        1. Calculate embedding of the query if not there already.

            e = GeminiLLM.embed(text: "<%=@query%>").embedding

        2. Search closest articles.

            Article.nearest_neighbors(:article_embedding, e, distance: "euclidean").first(5)


        3. RAG them into Langchain.

            TODO

        4. Summarize. Add a prompt like:
    </pre>

    <h3>Closest Articles to this embedding</h3>

    Query Info: <b><%= @query_info %></b><br/>
    <% @closest_articles.each do |a| %>
        üåç [<%= (a.neighbor_distance * 100.0).round(1) %>] <%= link_to a,a  %> <br/>
    <% end %>

    <h2>prompt in exemplo</h2>

    <pre>
    Answer this quesiton: <%= @query %> after reading the following articles which seem the most pertinent.
    Pay attention to the recency of the articles, since the date of articles is provided and today's date is <%= Date.today %>

    ----
    Articles
    </pre>


    <h2 class="text-3xl mb-4 font-bold" >[demo2] Live Results from Gemini + Function Calling (live OpenAPI call + proper prompting)</h2>

    <pre>
        1. Use Gemini f(x) calling with LangchainRb +
        2. Use LangchainRb Tool 'NewsRetriever'
        3. return news to UI.
        4. Summarize. like

        Short story (max 256 token):
        GeminiLLM.summarize(text: 'Please write a night-time story about the evil Amarone daemon who lived in Arena di Verona').raw_response.predictions[0]['content']

        Long story (2048 tokens):
        GeminiLLM.complete(prompt: 'Please write a night-time story about the evil Amarone daemon who lived in Arena di Verona', max_output_tokens: 2047).raw_response.predictions[0]['content']

    </pre>

    <div class="bg-blue-500 md:bg-green-500 p-4 rounded shadow-md">
        <div class="prose lg:prose-xl" >
            <%#= GeminiLLM.summarize(text: 'Please write a night-time story about the evil Amarone daemon who lived in Arena di Verona').raw_response.predictions[0]['content']
            # Verona colors: https://teamcolorcodes.com/hellas-verona-colors/
             %>
            <%=
            GeminiLLM.complete(prompt: 'Please write a night-time story about the evil Amarone daemon who lived in Arena di Verona', max_output_tokens: 2047).raw_response.predictions[0]['content']
             %>
        </div>
    </div>

</div>


<%#= console %>
