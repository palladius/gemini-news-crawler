<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sun, 12 May 2024 16:44:46 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Billed for unauthorized requests? Google Cloud Storage vs. AWS S3]]></title>
            <link>https://medium.com/google-cloud/billed-for-unauthorized-requests-google-cloud-storage-vs-aws-s3-8d4d6551fe72?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/8d4d6551fe72</guid>
            <category><![CDATA[aws]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[cloud-storage]]></category>
            <category><![CDATA[billing]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Marcos Manuel Ortega]]></dc:creator>
            <pubDate>Sat, 11 May 2024 04:44:46 GMT</pubDate>
            <atom:updated>2024-05-11T04:44:46.629Z</atom:updated>
            <content:encoded><![CDATA[<p>A recent story highlighted how unauthorized access to an empty AWS S3 bucket can result in surprising charges. This raised the question: can unauthorized access to Google Cloud Storage (GCS) also lead to unexpected bills?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*7wvAYoDth1XHDtqOA_BJJQ.jpeg" /></figure><h3>Catchup: what happened?</h3><p>A bit over a week ago, an interesting Medium story popped-up into many socialÂ feeds:</p><p><a href="https://medium.com/@maciej.pocwierz/how-an-empty-s3-bucket-can-make-your-aws-bill-explode-934a383cb8b1">How an empty S3 bucket can make your AWS bill explode</a></p><p>It even made into <a href="https://news.ycombinator.com/item?id=40203126">Hacker News frontpage</a>.</p><p>To sum up: a user found that unauthorized 403 requests to upload files to his AWS S3 empty bucket by other users were billed to his account, as his bucket ID was used as a default value into a popular open-source solution and thousands of deployments tried by mistake to upload backup data into hisÂ bucket.</p><p><strong>This was quite worrisome</strong>, as could be exploited by a malicious actor to bring anyone down just knowing any of our S3 bucket IDs: not by DoS but by <a href="https://academic.oup.com/cybersecurity/article/10/1/tyae004/7634012">Denial-of-Wallet attacks</a>.</p><p>This bore the questionâ€Šâ€”â€Š<em>would this also happen on Google CloudÂ Storage?</em></p><p>Spoiler alert: <strong><em>no, 403 PUT requests are not billed inÂ GCS.</em></strong></p><h3>Considerations</h3><h4>Base scenario</h4><p>The original story stated that 403 non-authorized requests to AWS S3 wereÂ billed.</p><p>We wanted to also verify whether in Google Cloud Storage 403 non-authorized PUT requests to private GCS buckets are billed orÂ not.</p><h4>Number ofÂ requests</h4><p>As per the <a href="https://cloud.google.com/storage/pricing#operations-by-class">GCS pricing docs</a>, listing and uploading objects are billed as <em>Class A operations</em> and downloading/reading objects as <em>Class B operations</em>. Pricing is per 1000 operations, so we aimed for a adequate number of requests to make sure we would notice theÂ charges.</p><h4>Requester paysÂ disabled</h4><p>In GCS and S3 you can opt-in for â€œrequester paysâ€, where the user making the request is paying for it instead of the bucketâ€™s owner, which is intended for other scenarios e.g. sharing public data, rather than e.g. public website static date. Therefore, it was intentionally disabled, as is the usual configuration.</p><h4>â€œAlways freeâ€ tier and billingÂ exports</h4><p>In Google Cloud, there is a <a href="https://cloud.google.com/free/docs/free-cloud-features#storage">free tier for Cloud Storage</a> for some amount of Class A and B operations, so it would mask the costs of our requests.</p><p>Therefore, <a href="https://cloud.google.com/billing/docs/how-to/export-data-bigquery">detailed usage cost data</a> was exported to BigQuery and analyzed using SQL, where we could see the charges for operations even if they would fall in this free tier or be subjected to credits, discounts and promotions.</p><p>This cost data is exported to BigQuery with some latency (usually several hours, depending on service and SKU), so results were checked after 24Â hours.</p><h3>Simulating unauthorized access: the experiment</h3><p>2 Google Cloud projects wereÂ created:</p><ol><li><em>gcs-unauthorized-requests</em></li><li><em>gcs-requesting-instance</em></li></ol><p><em>Note: Usually you would want to keep your project IDs private for security.</em></p><h4>Billing</h4><p>Billing was configured for both projects with an active, good-standing billing account, with detailed usage cost data exported to BigQuery.</p><h4>Details for gcs-unauthorized-requests project</h4><ul><li>Required roles: Cloud StorageÂ Admin</li><li>Enabled APIs: Cloud Storage (<em>storage.googleapis.com</em>), BigQuery (<em>bigquery.googleapis.com</em>, to analyze exported billing data, could also be run in anotherÂ project)</li><li>Cloud Storage resources: locationâ€Šâ€”â€Š<em>regional</em>, <em>europe-southwest1</em>, storage classâ€Šâ€”â€Š<em>standard</em></li><li>Private bucket: bucket-name-redacted, label bucket-private</li></ul><h4>Details for gcs-requesting-instance project</h4><ul><li>Required roles: Compute Engine Admin, Service AccountÂ Admin</li><li>Enabled APIs: Compute Engine (<em>compute.googleapis.com</em>)</li><li>Networkingâ€Šâ€”â€ŠVPC &amp; subnet:Â <em>default</em></li><li>Networkingâ€Šâ€”â€ŠFW rules: <em>default </em>VPC default rules â†’ SSH ingress enabled, all egressÂ enabled</li><li>VM instance: <em>requesting-instance</em></li><li>Region &amp; zone: <em>europe-southwest1-a</em> (Madrid)</li><li>Machine type: <em>e2-standard-16</em> (max. egress bandwidth of 16Â Gbps)</li></ul><blockquote>(Would have been less expensive and recommended to use a spot VM instead of a regular one, but forgot to check the optionâ€¦Â <em>oops</em>)</blockquote><ul><li>VM instance OS: Debian 12 (bookworm) boot disk from the GCP public imageÂ family</li><li>VM instance has GCP Cloud SDK already installed, as by default in (most) GCEÂ images</li><li>VM instance with an <em>ephemeral externalÂ IP</em></li><li>User-managed service account assigned to the VM <strong>without any roles/permissions assigned</strong>, to force a 403Â response</li></ul><h3>The test</h3><p>Test consisted in SSHing to requesting-instance VM and running this Bash command executing a 403 PUT request to the GCS bucket 2100 times (<em>more than the 1000 needed, just inÂ case</em>):</p><pre>for i in {1..2100};<br>do<br>  printf &quot;\niter $i\n&quot; &amp;&amp; \<br>  gsutil cp private-bucket-file.txt \<br>    gs://gcs-unauthorized-request-private;<br>done</pre><p>BigQuery SQL query for checking results (after 24 hours, given the expected delay for the billingÂ export):</p><pre>SELECT<br>  project.id as project_id,<br>  service.description as service_description,<br>  sku.description as sku_description,<br>  usage_start_time,<br>  usage_end_time,<br>  project.id,<br>  labels,<br>  location.region,<br>  resource.global_name,<br>  usage.amount,<br>  usage.unit,<br>  usage.amount_in_pricing_units,<br>  usage.pricing_unit<br>FROM<br>  `REDACTED.billing_export.gcp_billing_export_resource_v1_REDACTED`<br>WHERE<br>  usage_start_time &gt;= TIMESTAMP(&quot;2024-05-03&quot;)<br>  AND usage_start_time &lt; TIMESTAMP(&quot;2024-05-08&quot;)<br>  AND (project.id = &#39;gcs-unauthorized-requests&#39;<br>    OR project.id = &#39;gcs-requesting-instance&#39;)<br>  AND service.description = &#39;Cloud Storage&#39;<br>ORDER BY<br>  usage_start_time</pre><h3>Experiment results</h3><p>Remember: we wanted to check if 403 PUT requests to a private GCS bucket areÂ billed.</p><p>After executing the previous bashÂ command:</p><ul><li>Response status code:Â 403</li><li>Response: â€œAccessDeniedException: 403 <a href="mailto:requesting-instance@gcs-requesting-instance.iam.gserviceaccount.com">REDACTED@gcs-requesting-instance.iam.gserviceaccount.com</a> does not have storage.objects.create access to the Google Cloud Storage object.<br>Permission â€˜storage.objects.createâ€™ denied on resource (or it may notÂ exist).â€</li><li>Number of requests: 2100</li><li>Start: 24/05/07 08:26Â CEST</li><li>End: 24/05/07 09:19Â CEST</li></ul><p>After waiting +24 hours, we ran the query in the BigQuery billingÂ dataset.</p><blockquote><strong>SQL query results: </strong>No â€œRegional Standard Class A Operationsâ€ were shown for these projects andÂ dates</blockquote><h3>Verdict: No charges for unauthorized access</h3><p>The good news: Google Cloud Storage doesnâ€™t bill for unauthorized PUT requests, neither to the private bucket owner nor the unauthorized requester.</p><h4>What This Means forÂ You</h4><p>Unlike AWS S3, Google Cloud Storage protects you from this â€œDenial-of-Walletâ€ attack where unauthorized access can inflate your bill. This adds another layer of security and cost control for your Cloud StorageÂ buckets.</p><h3>Further Exploration</h3><p>While unauthorized PUT requests are free, we plan to investigate billing for other access scenarios, including unauthorized GET requests and public bucket access attempts. In particular,</p><ol><li>Check if 403 GET requests to a private GCS bucket existing file areÂ billed</li><li>Check if 403 GET requests to a private GCS bucket non-existing file areÂ billed</li><li>Check if 403 PUT requests to a public GCS bucket areÂ billed</li><li>Check if 403 GET requests to a public GCS bucket non-existing file areÂ billed</li></ol><h4>Special thanksÂ to</h4><ul><li><a href="https://cloud.google.com/innovators/champions">Google Cloud Champions Innovator program</a>, for their membersâ€™ support and feedback.</li><li><a href="https://cloud.google.com/innovators/innovatorsplus">Google Cloud Innovators Plus program</a>, for their annual free GCP credits which were used for this (and many others)Â project.</li></ul><blockquote>This article was jointly written by Marcos Manuel Ortega (<a href="https://www.linkedin.com/in/marcosmanuelortega/">LinkedIn</a>), director at Indavelopers, and Julio Quinteros P. (<a href="https://twitter.com/jquinterosp">@jquinterosp</a>), Data &amp; AI/ML manager/practice lead atÂ Axmos</blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8d4d6551fe72" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/billed-for-unauthorized-requests-google-cloud-storage-vs-aws-s3-8d4d6551fe72">Billed for unauthorized requests? Google Cloud Storage vs. AWS S3</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud SLO demystified: Uncovering metrics behind predefined SLOs]]></title>
            <link>https://medium.com/google-cloud/google-cloud-slo-demystified-uncovering-metrics-behind-predefined-slos-40b153970479?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/40b153970479</guid>
            <category><![CDATA[observability]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[sre]]></category>
            <category><![CDATA[gcp-security-operations]]></category>
            <category><![CDATA[reliability]]></category>
            <dc:creator><![CDATA[minherz]]></dc:creator>
            <pubDate>Sat, 11 May 2024 04:43:14 GMT</pubDate>
            <atom:updated>2024-05-11T04:43:14.226Z</atom:updated>
            <content:encoded><![CDATA[<p><em>This post is mirrored from my personal website </em><a href="https://leoy.blog"><em>leoy.blog</em></a><em>. See reader-friendly </em><a href="https://leoy.blog/posts/google-cloud-slo-demystified/"><em>original</em></a><em>. This material was prepared with help from </em><a href="https://www.linkedin.com/in/ethantruong/"><em>EthanÂ Truong</em></a><em>.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/799/1*MHwc3EM1GjxKpsDXL3YCTQ.png" /></figure><p>Google Cloud supports <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring"><strong>service monitoring</strong></a> by defining and tracking <a href="https://en.wikipedia.org/wiki/Service-level_objective"><strong>SLO</strong></a> of the services based on their metrics that are ingested to Google Cloud. This support greatly simplifies implementing SRE practices for services that are deployed to Google Cloud or that store telemetry data there. To make it even more simple to developers, the service monitoring is able to automatically detect many types of managed services and supports predefined <em>availability</em> and <em>latency</em> <a href="https://en.wikipedia.org/wiki/Service_level_indicator"><strong>SLI</strong></a> definitions for them.<br>When you define a new SLO you are prompted to select a predefined SLI or to define yourÂ own.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/757/0*78QiebDtoB3PG1KF.png" /></figure><p>While it is convenient to use predefined SLIs you arenâ€™t provided with information about SLI definitions. If you already defined an SLO using one of predefined SLIs, you can get its detailed description using the <a href="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives/get"><strong>services.serviceLevelObjectives.get</strong></a> API. For example, the following command returns JSON payload that describes the SLO named availability_slo of the frontend service that was defined using the predefined availability SLI:</p><pre>curl -X GET \<br>https://monitoring.googleapis.com/v3/projects/${GOOGLE_CLOUD_PROJECT}/\<br>services/frontend/\<br>serviceLevelObjectives/frontend-availability-slo?view=EXPLICIT \<br>-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>-H &quot;Content-Type: application/json; charset=utf-8&quot;</pre><p>This example uses gcloud <a href="https://cloud.google.com/sdk/gcloud"><strong>CLI</strong></a> and the environment variable GOOGLE_CLOUD_PROJECT. This variable is automatically set by <a href="https://cloud.google.com/shell/docs/launching-cloud-shell"><strong>Cloud Shell</strong></a>. However, if you run this command in your shell, you will need to <a href="https://cloud.google.com/sdk/docs/install-sdk"><strong>install</strong></a> gcloud, then to authenticate vs Google Cloud and to set up the GOOGLE_CLOUD_PROJECT environment variable to the project ID where your SLO is defined. The following paragraphs describe predefined SLIs to enable you to make an educated choice next time you useÂ them.</p><h3>Managed (auto-detected) services</h3><p>The <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/api/using-api#managing-services"><strong>managed services</strong></a> generate metrics that are further used to calculate predefined SLIs. The service monitoring automatically detects these managed services or the user can provision them manually. As long as the service is defined as one of the basic services and uses one of the following types, it can leverage the predefined SLIs.</p><ul><li>APP_ENGINE â€“ Every <a href="https://cloud.google.com/appengine/docs/standard/testing-and-deploying-your-app"><strong>deployed application</strong></a> is considered a service. This service can be monitored using multiple ways including SLOs.</li><li>ISTIO_CANONICAL_SERVICE â€“ When vanilla Istio is configured to ingest its metrics to Cloud Monitoring, its services are auto-detected as Istio canonical services. GKE workloads that leverage the managed version of Istio (aka <a href="https://cloud.google.com/service-mesh"><strong>ASM</strong></a>) do not use thisÂ type.</li><li>CLUSTER_ISTIO â€“ Services that run on GKE with ASM are identified as Cluster Istio services. Note that since ASM supports Kubernetes clusters on Azure and AWS you can implement service monitoring for these clusters asÂ well.</li><li>CLOUD_RUN â€“ Cloud Run <a href="https://cloud.google.com/run/docs/overview/what-is-cloud-run#services"><strong>services</strong></a> are auto-detected using this type. Important to remember that service monitoring does not support Cloud RunÂ jobs.</li></ul><blockquote><strong>NOTE:</strong> Not all services that Service monitoring automatically detects, have predefined SLIs. For example, services of types GKE_SERVICE or GKE_WORKLOAD do not have predefined SLIs (because they do not ingest any metrics to GoogleÂ Cloud).</blockquote><h3>Predefined SLIs</h3><p>All predefined SLIs are measured in % and calculated based on the well known formulaÂ of</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/321/0*SFvP6lTuqqsst9Qp.jpg" /></figure><p>The predefined SLIs use a <strong>resource type</strong> and a <strong>metric</strong> to get the event data. The <strong>availability metric</strong> is used to get availability events and the <strong>latency metric</strong> is used to get latency events. From all metric data the good events are derived using a <strong>good service filter</strong> and valid eventsâ€Šâ€”â€Šusing the union of <strong>good service filter</strong> <em>AND</em> <strong>bad service filter</strong>. The filters are built using metric labels and leverage the fact that both availability and latency metrics have the same set of labels. You can use the following MQL query to see the <em>validÂ events</em>:</p><pre>fetch %%resource%%<br>| metric &#39;%%metric%%&#39;<br>| filter (%%good_service_filter%%) || (%%bad_service_filter%%)</pre><p>And this MQL query to see the <em>goodÂ events</em>:</p><pre>fetch %%resource%%<br>| metric &#39;%%metric%%&#39;<br>| filter %%good_service_filter%%</pre><blockquote><strong>NOTE:</strong> You will have to replace values enclosed with double percentage (%%) with the actual values of the predefined SLIs.</blockquote><h3>AppEngine service</h3><p>Detects both standard and flexible AppEngine services.</p><p><strong>Resource type</strong>:Â gae_app</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#appengine/http/server/response_count"><strong>appengine.googleapis.com/http/server/response_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#appengine/http/server/response_latencies"><strong>appengine.googleapis.com/http/server/response_latencies</strong></a></p><p><strong>Good serviceÂ filter</strong>:</p><pre>resource.module_id = &#39;MODULE_ID&#39; &amp;&amp;<br>metric.response_code &gt;= 200 &amp;&amp; metric.response_code &lt;= 299</pre><blockquote><strong>NOTE:</strong> MODULE_ID should be replaced with the actual AppEngine service/module name. See <a href="https://cloud.google.com/monitoring/api/resources#tag_gae_app"><strong>gae_app</strong></a> resource description for more information.</blockquote><p><strong>Bad serviceÂ filter</strong>:</p><pre>resource.module_id = &#39;MODULE_ID&#39; &amp;&amp;<br>metric.response_code &gt;= 500 &amp;&amp; metric.response_code &lt;= 599</pre><h3>Canonical IstioÂ service</h3><p>Detects Istio services provisioned by OSSÂ Istio.</p><p><strong>Resource type</strong>: istio_canonical_service</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/request_count"><strong>istio.io/service/server/request_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/response_latencies"><strong>istio.io/service/server/response_latencies</strong></a></p><p><strong>Good serviceÂ filter</strong>:</p><pre>resource.mesh_uid = &#39;MESH_ID&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>resource.canonical_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 200 &amp;&amp; metric.response_code &lt;= 299</pre><blockquote><strong>NOTE:</strong> MESH_ID should be replaced with the identifier for an Istio service mesh. K8S_NAMESPACE should be replaced with the namespace where the service is manifested. K8S_SERVICE_NAME should be replaced with the name of the <a href="https://cloud.google.com/service-mesh/docs/canonical-service"><strong>canonical service</strong></a>. See <a href="https://cloud.google.com/monitoring/api/resources#tag_istio_canonical_service"><strong>istio_canonical_service</strong></a> resource description for more information.</blockquote><p><strong>Bad serviceÂ filter</strong>:</p><pre>resource.mesh_uid = &#39;MESH_ID&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>resource.canonical_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 500 &amp;&amp; metric.response_code &lt;= 599</pre><h3>ASM service</h3><p>Detects the managed flavor of the Istio service. Used with Istio services provisioned by ASM, hence uses a different resourceÂ type.</p><p><strong>Resource type</strong>: k8s_container</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/request_count"><strong>istio.io/service/server/request_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/response_latencies"><strong>istio.io/service/server/response_latencies</strong></a></p><p><strong>Good serviceÂ filter</strong>:</p><pre>resource.cluster_name = &#39;CLUSTER_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_namespace = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 200 &amp;&amp; metric.response_code &lt;= 299</pre><blockquote><strong>NOTE:</strong> CLUSTER_NAME should be replaced with the name of the cluster running the service. LOCATION should be replaced with the location (either zone or region) of the service. K8S_NAMESPACE should be replaced with the namespace where the service is manifested. K8S_SERVICE_NAME should be replaced with the name of the service. See <a href="https://cloud.google.com/monitoring/api/resources#tag_k8s_container"><strong>k8s_container</strong></a> and the <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/request_count"><strong>metric</strong></a> descriptions for more information.</blockquote><p><strong>Bad serviceÂ filter</strong>:</p><pre>resource.cluster_name = &#39;CLUSTER_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_namespace = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 500 &amp;&amp; metric.response_code &lt;= 599</pre><h3>Cloud RunÂ service</h3><p>Detects a service deployed at CloudÂ Run.</p><p><strong>Resource type</strong>: cloud_run_revision</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#run/request_count"><strong>run.googleapis.com/request_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#run/request_latencies"><strong>run.googleapis.com/request_latencies</strong></a></p><p><strong>Good serviceÂ filter</strong>:</p><pre>resource.service_name = &#39;SERVICE_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>metric.response_code_class = &#39;5xx&#39;</pre><blockquote><strong>NOTE:</strong> SERVICE_NAME should be replaced with the name of the Cloud Run service. LOCATION should be replaced with the region where the service is deployed. See <a href="https://cloud.google.com/monitoring/api/resources#tag_cloud_run_revision"><strong>cloud_run_revision</strong></a> for more information.</blockquote><p><strong>Bad serviceÂ filter</strong>:</p><pre>resource.service_name = &#39;SERVICE_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>metric.response_code_class = &#39;5xx&#39;</pre><h3>Afterword</h3><p>The predefined SLIs are used with the â€œ<a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#slo-type-request"><strong>request-based</strong></a>â€ SLOs. This is because the auto-detected services communicate using requests. If you use the services differently or, in your case the metrics used in the predefined SLIs do not reflect good and valid events correctly, use <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/ui/create-slo#svcmon-sli-other"><strong>custom SLI</strong></a> for your availability and latencyÂ SLOs.</p><p>The shown filters values use syntax that is compatible with MQL. If you plan to reuse them in custom SLIs with <a href="https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/monitoring_slo"><strong>Terraform</strong></a> or by calling <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/api/api-structures"><strong>API</strong></a>, you should reformat it. The main changes include replacing &amp;&amp; with AND, changing the keys of the labels by adding label suffix (e.g. metric.response_code will become metric.label.response_code). Look into documentation for more guidelines.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=40b153970479" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/google-cloud-slo-demystified-uncovering-metrics-behind-predefined-slos-40b153970479">Google Cloud SLO demystified: Uncovering metrics behind predefined SLOs</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GCP Cross-region internal application load balancerÂ : why and how]]></title>
            <link>https://medium.com/google-cloud/gcp-cross-region-internal-application-load-balancer-why-and-how-f3a33226d690?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/f3a33226d690</guid>
            <category><![CDATA[load-balancer]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[networking]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Gauravmadan]]></dc:creator>
            <pubDate>Sat, 11 May 2024 04:42:52 GMT</pubDate>
            <atom:updated>2024-05-11T04:42:52.833Z</atom:updated>
            <content:encoded><![CDATA[<h3>GCP Cross-region internal application load balancerÂ : why andÂ how</h3><p>ContextÂ :</p><p>A Google Cloud<strong><em> internal Application Load Balancer</em></strong> is a proxy-based layer 7 load balancer that enables you to run and scale your services behind a single internal IP address. The internal Application Load Balancer distributes HTTP and HTTPS traffic to backends hosted on a variety of Google Cloud platforms such as Compute Engine, Google Kubernetes Engine (GKE), and Cloud Run. This load balancer is available in 2 flavorsÂ :</p><ol><li>Regional internal application loadÂ balancer</li><li>Cross-regional internal application loadÂ balancer</li></ol><p>The cross-region internal application load balancer enables you to load balance traffic to backend services that are globally distributed, including traffic management that ensures traffic is directed to the closestÂ backend.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/911/0*1RXP-_RyrxR_ngJ6" /></figure><p>The idea of this blog post is to see how cross-regional internal application load balancer is deployed and its use-cases under steady state and failover situations.</p><p>Failover may fall in 2 categoriesÂ : (a) failover of a regional backend (b) failover of iLBÂ frontend</p><ul><li>Failover of Frontend: When the proxy tasks in a region (where the forwarding rule is configured) fail. Failover to a forwarding rule in a different region is done using a DNS failoverÂ policy.</li><li>Failover of Backend: When all of the backends (e.g. VM instances) in a region fail, the load balancer automatically chooses healthy, available backends in the next closestÂ region.</li></ul><h3>Topology</h3><p>In the test topology used in this blogÂ , I had one VPC with subnets in 2 regionsÂ : asia-south1 ( Mumbai) and asia-south2 ( Delhi )Â . The CIDR used in each subnet is shown in the topology diagram belowÂ . These 2 subnets host the instance groups where web application servers areÂ hosted.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*XsF-O-_jjjgd-dLW" /></figure><p>As a next step ( and a mandatory step ) for setting up cross regional internal application load balancerÂ , it is necessary to reserve PROXY subnet for each region in contextÂ . In my exampleÂ , I have reserved 2 proxy subnets ( one each for asia-south1 and asis-south2 )</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/742/0*jCHDHuwtb6bpguLO" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tZyjIMJGJNJCbZ9h" /></figure><p>Setup Load balancerÂ : Lets break this in 3 simpleÂ parts</p><ol><li>Front endÂ setup</li><li>Backend setup</li><li>Routing rules</li></ol><p><strong>Front EndÂ :</strong> Create one front end per regionÂ . You may choose the protocol as HTTP or HTTPs for the front endÂ . In our caseÂ , we had following configured for the frontÂ end</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oRK2qM9x-wbnZ3K4" /></figure><p><strong>Backend</strong></p><p>Created one backendÂ service</p><p>In this backend service â†’ I created 2 backendsÂ . One for each regional instance group.In my caseÂ , each backend consist of instance groups running webÂ server.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fx1m5yQriei4epnU" /></figure><p>Sample output of load balancer backendÂ service</p><pre>$ gcloud compute backend-services describe cross-region-bserv-for-ilb --global<br>affinityCookieTtlSec: 0<br>backends:<br>- balancingMode: UTILIZATION<br>  capacityScaler: 1.0<br>  group: https://www.googleapis.com/compute/v1/projects/gm-test-337806/zones/asia-south1-c/instanceGroups/lb-test-mumbai-umig<br>  maxRatePerInstance: 100.0<br>  maxUtilization: 0.8<br>- balancingMode: UTILIZATION<br>  capacityScaler: 1.0<br>  group: https://www.googleapis.com/compute/v1/projects/gm-test-337806/zones/asia-south2-a/instanceGroups/lb-test-delhi-01<br>  maxRatePerInstance: 100.0<br>  maxUtilization: 0.8<br>connectionDraining:<br>  drainingTimeoutSec: 300<br>creationTimestamp: &#39;2024-04-27T21:04:05.231-07:00&#39;<br>description: &#39;&#39;<br>fingerprint: WRypjLxKN0M=<br>healthChecks:<br>- https://www.googleapis.com/compute/v1/projects/gm-test-337806/global/healthChecks/auto-health<br>id: &#39;1322192009993155674&#39;<br>kind: compute#backendService<br>loadBalancingScheme: INTERNAL_MANAGED<br>localityLbPolicy: ROUND_ROBIN<br>logConfig:<br>  enable: false<br>  optionalMode: EXCLUDE_ALL_OPTIONAL<br>name: cross-region-bserv-for-ilb<br>port: 80<br>portName: lb-test-mumbai-http<br>protocol: HTTP<br>selfLink: https://www.googleapis.com/compute/v1/projects/gm-test-337806/global/backendServices/cross-region-bserv-for-ilb<br>sessionAffinity: NONE<br>timeoutSec: 30<br>usedBy:<br>- reference: https://www.googleapis.com/compute/v1/projects/gm-test-337806/global/urlMaps/cross-regional-internal-layer7</pre><h3>TEST #Â 1</h3><p>Client sending traffic to regional front-endÂ :In this testÂ , we assumed that regional clients will send traffic to respective iLB front endÂ ; i.e Mumbai client sent traffic to ilB front end in Mumbai (asia-south1) and Delhi client sent requests to Delhi (asia-south2) frontÂ end</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JQ_eqgNPJ7FLA4Km" /></figure><p>Test initiated from TEST-CLIENT-2 (DelhiÂ region)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nsf8Ldi_ft8FSE8Ard0sSA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wNpJFYX8rVZbXdpRiAJxVA.png" /></figure><p>This worked as expectedÂ . Customer relied on making separate entries in DNS for each front end and this was used by respective clients in their application access method. The load balancer served traffic from closest healthy backend available.</p><h3>TEST #Â 2</h3><p>Client sending traffic to cross-regional front-endÂ : This is continuation of test case 1Â , where the requirement is to access a workload in a cross-regionÂ ; i.e. client in asia-south1 (Mumbai) trying to access workload in asia-south2 by accessing front end name / IP of layer7 iLB in asia-south2 region</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5xlcWKelOS-SYrhF" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4gFU4GGLPR868OnKlK-chg.png" /></figure><p>HenceÂ , once the requests land on a iLB frontendÂ , it tries to route it to a healthy backend nearest toÂ it</p><h3>TEST #Â 3</h3><p>Failover ScenarioÂ : In this caseÂ , letâ€™s assume service in asia-south2 failed ( Or all VMs down )Â . For simulation in my setupÂ , I removed the backend in Delhi region from the iLB backend service configuration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5V3H5pYuX7hmvtDc" /></figure><p>Lets see theÂ results</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NPgfHr0OtxKuAAAPfwG98Q.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zz7jqMMh8ezDuhxHai22CQ.png" /></figure><p>HenceÂ , as expectedÂ , in event of no healthy backendÂ , the iLB will redirect the traffic to healthy backends in next closestÂ region.</p><h3>TEST #Â 4</h3><p>Combining Cross region ILB functionality with GCP CLOUD DNS GEO routingÂ POLICY</p><p>This is most interesting usecaseÂ . HereÂ ; instead of using separate domain names for application hosted in asia-south1 / asia-south2, the customer created a domain name called â€œ<a href="http://application.myapp.com">application.myapp.com</a>â€. In addition to thisÂ , customer created a GEO routing policy specifying region as â€˜asia-south1â€™ resolve to 10.10.152.10 [ load balancer front end in asia-south1 ] and asia-south2 resolve ot 10.10.151.11 [ load balancer front end in asia-south1 ]Â . Sample DNS configuration is as followsÂ -</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*u81MR6Lacpz10SnG" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*eNvPTqsf_g-1OAKE" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*A-AcPldmLJwaN4B4R8RFHQ.png" /></figure><p>When client attempted to access the app behind<strong> </strong><a href="http://application.myapp.com">application.myapp.com</a><strong>Â </strong>; the response came from the nearbyÂ region</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*w1m9uOolhhvormxUGHsUXg.png" /></figure><p>BACKEND FAILOVER ( no healthy backend in one of regionÂ )</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QST9fnimdqhQMzGi" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0hChkmK94Q9SMUc8Qdhq9w.png" /></figure><p>AgainÂ , as expectedÂ , no matter which frontend IP the query landsÂ , iLB send the request to nearby healthy backend. For exampleÂ , if client in Delhi region sends request to front-end in Delhi-region of iLBÂ , this will be routed to next near healthy backend ( which is Mumbai in my setupÂ )</p><h3>TEST #Â 5</h3><p>This test has to do with failover of proxy task in a given regionÂ . This is where Failover Policy of Cloud DNS can comeÂ handy.</p><p>In this setupÂ , customer needs to serve everything from Asia-south1 application instance and if this is not available / reachableÂ ; asia-south2 instance shall be consideredÂ .</p><p>Lets look at Cloud DNS FailoverÂ Policy</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7fuPsVppPUXHl6cxa31kaA.png" /></figure><p>This simply means that clients will resolve to iLB frontend IP address in Asia-south1 ( 10.10.152.10 ) and if this isnt reachableÂ ; clients in both regions will resolve app1.myapp.com to iLB front-end IP in asia-south2 ( 10.10.151.12 )</p><p>Hence in steady state the setup will work likeÂ follows</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sQ5ubMo1PNetYpbPngKuUQ.png" /></figure><p>However in case of Front end failÂ , the setup will work like following</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93E6z75MsUUVRDhMInYsRw.png" /></figure><p>Useful URLs</p><p><a href="https://cloud.google.com/load-balancing/docs/l7-internal/setting-up-l7-cross-reg-internal">Setup cross-regional internal L7 loadÂ balancer</a></p><p><strong>Disclaimer</strong>: This is to inform readers that the views, thoughts, and opinions expressed in the text belong solely to the author, and not necessarily to the authorâ€™s employer, organization, committee or other group or individual.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f3a33226d690" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/gcp-cross-region-internal-application-load-balancer-why-and-how-f3a33226d690">GCP Cross-region internal application load balancerÂ : why and how</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[API to Chatbot in less than 5 minutes | Vertex AI Extensions]]></title>
            <link>https://medium.com/google-cloud/api-to-chatbot-in-less-than-5-minutes-27ad6c4a063d?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/27ad6c4a063d</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[chatbots]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <dc:creator><![CDATA[Vaibhav Malpani]]></dc:creator>
            <pubDate>Fri, 10 May 2024 10:17:51 GMT</pubDate>
            <atom:updated>2024-05-10T10:17:51.650Z</atom:updated>
            <content:encoded><![CDATA[<p>Learn how to quickly create a chatbot over your API using Vertex AI Extensions. No coding or training required. Interact with your API seamlessly through a chat interface. Read more to explore the possibilities!</p><h3>What isÂ API?</h3><p>An API (Application Programming Interface) serves as a bridge between various software programs. It defines a set of rules and requirements that apps may follow to interact with oneÂ another.</p><h3>What isÂ Chatbot?</h3><p>A software program that simulates human-user conversations. Chatbots are frequently used in messaging apps, websites, and customer service to offer information or doÂ tasks.</p><p><strong>Traditionally</strong>, a Chatbot is trained on intents to understand which data to be fetched, what all parameters are required and how to extract them from the prompt given by user and at the end format the API response for easy reading. This will improve readability for theÂ user.</p><h4><strong>Problem Statement:</strong></h4><p>If any changes are made to API, like taking in a new parameter, change in the response from API, in that case, we will have to make the changes in the chatbot as well to handle the new changes. Similarly, if a new API is added, then the whole process of creating intent, extracting parameters, prettifying API response.</p><h4><strong>Solution:</strong></h4><p>Using Extension in Vertex AI, to build Chatbots over your API. How to do it?? Just follow the stepsÂ below!!</p><blockquote><strong>Disclaimer: Extensions in Vertex AI is still in PREVIEW. It </strong>Is under development and gaining functionalities.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NzBLlx8ssBksQoAYY9f9Yg.jpeg" /><figcaption>AI Generated by SDXLÂ 1.0</figcaption></figure><h4><strong>How to Build a Chatbot over yourÂ API?</strong></h4><ol><li>Create a sample <a href="https://swagger.io/docs/specification/basic-structure/"><strong><em>OpenAPI specification file</em></strong></a><strong><em> </em></strong>(only yaml file supported till now). I have used a sample file from <a href="https://petstore.swagger.io/#/"><strong>swagger</strong></a><strong> </strong>and trimmed down the file to only include API related toÂ pets.</li><li>Login to Google cloud console, and navigate to <a href="https://console.cloud.google.com/vertex-ai/extensions"><strong>Extensions in Vertex AI</strong></a>. Click on â€œCreate Extensionâ€</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/967/1*XhAbnI0WjAi2LwiGO9NqRg.png" /></figure><p>3. Enter the name that you want to give to the extension, we have given name as â€œpet_storeâ€. Then enter any description that you want and in the Extension Type, Click on <strong>â€œCustomâ€. </strong><em>(For other 2 Extension Types stay tuned and follow, I am going to soon write aboutÂ them)</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/1*n4HJxa0_rimgxzJN0EoN5w.png" /></figure><p>4. Once you select Custom Extension Type, you will get a below form, just enter details according to your API. In the OpenAPI Spec file section, upload you YAML file created in STEPÂ 1.</p><p>If there are no error, you would get the below confirmation as â€œNo errors detectedâ€. Along with that it will also list down all the APIs that are present in the YAML file. (as seen in the below image). For this demo, with will select â€œNo Authenticationâ€, but if your application requires authentication you can set that up in the â€œAuthenticationâ€ dropdown.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/620/1*a1AYbiYTqah3RI1vpg6tGg.png" /></figure><p>5. Once the extension is created you would see a screen like shown below. This shows that your Chatbot is ready and you can query it in the below text box. Let us try some examples andÂ check.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VTDjZCc7YoZULPsMZN79Cw.png" /></figure><h4>Testing theÂ Chatbot:</h4><ol><li>Lets try to create a new pet in our pet store. I just put the query as â€œadd a petâ€ and it was quickly able to understand what are the required parameters to create a pet. So the bot asked me one by one to enter the name and then the id for our new pet. Once it got all the required information, it created theÂ pet.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Xj_zCG3FaOXULUAbx4gNlw.jpeg" /></figure><p>2. Letâ€™s try to get the information for our newly created pet. When I ask â€œget petâ€, it understood that there is not API to get all pets and hence asked me for an id. Once the id is entered, the chatbot was able to fetch the information for the pet we just nowÂ created.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aY_yRDGdq3PUsFbJ7COFOw.jpeg" /></figure><p>3. Letâ€™s add one more pet. When queried for â€œadd a petâ€, it asked for the name, then id and then finally the status. Notice how it gave me the options for status (available, pending or sold). This is because these are the options mentioned in the YAML file for the statusÂ field.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GAioFu8z194NXUT8eqYVnA.jpeg" /></figure><p>4. Letâ€™s try to get the new pet that we created. The Chatbot gets the complete information when the id isÂ entered.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AqY7U5u3i0Fqz3Kolsnd9Q.jpeg" /></figure><p>5. Now let us try to update the pet and see if the bot is able to handle it. So when I query â€œupdate petâ€, it asks for the id, and then asks what field i need to update, is it name or status. So when I enter â€œstatus is soldâ€, it updates that and gives the response for the successful statusÂ update.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vxeEJ46eOZUPZC9R70CPIg.jpeg" /></figure><p>6. To be actually sure if the value is updated in backend or not, I tried to get the pet infomation. But this time, in the first query itself i have the id and did a query as â€œget pet 123â€. This time the Chatbot got the id and did not ask again for the id. In the very next step, it has come back with the response saying the pet with id 123 isÂ sold.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1013/1*XnLPpJCCto9yMK2FT4YuXg.jpeg" /></figure><p>7. Finally, lets now try to delete the pet. When I query â€œdelete petâ€, the Chatbot asks me for the id, and when the id is entered it deletes that entry for pet with idÂ 123.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1010/1*4opSTPvFIBWlGFsfxmjObg.jpeg" /></figure><p>As seen above, We were able to interact with our API, perform CRUD operations (Create, Read, Update, Delete) through the Chatbot which required no training, noÂ coding!!</p><h3>Opportunities for improvement<strong>:</strong></h3><ol><li>It only supports Content Type with â€œapplication/JSONâ€ for input andÂ output.</li><li>Currently it does not work with query parameters.</li><li>There should be an option to update the YAML file once uploaded. Currently to update the YAML, you would have to delete the old Extension and create a newÂ one.</li><li>Currently it only supports calling the Chatbot through REST API or through GCP console. It would be great to have it integrated in the Vertex AIÂ SDK.</li></ol><h3>Conclusions:</h3><ol><li>It have become very easy to create a Chatbot over yourÂ APIs.</li><li>You wonâ€™t have to train the Chatbot again once new APIs areÂ added.</li><li>You wonâ€™t have to manage the context and parameter values given by theÂ user.</li></ol><h3>If you enjoyed this post, give it a clap! ğŸ‘Â ğŸ‘</h3><h3>Interested in similar content? Follow me on <a href="https://medium.com/@IVaibhavMalpani">Medium</a>, <a href="https://twitter.com/IVaibhavMalpani">Twitter</a>, <a href="https://www.linkedin.com/in/ivaibhavmalpani/">LinkedIn</a> forÂ more!</h3><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=27ad6c4a063d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/api-to-chatbot-in-less-than-5-minutes-27ad6c4a063d">API to Chatbot in less than 5 minutes | Vertex AI Extensions</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unlocking Codebase Insights with Vertex AI Gemini 1.5 Pro: A Comprehensive Guide]]></title>
            <link>https://medium.com/google-cloud/unlocking-codebase-insights-with-vertex-ai-gemini-1-5-pro-a-comprehensive-guide-1f40a6f488e9?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/1f40a6f488e9</guid>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <dc:creator><![CDATA[aakash gouda]]></dc:creator>
            <pubDate>Fri, 10 May 2024 10:16:57 GMT</pubDate>
            <atom:updated>2024-05-10T10:16:57.525Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>Vertex AI Gemini 1.5 Pro, with its remarkable 1 million token context window, revolutionizes code analysis. This advanced large language model empowers developers to gain deeper insights, streamline workflows, and enhance code quality. This blog delves into the capabilities of Gemini 1.5 Pro, demonstrating its prowess in various code-related tasks.</p><p>The tutorial utilizes the â€œOnline Boutiqueâ€ GitHub repository, a microservices demo application, as the codebase for analysis. After setting up the environment and installing necessary libraries, the code is processed to create an index and extract relevant text for Gemini 1.5 Pro toÂ analyze.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xBPBo2-MHYXAMQVJE_KNCA.png" /></figure><h3>Harnessing Gemini 1.5 Proâ€™sÂ Power</h3><p><strong>Codebase Summarization:</strong> Say goodbye to manually sifting through thousands of lines of code. Gemini 1.5 Pro can automatically generate concise and informative summaries of your codebase, highlighting key components, functionalities, and relationships. This saves you valuable time and provides a high-level overview for onboarding new developers or revisiting older projects.</p><pre>prompt = &quot;&quot;&quot;<br>  Give me a summary of this codebase, and tell me the top 3 things that I can learn from it.<br>&quot;&quot;&quot;</pre><p><strong>Developer Onboarding:</strong> Generating a comprehensive getting started guide becomes a breeze. Gemini 1.5 Pro can create step-by-step instructions and documentation, simplifying the onboarding process for new developers.</p><pre>prompt = &quot;&quot;&quot;<br>  Provide a getting started guide to onboard new developers to the codebase.<br>&quot;&quot;&quot;</pre><p><strong>Bug Detection</strong>: Uncovering critical issues is crucial for maintaining code quality. Gemini 1.5 Pro analyzes the codebase and pinpoints the top three most severe bugs, aiding developers in prioritizing fixes.</p><pre>prompt = &quot;&quot;&quot;<br>  Find the top 3 most severe issues in the codebase.<br>&quot;&quot;&quot;</pre><p><strong>Bug Fixing</strong>: Not only can Gemini 1.5 Pro identify bugs, it can also suggest code fixes. By analyzing the code and understanding the context, it offers potential solutions to resolve identified issues.</p><pre>prompt = &quot;&quot;&quot;<br>  Find the most severe bug in the codebase that you can provide a code fix for.<br>&quot;&quot;&quot;</pre><p><strong>Feature Implementation</strong>: Implementing new features can be streamlined with Gemini 1.5 Proâ€™s function-calling capabilities. By leveraging tools like GitHub issue extraction, the model can understand feature requests and generate code for their implementation.</p><pre>prompt = (<br>                &quot;Implement the following feature request&quot;<br>                + FEATURE_REQUEST_URL<br>                + &quot;\n&quot;<br>                + issue_data[&quot;body&quot;]<br>            )</pre><p><strong>Troubleshooting Guide</strong>: Creating a troubleshooting guide becomes efficient. Gemini 1.5 Pro analyzes common issues and provides step-by-step solutions, assisting developers in resolving problems effectively.</p><pre>prompt = &quot;&quot;&quot;<br>    Provide a troubleshooting guide to help resolve common issues.<br>&quot;&quot;&quot;</pre><p><strong>Reliability Enhancement:</strong> Gemini 1.5 Pro suggests best practices and improvements to enhance application reliability. By referencing external resources and analyzing code structure, it recommends strategies to ensure stable performance.</p><pre>prompt = &quot;&quot;&quot;<br>  How can I make this application more reliable? Consider best practices from https://www.r9y.dev/<br>&quot;&quot;&quot;</pre><p><strong>Security Enhancement:</strong> Security is paramount. Gemini 1.5 Pro helps identify potential vulnerabilities and recommends best practices to make the application moreÂ secure.</p><pre>prompt = &quot;&quot;&quot;<br>  How can you secure the application?<br>&quot;&quot;&quot;</pre><p><strong>Knowledge Reinforcement: </strong>Solidify your understanding of the codebase with a quiz generated by Gemini 1.5 Pro. The model creates questions based on the concepts used in the code, promoting knowledge retention and comprehension.</p><pre>prompt = &quot;&quot;&quot;<br>  Create a quiz about the concepts used in my codebase to help me solidify my understanding.<br>&quot;&quot;&quot;</pre><p><strong>Component Tutorials:</strong> Creating quickstart tutorials for specific components is simplified. Gemini 1.5 Pro provides end-to-end guidance, including configuration steps and highlighting key capabilities within the application context.</p><pre>prompt = &quot;&quot;&quot;<br>  Please write an end-to-end quickstart tutorial that introduces AlloyDB,<br>  shows how to configure it with the CartService,<br>  and highlights key capabilities of AlloyDB in context of the Online Boutique application.<br>&quot;&quot;&quot;</pre><p><strong>Git Changelog Generation:</strong> Understanding changes between Git commits is essential. Gemini 1.5 Pro analyzes Git diffs and summarizes the most important aspects of the changes, providing developers with clear insights into code evolution.</p><pre>prompt = &quot;&quot;&quot;<br>  Given the below git diff output, Summarize the important changes made.<br>&quot;&quot;&quot;</pre><h3><strong>Conclusion</strong></h3><p>Vertex AI Gemini 1.5 Pro empowers developers to analyze codebases with unprecedented efficiency and depth. From summarization and bug fixing to feature implementation and security enhancement, the model offers a comprehensive suite of capabilities to optimize workflows and enhance code quality. By leveraging Gemini 1.5 Pro, developers can unlock valuable insights and streamline the development process, ultimately building more robust and secure applications.</p><p>Special Thanks to <a href="https://medium.com/u/5644dc670e09">Eric Dong</a> for code and prompts development.</p><p>Link to <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase_with_gemini_1_5_pro.ipynb">fullÂ code</a>.</p><h3>References:</h3><ul><li><a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/</a></li><li><a href="https://cloud.google.com/vertex-ai?hl=en">https://cloud.google.com/vertex-ai?hl=en</a></li></ul><p>Thanks forÂ reading.</p><p><em>Your feedback and questions are highly appreciated. You can connect with me via </em><a href="https://www.linkedin.com/in/aakash-gouda/">LinkedIn</a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1f40a6f488e9" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/unlocking-codebase-insights-with-vertex-ai-gemini-1-5-pro-a-comprehensive-guide-1f40a6f488e9">Unlocking Codebase Insights with Vertex AI Gemini 1.5 Pro: A Comprehensive Guide</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Setup Temporary elevated access for Google Cloud with PAM]]></title>
            <link>https://medium.com/google-cloud/setup-temporary-elevated-access-for-google-cloud-with-pam-1ed98d6098b3?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/1ed98d6098b3</guid>
            <category><![CDATA[security]]></category>
            <category><![CDATA[privilege-escalation]]></category>
            <category><![CDATA[iam-roles]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-security-operations]]></category>
            <dc:creator><![CDATA[Damian Sztankowski]]></dc:creator>
            <pubDate>Fri, 10 May 2024 05:50:10 GMT</pubDate>
            <atom:updated>2024-05-10T05:50:10.847Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*K83VzvK4cZWYiMUiLK9A2w.png" /></figure><h3>Intro</h3><p>I was fighting recently with approach for granting time-based access to my organization. I didnâ€™t wanted to use 3rd party solution like CyberArk (which is super great). I wanted to utilize something cloud native, provided by vendor. I found open-community project called â€œJust-in-timeâ€. That was fantastic approach at that time. But today Iâ€™ve discovered that Google Cloud pushed into Pre-GA phase service called Privileged Access ManagerÂ (PAM)</p><p>â¡ï¸ More info about JITÂ <a href="https://cloud.google.com/architecture/manage-just-in-time-privileged-access-to-project">here</a></p><h3>Short info aboutÂ PAM</h3><p>Privileged Access Manager (PAM) is a security solution designed to manage, monitor, and secure access to privileged accounts within an organizationâ€™s IT infrastructure. These privileged accounts typically have elevated permissions and access to sensitive resources, making them a prime target for cyber attackers.</p><p>You can use PAM in following situations:</p><ul><li>As a general practice, instead of permanently granting roles. Combined with approvals and justifications, this lets you manage access to sensitive resources without resorting to permanently over-permissioned principals. You can also use PAM to audit when granted roles were active for a principal, what resources were accessible during that time, what the justification was for that access, and who approved theÂ access.</li><li>For select emergency responders, to allow them to perform critical tasks without having to wait for approval. You can require justifications for additional context on why the emergency access isÂ needed.</li><li>For service accounts, to only grant them roles when they need the roles to perform an automated task.</li><li>For contractors, to allow temporary access to resources during business hours that requires approvals and justifications.</li></ul><h3>How to enableÂ PAM</h3><p>PAM can be configured at the organization, folder, or project level. You need also mandatory permissions to do that, so before youÂ start:</p><p>Grant you the following IAM roles on the organization, folder, orÂ project:</p><ul><li>To create, update, and delete entitlements: Privileged Access Manager Admin (roles/privilegedaccessmanager.admin). Additionally, either Folder IAM Admin (roles/resourcemanager.folderIamAdmin), Project IAM Admin (roles/resourcemanager.projectIamAdmin), or Security Admin (roles/iam.securityAdmin)</li><li>To view entitlements and grants: <a href="https://cloud.google.com/iam/docs/understanding-roles#privilegedaccessmanager.viewer">Privileged Access Manager Viewer </a>(roles/privilegedaccessmanager.viewer)</li><li>To view audit logs: <a href="https://cloud.google.com/iam/docs/understanding-roles#logs.viewer">Logs Viewer </a>(roles/logs.viewer)</li></ul><p>The following permissions are required to work with entitlements andÂ grants:</p><ul><li>To enable PAM at the organization, folder, or projectÂ scope:</li><li>privilegedaccessmanager.locations.checkOnboardingStatus</li><li>resourcemanager.organizations.get</li><li>resourcemanager.organizations.getIamPolicy</li><li>resourcemanager.organizations.setIamPolicy</li><li>resourcemanager.folders.get</li><li>resourcemanager.folders.getIamPolicy</li><li>resourcemanager.folders.setIamPolicy</li><li>resourcemanager.projects.get</li><li>resourcemanager.projects.getIamPolicy</li><li>resourcemanager.projects.setIamPolicy</li><li>To manage entitlements andÂ grants:</li><li>resourcemanager.folders.get</li><li>resourcemanager.organizations.get</li><li>resourcemanager.projects.get</li><li>privilegedaccessmanager.entitlements.create</li><li>privilegedaccessmanager.entitlements.delete</li><li>privilegedaccessmanager.entitlements.get</li><li>privilegedaccessmanager.entitlements.list</li><li>privilegedaccessmanager.entitlements.setIamPolicy</li><li>privilegedaccessmanager.grants.get</li><li>privilegedaccessmanager.grants.list</li><li>privilegedaccessmanager.grants.revoke</li><li>privilegedaccessmanager.locations.get</li><li>privilegedaccessmanager.locations.list</li><li>privilegedaccessmanager.operations.delete</li><li>privilegedaccessmanager.operations.get</li><li>privilegedaccessmanager.operations.list</li><li>To view entitlements andÂ grants:</li><li>resourcemanager.folders.get</li><li>resourcemanager.organizations.get</li><li>resourcemanager.projects.get</li><li>privilegedaccessmanager.entitlements.get</li><li>privilegedaccessmanager.entitlements.list</li><li>privilegedaccessmanager.grants.get</li><li>privilegedaccessmanager.grants.list</li><li>privilegedaccessmanager.locations.get</li><li>privilegedaccessmanager.locations.list</li><li>privilegedaccessmanager.operations.get</li><li>privilegedaccessmanager.operations.list</li><li>To view audit logs: logging.logEntries.list</li></ul><p>You might also be able to get these permissions with <a href="https://cloud.google.com/iam/docs/creating-custom-roles">custom roles</a> or other <a href="https://cloud.google.com/iam/docs/understanding-roles">predefined roles</a>.</p><p>â¡ï¸ More info about managing accesses can be foundÂ <a href="https://cloud.google.com/iam/docs/granting-changing-revoking-access">here</a></p><p>Once granted, complete following steps to enableÂ PAM:</p><ol><li>Go to the <strong>Privileged Access Manager</strong>Â page.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/637/1*3guT0c9zLqp8yYEBwBY5Cw.png" /><figcaption>Fig1. PAM</figcaption></figure><p>2. Select the organization, folder, or project that you want to enable PAMÂ for.</p><p>â„¹ï¸ Enabling PAM for an organization or folder also enables PAM for all folders and projects below them in the <a href="https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy">resource hierarchy</a>.</p><p>3. Click <strong>Enable PAM</strong> to enable the service for the selected resource scope. 4. When asked to grant the <strong>Privileged Access Manager Service Agent</strong> role to the <a href="https://cloud.google.com/iam/docs/service-account-types#service-agents">Privileged Access Manager Service Agent</a> to manage privilege escalations, click <strong>GrantÂ role</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/1*AQ6v4M_B9nHjps7pIuX4pQ.png" /><figcaption>Fig2. Complete setup forÂ PAM</figcaption></figure><p>5. Click <strong>CompleteÂ setup</strong>.</p><h3>How to create entitlements</h3><p>Once PAM has been enabled, we are able to create entitlements.</p><blockquote>Entitlements can be created at the organization, folder, or project level. Roles granted by an entitlement at each level follow the Google Cloud <a href="https://cloud.google.com/iam/docs/resource-hierarchy-access-control">resource hierarchy</a>.</blockquote><blockquote>For example, roles granted by an entitlement at the organization level are inherited at the folder and projectÂ levels.</blockquote><blockquote><a href="https://cloud.google.com/iam/docs/pam-create-entitlements">https://cloud.google.com/iam/docs/pam-create-entitlements</a></blockquote><h4>Before youÂ begin</h4><p>Make sure you have <a href="https://cloud.google.com/iam/docs/pam-permissions-and-setup">enabled PAM and set up permissions forÂ it</a>.</p><p>To create entitlements complete following steps:</p><ol><li>Go to the <strong>Privileged Access Manager</strong>Â page.</li><li>Select the organization, folder, or project you want the entitlement to applyÂ to.</li><li>Click the <strong>Entitlements</strong> tab.</li><li>Click <strong>Create</strong>.</li><li>You will see following image. Fill out mandtary fields (marked with red asteriks)</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/520/1*1XzpPe38nFrZC22fzFL1NQ.png" /><figcaption>Fig3. Entitlement for SQL AdminÂ example</figcaption></figure><p>6. Add requestors. You can add email group (best approach) or individual principal</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/551/1*veB9fw1zS5wkA17cNWTk1g.png" /><figcaption>Fig4. Adding requestors to entitlement</figcaption></figure><p>7. Add approvers. You can add email group or single principal as approver. Additionally, you can configure entitlement to being approved automatically by ticking â€œActivate access without approvalsâ€ checkbox.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/537/1*2X7aj1G7S3ueS6x_YoF7UA.png" /><figcaption>Fig5. Adding approvers</figcaption></figure><p>8. You can add additional notifications emails if for example you want to notify customers also about entitlement.</p><blockquote>Add the email addresses of people to notify when the entitlement is available to request, when a grant is pending approval, and when a requester is granted access. Google identities associated with the entitlement, like approvers and requesters, are automatically notified. However, you might want to notify a different set of email addresses, especially if youâ€™re using <a href="https://cloud.google.com/iam/docs/workforce-identity-federation">Workforce Identity Federation</a>.</blockquote><blockquote><a href="https://cloud.google.com/iam/docs/pam-create-entitlements">https://cloud.google.com/iam/docs/pam-create-entitlements</a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/526/1*3P0BjHgaihkTMPIhM_gj6Q.png" /><figcaption>Fig6. Additional notifications</figcaption></figure><p>9. Click <strong>Create Entitlement</strong>.</p><p>â„¹ï¸ You can use also condition(s) to configure more restricted accesses. More infoÂ <a href="https://cloud.google.com/iam/docs/conditions-overview">here</a></p><p>10. Once done, you should get information about assignment on yourÂ email.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dpeY2waBVlggPSxP2ck0nw.png" /><figcaption>Fig7. Assignment information</figcaption></figure><p>â„¹ï¸ You are able also to configure PAM programatically. More infoÂ <a href="https://cloud.google.com/iam/docs/pam-create-entitlements#create_entitlements_programmatically">here</a></p><h3>How to useÂ PAM</h3><h4>Requestor side</h4><p>Before you begin, you must have access to project, folder or organization where access will be requested.</p><p>If you have mentioned access, complete following steps to send a request for particular entitlement viaÂ PAM.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/803/1*pxfxsnokWVDUL2g3NcdveQ.png" /><figcaption>Fig8. Lack of SQL permissions</figcaption></figure><ol><li>Go to the <strong>Privileged Access Manager</strong>Â page.</li><li>Select the organization, folder, or project you want to request a grantÂ in.</li><li>In the <strong>My entitlements</strong> tab, find the entitlement to request against, and then click <strong>Request grant</strong> in the sameÂ row.</li></ol><p>4. Provide the following details:</p><ul><li>The duration required for the grant, up to the maximum duration set on the entitlement.</li><li>If required, a justification for theÂ grant.</li><li>Optional: Which email addresses to notify of the grant request. Google identities associated with approvers are automatically notified. However, you might want to notify a different set of email addresses, especially if youâ€™re using <a href="https://cloud.google.com/iam/docs/workforce-identity-federation">Workforce Identity Federation</a>.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FW9ggjsRavYWjJw1S-Glqg.png" /><figcaption>Fig10. Providing details</figcaption></figure><p>5. Click <strong>RequestÂ grant</strong>.</p><p>6. To see your grant history including approval statuses, click the <strong>Grants</strong> tab, followed by the <strong>My grants</strong>Â tab.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YDeyOTUjvg7-LoboEOMrfQ.png" /><figcaption>Fig11 Status</figcaption></figure><h4>Approver side</h4><p>As an approver you have big power. Remember, that YOU are not able to APPROVE YOURÂ REQUEST.</p><p>To approve / deny request complete following steps:</p><ol><li>Go to the <strong>Privileged Access Manager</strong>Â page.</li><li>Click the <strong>Approve grants</strong> tab, followed by the <strong>Pending approval</strong>Â tab.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UPjtzXxBaqXz9gWqIe7d_g.png" /><figcaption>Fig12. Pending approvals</figcaption></figure><ol><li>In the row related to the request you want to approve or deny, click <strong>Approve/deny</strong>.</li><li>If a justification is required, enter it in the <strong>Comment</strong>Â field.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/562/1*_4R6jmWDMnKOWgtMD2JnpA.png" /><figcaption>Fig13. Approve / DenyÂ request</figcaption></figure><p>3. Click either <strong>Approve</strong> orÂ <strong>Deny</strong>.</p><p>4. All actions areÂ audited</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1YXnKEcUYWaX9suo--8IrQ.png" /><figcaption>Fig14. AuditÂ logs</figcaption></figure><p>Both approver and requestor receive related information about statusÂ request.</p><h3>How to revokeÂ access</h3><p>It might be possible that you will be forced to revoke particular access due to any kind ofÂ reason.</p><p>Complete following steps to revoke already grantedÂ access:</p><p>To revoke a specific grant made against an entitlement, complete the following instructions:</p><ol><li>Go to the <strong>Privileged Access Manager</strong>Â page.</li><li>Select the organization, folder, or project you want to revoke grantsÂ in.</li><li>Click the <strong>Grants</strong> tab, followed by the <strong>Grants for all users</strong> tab. This contains all grants across all requesters, the grant statuses, and their associated entitlement details.</li><li>In the table, click more_vert <strong>More options</strong> in the same row as a grant you want toÂ revoke.</li><li>To revoke an active grant, click <strong>RevokeÂ grant</strong>.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/335/1*VhP5LRBl6xX-Hlxm2XynoA.png" /><figcaption>Fig15. RevokingÂ access</figcaption></figure><p>To revoke all active grants made against an entitlement, complete the following instructions:</p><ol><li>Go to the <strong>Privileged Access Manager</strong>Â page.</li><li><a href="https://console.cloud.google.com/iam-admin/pam/entitlements/">Go to Privileged AccessÂ Manager</a></li><li>Click the <strong>Entitlements</strong> tab, followed by the <strong>Entitlements for all users</strong> tab. Here you can find the available entitlements, the roles they grant, and their valid requesters and approvers.</li><li>In the table, click more_vert <strong>More options</strong> in the same row as an entitlement you want to revoke the grantsÂ for.</li><li>Click <strong>Revoke allÂ grants</strong>.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/387/1*po1AlgzU5R0ggF7bzCeF2g.png" /><figcaption>Fig16. Revoke allÂ access</figcaption></figure><h3>Logs</h3><p>If you need provide logs for auditing purposes or such, you are able to search logs directly from Logs Explorer.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LBlIsHEegHdJA5VS7lJF2Q.png" /><figcaption>Fig17. Logs explorer forÂ PAM</figcaption></figure><p>To be able to find logs in Logs Explorer complete following steps:</p><ol><li>Go to Logs Explorer under the project where PAM has been configured</li><li>Use following query to list all logs related withÂ PAM</li></ol><pre>resource.type=&quot;audited_resource&quot;<br>resource.labels.service=&quot;privilegedaccessmanager.googleapis.com&quot;</pre><p>3. Click â€œRunÂ queryâ€</p><h3>Limitations</h3><ol><li>One and mos important information about PAM is, that service is in Pre-GA, so donâ€™t use this on production before GAÂ release.</li><li>Basic roles arenâ€™t supported.</li><li>Up to 20 roles to be granted on the organization, folder, orÂ project</li><li>Up to 20 valid requesting principals ( particular principals or groups ). Email group participants within a group is no counted in thisÂ number.</li><li>You can add up to 20 approving identities per entitlement.</li></ol><p>Do not forget the ğŸ‘âœŒï¸â¤ï¸ if you like thisÂ content!</p><p>Also, I will be glad if you hit the follow button so you get notified of my newÂ posts.</p><p>You can also follow me on <a href="https://www.linkedin.com/in/damian-sztankowski-95878b2a8/"><strong>LinkedIn</strong></a><strong>.</strong></p><p>You can join our <a href="https://www.linkedin.com/groups/9824115/"><strong>Google Cloud Community Poland</strong></a> LinkedIn group. Itâ€™sÂ open!</p><p>Thank you!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1ed98d6098b3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/setup-temporary-elevated-access-for-google-cloud-with-pam-1ed98d6098b3">Setup Temporary elevated access for Google Cloud with PAM</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Leveraging Gemini 1.5 API for Automated Test Case Generation Reverse Engineering]]></title>
            <link>https://medium.com/google-cloud/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/2ee8789f01db</guid>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[test-automation]]></category>
            <category><![CDATA[generative-ai]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Thu, 09 May 2024 09:30:13 GMT</pubDate>
            <atom:updated>2024-05-09T09:30:13.152Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*fTtML3Sm1TuQNhQP.jpg" /></figure><h3>Abstract</h3><p>This report examines leveraging Gemini 1.5 API with Google Apps Script to automate sample input creation during script reverse engineering. Traditionally, this process is manual and time-consuming, especially for functions with numerous test cases. Gemini 1.5 APIâ€™s potential to streamline development by automating input generation is explored through applying reverse engineering techniques to Google Apps ScriptÂ samples.</p><h3>Introduction</h3><p>With the release of Gemini 1.5 API, users gained the ability to process more complex data, opening doors for various application developments. This report explores the potential of using Gemini 1.5 API in conjunction with Google Apps Script to achieve reverse engineering for script development and improvement.</p><p>Traditionally, script development involves manually crafting sample input values. This process can be time-consuming, especially when creating functions or testing code retrieved from online resources like Stack Overflow. Each function might require numerous test cases, and manually generating these inputs can be a bottleneck.</p><p>Gemini 1.5 API offers a potential solution by automating sample input value creation. This could significantly reduce development time and effort. This report investigates this possibility by applying reverse engineering techniques to various Google Apps Script samples using Gemini 1.5Â API.</p><p>Here, we will explore how Gemini 1.5 API can be used to automate sample input value generation for reverse engineering scripts written in Google AppsÂ Script.</p><h3>Usage</h3><p>In order to test this script, please do the following flow.</p><h3>1. Create an APIÂ key</h3><p>Please access <a href="https://ai.google.dev/gemini-api/docs/api-key">https://ai.google.dev/gemini-api/docs/api-key</a> and create your API key. At that time, please enable Generative Language API at the API console. This API key is used for this sampleÂ script.</p><p>This official document can be also seen.Â <a href="https://ai.google.dev/">Ref</a>.</p><h3>2. Create a Google Apps ScriptÂ project</h3><p>In this report, Google Apps Script is used. Of course, the method introducing this report can be also used in other languages.</p><p>Here, in order to test the following sample scripts, please create a standalone Google Apps Script project. Of course, this script can be also used with the container-bound script.</p><p>And, please open the script editor of the Google Apps ScriptÂ project.</p><h3>3. Install Google Apps ScriptÂ library</h3><p>In order to easily access Gemini API, I created a Google Apps Script library <a href="https://github.com/tanaikech/GeminiWithFiles">GeminiWithFiles</a>. In the following sample scripts, this library is used. So, please install it. You can see how to install it atÂ <a href="https://github.com/tanaikech/GeminiWithFiles?tab=readme-ov-file#1-use-geminiwithfiles-as-a-google-apps-script-library">here</a>.</p><h3>4. Sample scriptÂ 1</h3><p>The sample functions were selected from <a href="https://github.com/tanaikech/UtlApp">my repository</a>.</p><ul><li><a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#transpose">transpose</a>: Transpose 2 dimensional array.</li><li><a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#removeduplicatedvalues">removeDuplicatedValues</a>: Remove duplicated values from 1 dimensional array.</li><li><a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#compilingnumbers">compilingNumbers</a>: Compiling Continuous Numbers using Google AppsÂ Script.</li><li><a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#unpivot">unpivot</a>: Converting 2-dimensional array as unpivot (reverseÂ pivot).</li><li><a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#expanda1notations">expandA1Notations</a>: This method is used for expanding A1Notations.</li></ul><p>The sample script demonstrating these functions is provided below. In this example, all functions can be executed in a single API call. When I ran this script, it returned a total of 2,880Â tokens.</p><p>The sample first creates input values using Gemini. To test these values, the script then uses them with the function implemented in Google Apps Script. Finally, both the input and output values areÂ printed.</p><p>JSON schema is employed here to generate content. This ensures the stable generation of complex JSON objects by Gemini. <a href="https://medium.com/google-cloud/taming-the-wild-output-effective-control-of-gemini-api-response-formats-with-response-mime-type-da273c08be85">Ref</a> As a result, I opted to use it in this instance.</p><pre>function myFunction() {<br><br>  const apiKey = &quot;###&quot;; // Please set your API key.<br><br>  const functionObj = {<br>    transpose: function transpose(array) {<br>      /**<br>       * ### Description<br>       * When the inputted array is 2 dimensional array, true is returned.<br>       *<br>       * @param {Array} array 2 dimensional array.<br>       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.<br>       */<br>      function is2DimensionalArray(array) {<br>        return array.every((r) =&gt; Array.isArray(r));<br>      }<br><br>      /**<br>       * ### Description<br>       * Transpose 2 dimensional array.<br>       *<br>       * @param {Array} array 2 dimensional array.<br>       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.<br>       * @return {Array} Transposed array.<br>       */<br>      function transpose(array, check = true) {<br>        if (check &amp;&amp; !is2DimensionalArray(array)) {<br>          throw new Error(&quot;Please use 2 dimensional array.&quot;);<br>        }<br>        return array[0].map((_, col) =&gt; array.map((row) =&gt; row[col] || null));<br>      }<br>      return transpose(array);<br>    },<br>    removeDuplicatedValues: function removeDuplicatedValues(array) {<br>      /**<br>       * ### Description<br>       * Remove duplicated values from 1 dimensional array.<br>       *<br>       * @param {Array} array 1 dimensional array.<br>       * @return {Object} Object including removeDuplicatedValues, duplicatedValues and numberOfDuplicate.<br>       */<br>      function removeDuplicatedValues(array) {<br>        if (!Array.isArray(array)) {<br>          throw new Error(&quot;Please use 1 dimensional array.&quot;);<br>        }<br>        const obj = array.reduce(<br>          (m, e) =&gt; m.set(e, m.has(e) ? m.get(e) + 1 : 1),<br>          new Map()<br>        );<br>        const e = [...obj.entries()];<br>        return {<br>          removeDuplicatedValues: [...obj.keys()],<br>          duplicatedValues: e.reduce((ar, [k, v]) =&gt; {<br>            if (v != 1) ar.push(k);<br>            return ar;<br>          }, []),<br>          numberOfDuplicate: Object.fromEntries(e),<br>        };<br>      }<br>      return removeDuplicatedValues(array);<br>    },<br>    compilingNumbers: function compilingNumbers(array) {<br>      /**<br>       * ### Description<br>       * Compiling Continuous Numbers using Google Apps Script.<br>       *<br>       * @param {Array} array Input array.<br>       * @return {Array} Array including object like [{&quot;start&quot;:1,&quot;end&quot;:1},{&quot;start&quot;:3,&quot;end&quot;:5},{&quot;start&quot;:7,&quot;end&quot;:7},{&quot;start&quot;:9,&quot;end&quot;:11},{&quot;start&quot;:13,&quot;end&quot;:13}].<br>       */<br>      function compilingNumbers(array) {<br>        if (!(Array.isArray(array) &amp;&amp; array.every((e) =&gt; !isNaN(e)))) {<br>          throw new Error(&quot;Please give an array including numbers.&quot;);<br>        }<br>        const { values } = [...new Set(array.sort((a, b) =&gt; a - b))].reduce(<br>          (o, e, i, a) =&gt; {<br>            if (<br>              o.temp.length == 0 ||<br>              (o.temp.length &gt; 0 &amp;&amp; e == o.temp[o.temp.length - 1] + 1)<br>            ) {<br>              o.temp.push(e);<br>            } else {<br>              if (o.temp.length &gt; 0) {<br>                o.values.push({<br>                  start: o.temp[0],<br>                  end: o.temp[o.temp.length - 1],<br>                });<br>              }<br>              o.temp = [e];<br>            }<br>            if (i == a.length - 1) {<br>              o.values.push(<br>                o.temp.length &gt; 1<br>                  ? { start: o.temp[0], end: o.temp[o.temp.length - 1] }<br>                  : { start: e, end: e }<br>              );<br>            }<br>            return o;<br>          },<br>          { temp: [], values: [] }<br>        );<br>        return values;<br>      }<br>      return compilingNumbers(array);<br>    },<br>    unpivot: function unpivot(values) {<br>      /**<br>       * ### Description<br>       * When the inputted array is 2 dimensional array, true is returned.<br>       *<br>       * @param {Array} array 2 dimensional array.<br>       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.<br>       */<br>      function is2DimensionalArray(array) {<br>        return array.every((r) =&gt; Array.isArray(r));<br>      }<br><br>      /**<br>       * ### Description<br>       * Converting 2-dimensional array as unpivot (reverse pivot).<br>       *<br>       * @param {Array} values 2 dimensional array.<br>       * @return {Array} 2 dimensional array converted as unpivot (reverse pivot).<br>       */<br>      function unpivot(values) {<br>        if (!Array.isArray(values) || !is2DimensionalArray(values)) {<br>          throw new Error(&quot;Please give an array of values.&quot;);<br>        }<br>        const [[, ...h], ...v] = values;<br>        return h.flatMap((hh, i) =&gt; v.map((t) =&gt; [hh, t[0], t[i + 1]]));<br>      }<br>      return unpivot(values);<br>    },<br>    expandA1Notations: function expandA1Notations(a1Notations) {<br>      /**<br>       * ### Description<br>       * Converting colum letter to column index. Start of column index is 0.<br>       * @param {String} letter Column letter.<br>       * @return {Number} Column index.<br>       */<br>      function columnLetterToIndex(letter = null) {<br>        if (letter === null || typeof letter != &quot;string&quot;) {<br>          throw new Error(&quot;Please give the column letter as a string.&quot;);<br>        }<br>        letter = letter.toUpperCase();<br>        return [...letter].reduce(<br>          (c, e, i, a) =&gt;<br>            (c += (e.charCodeAt(0) - 64) * Math.pow(26, a.length - i - 1)),<br>          -1<br>        );<br>      }<br><br>      /**<br>       * ### Description<br>       * Converting colum index to column letter. Start of column index is 0.<br>       * Ref: https://stackoverflow.com/a/53678158/7108653<br>       * @param {Number} index Column index.<br>       * @return {String} Column letter.<br>       */<br>      function columnIndexToLetter(index = null) {<br>        if (index === null || isNaN(index)) {<br>          throw new Error(<br>            &quot;Please give the column indexr as a number. In this case, 1st number is 0.&quot;<br>          );<br>        }<br>        return (a = Math.floor(index / 26)) &gt;= 0<br>          ? columnIndexToLetter(a - 1) + String.fromCharCode(65 + (index % 26))<br>          : &quot;&quot;;<br>      }<br><br>      /**<br>       * ### Description<br>       * This method is used for expanding A1Notations.<br>       * @param {Array} a1Notations Array including A1Notations.<br>       * @return {Array} Array including the expanded A1Notations.<br>       */<br>      function expandA1Notations(a1Notations, maxRow = &quot;10&quot;, maxColumn = &quot;Z&quot;) {<br>        if (!Array.isArray(a1Notations) || a1Notations.length == 0) {<br>          throw new Error(&quot;Please give a1Notations (Array).&quot;);<br>        }<br>        const reg1 = new RegExp(&quot;^([A-Z]+)([0-9]+)$&quot;);<br>        const reg2 = new RegExp(&quot;^([A-Z]+)$&quot;);<br>        const reg3 = new RegExp(&quot;^([0-9]+)$&quot;);<br>        return a1Notations.map((e) =&gt; {<br>          const a1 = e.split(&quot;!&quot;);<br>          const r = a1.length &gt; 1 ? a1[1] : a1[0];<br>          const [r1, r2] = r.split(&quot;:&quot;);<br>          if (!r2) return [r1];<br>          let rr;<br>          if (reg1.test(r1) &amp;&amp; reg1.test(r2)) {<br>            rr = [r1.toUpperCase().match(reg1), r2.toUpperCase().match(reg1)];<br>          } else if (reg2.test(r1) &amp;&amp; reg2.test(r2)) {<br>            rr = [<br>              [null, r1, 1],<br>              [null, r2, maxRow],<br>            ];<br>          } else if (reg1.test(r1) &amp;&amp; reg2.test(r2)) {<br>            rr = [r1.toUpperCase().match(reg1), [null, r2, maxRow]];<br>          } else if (reg2.test(r1) &amp;&amp; reg1.test(r2)) {<br>            rr = [[null, r1, maxRow], r2.toUpperCase().match(reg1)];<br>          } else if (reg3.test(r1) &amp;&amp; reg3.test(r2)) {<br>            rr =<br>              Number(r1) &gt; Number(r2)<br>                ? [<br>                    [null, &quot;A&quot;, r2],<br>                    [null, maxColumn, r1],<br>                  ]<br>                : [<br>                    [null, &quot;A&quot;, r1],<br>                    [null, maxColumn, r2],<br>                  ];<br>          } else if (reg1.test(r1) &amp;&amp; reg3.test(r2)) {<br>            rr = [r1.toUpperCase().match(reg1), [null, maxColumn, r2]];<br>          } else if (reg3.test(r1) &amp;&amp; reg1.test(r2)) {<br>            let temp = r2.toUpperCase().match(reg1);<br>            rr =<br>              Number(temp[2]) &gt; Number(r1)<br>                ? [<br>                    [null, temp[1], r1],<br>                    [null, maxColumn, temp[2]],<br>                  ]<br>                : [temp, [null, maxColumn, r1]];<br>          } else {<br>            throw new Error(&quot;Wrong a1Notation: &quot; + r);<br>          }<br>          const obj = {<br>            startRowIndex: Number(rr[0][2]),<br>            endRowIndex:<br>              rr.length == 1 ? Number(rr[0][2]) + 1 : Number(rr[1][2]) + 1,<br>            startColumnIndex: columnLetterToIndex(rr[0][1]),<br>            endColumnIndex:<br>              rr.length == 1<br>                ? columnLetterToIndex(rr[0][1]) + 1<br>                : columnLetterToIndex(rr[1][1]) + 1,<br>          };<br>          let temp = [];<br>          for (let i = obj.startRowIndex; i &lt; obj.endRowIndex; i++) {<br>            for (let j = obj.startColumnIndex; j &lt; obj.endColumnIndex; j++) {<br>              temp.push(columnIndexToLetter(j) + i);<br>            }<br>          }<br>          return temp;<br>        });<br>      }<br>      return expandA1Notations(a1Notations);<br>    },<br>  };<br><br>  const g = GeminiWithFiles.geminiWithFiles({<br>    apiKey,<br>    response_mime_type: &quot;application/json&quot;,<br>    doCountToken: true,<br>  });<br><br>  const functions = Object.entries(functionObj)<br>    .map(<br>      ([k, v]) =&gt;<br>        `&lt;FunctionName&gt;${k}&lt;/FunctionName&gt;&lt;Function&gt;${v.toString()}&lt;/Function&gt;`<br>    )<br>    .join(&quot;&quot;);<br>  const jsonSchema = {<br>    title: &quot;5 input values for giving each function&quot;,<br>    description: `Proposal 5 input values for giving each function. ${functions} Don&#39;t propose &quot;empty&quot;, &quot;null&quot;, &quot;undefined&quot; as values.`,<br>    type: &quot;array&quot;,<br>    items: {<br>      type: &quot;object&quot;,<br>      properties: {<br>        functionName: { description: &quot;Function name&quot;, type: &quot;string&quot; },<br>        inputValues: {<br>          description: `Proposed 5 input values. Don&#39;t propose &quot;empty&quot;, &quot;null&quot;, &quot;undefined&quot; as values.`,<br>          type: &quot;array&quot;,<br>          items: {<br>            description: &quot;Proposed input value&quot;,<br>            type: &quot;array|object|string|number&quot;,<br>          },<br>        },<br>      },<br>      additionalProperties: false,<br>    },<br>  };<br>  let res = g.generateContent({ jsonSchema });<br>  if (typeof res == &quot;string&quot;) {<br>    try {<br>      res = JSON.parse(res);<br>    } catch ({ stack }) {<br>      console.error(stack);<br>      return;<br>    }<br>  }<br>  const result = res.reduce((o, { functionName, inputValues }) =&gt; {<br>    try {<br>      o[functionName] = [];<br>      inputValues.forEach((input) =&gt; {<br>        const output = functionObj[functionName](input);<br>        o[functionName].push({ input, output });<br>      });<br>    } catch ({ stack }) {<br>      console.log(stack);<br>    }<br>    return o;<br>  }, {});<br>  console.log(JSON.stringify(result));<br>}</pre><p>When this script is run, the following result is obtained. You can see that valid input and output values areÂ created.</p><pre>{<br>  &quot;transpose&quot;: [<br>    { &quot;input&quot;: [[1, 2], [3, 4]], &quot;output&quot;: [[1, 3], [2, 4]] },<br>    { &quot;input&quot;: [[&quot;a&quot;, &quot;b&quot;], [&quot;c&quot;, &quot;d&quot;]], &quot;output&quot;: [[&quot;a&quot;, &quot;c&quot;], [&quot;b&quot;, &quot;d&quot;]] },<br>    { &quot;input&quot;: [[&quot;a1&quot;, &quot;b1&quot;], [&quot;c1&quot;, &quot;d1&quot;], [&quot;e1&quot;, &quot;f1&quot;]], &quot;output&quot;: [[&quot;a1&quot;, &quot;c1&quot;, &quot;e1&quot;], [&quot;b1&quot;, &quot;d1&quot;, &quot;f1&quot;]] },<br>    { &quot;input&quot;: [[true, false], [false, true]], &quot;output&quot;: [[true, null], [null, true]] },<br>    { &quot;input&quot;: [[1, &quot;a&quot;], [&quot;c&quot;, true]], &quot;output&quot;: [[1, &quot;c&quot;], [&quot;a&quot;, true]] }<br>  ],<br><br>  &quot;removeDuplicatedValues&quot;: [<br>    { &quot;input&quot;: [1, 2, 3, 4, 5], &quot;output&quot;: { &quot;removeDuplicatedValues&quot;: [1, 2, 3, 4, 5], &quot;duplicatedValues&quot;: [], &quot;numberOfDuplicate&quot;: { &quot;1&quot;: 1, &quot;2&quot;: 1, &quot;3&quot;: 1, &quot;4&quot;: 1, &quot;5&quot;: 1 } } },<br>    { &quot;input&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;], &quot;output&quot;: { &quot;removeDuplicatedValues&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;], &quot;duplicatedValues&quot;: [], &quot;numberOfDuplicate&quot;: { &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1, &quot;d&quot;: 1, &quot;e&quot;: 1 } } },<br>    { &quot;input&quot;: [1, 2, 1, 3, 2, 4, 3, 5, 4], &quot;output&quot;: { &quot;removeDuplicatedValues&quot;: [1, 2, 3, 4, 5], &quot;duplicatedValues&quot;: [1, 2, 3, 4], &quot;numberOfDuplicate&quot;: { &quot;1&quot;: 2, &quot;2&quot;: 2, &quot;3&quot;: 2, &quot;4&quot;: 2, &quot;5&quot;: 1 } } },<br>    { &quot;input&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;, &quot;b&quot;, &quot;d&quot;, &quot;c&quot;, &quot;e&quot;, &quot;d&quot;], &quot;output&quot;: { &quot;removeDuplicatedValues&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;], &quot;duplicatedValues&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;], &quot;numberOfDuplicate&quot;: { &quot;a&quot;: 2, &quot;b&quot;: 2, &quot;c&quot;: 2, &quot;d&quot;: 2, &quot;e&quot;: 1 } } },<br>    { &quot;input&quot;: [1, &quot;a&quot;, 2, &quot;b&quot;, 1, &quot;c&quot;, 2, &quot;d&quot;, 1, &quot;e&quot;], &quot;output&quot;: { &quot;removeDuplicatedValues&quot;: [1, &quot;a&quot;, 2, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;], &quot;duplicatedValues&quot;: [1, 2], &quot;numberOfDuplicate&quot;: { &quot;1&quot;: 3, &quot;2&quot;: 2, &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1, &quot;d&quot;: 1, &quot;e&quot;: 1 } } }<br>  ],<br><br>  &quot;compilingNumbers&quot;: [<br>    { &quot;input&quot;: [1, 2, 3, 4, 5], &quot;output&quot;: [{ &quot;start&quot;: 1, &quot;end&quot;: 5 }] },<br>    { &quot;input&quot;: [1, 3, 5, 7, 9, 11, 13], &quot;output&quot;: [{ &quot;start&quot;: 1, &quot;end&quot;: 1 }, { &quot;start&quot;: 3, &quot;end&quot;: 3 }, { &quot;start&quot;: 5, &quot;end&quot;: 5 }, { &quot;start&quot;: 7, &quot;end&quot;: 7 }, { &quot;start&quot;: 9, &quot;end&quot;: 9 }, { &quot;start&quot;: 11, &quot;end&quot;: 11 }, { &quot;start&quot;: 13, &quot;end&quot;: 13 }] },<br>    { &quot;input&quot;: [1, 3, 5, 7, 8, 10, 12, 13], &quot;output&quot;: [{ &quot;start&quot;: 1, &quot;end&quot;: 1 }, { &quot;start&quot;: 3, &quot;end&quot;: 3 }, { &quot;start&quot;: 5, &quot;end&quot;: 5 }, { &quot;start&quot;: 7, &quot;end&quot;: 8 }, { &quot;start&quot;: 10, &quot;end&quot;: 10 }, { &quot;start&quot;: 12, &quot;end&quot;: 13 }] },<br>    { &quot;input&quot;: [1, 2, 4, 5, 7, 8, 10, 11, 13, 14], &quot;output&quot;: [{ &quot;start&quot;: 1, &quot;end&quot;: 2 }, { &quot;start&quot;: 4, &quot;end&quot;: 5 }, { &quot;start&quot;: 7, &quot;end&quot;: 8 }, { &quot;start&quot;: 10, &quot;end&quot;: 11 }, { &quot;start&quot;: 13, &quot;end&quot;: 14 }] },<br>    { &quot;input&quot;: [1, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15], &quot;output&quot;: [{ &quot;start&quot;: 1, &quot;end&quot;: 3 }, { &quot;start&quot;: 5, &quot;end&quot;: 6 }, { &quot;start&quot;: 8, &quot;end&quot;: 9 }, { &quot;start&quot;: 11, &quot;end&quot;: 12 }, { &quot;start&quot;: 14, &quot;end&quot;: 15 }] }<br>  ],<br><br>  &quot;unpivot&quot;: [<br>    { &quot;input&quot;: [[&quot;name&quot;, &quot;score1&quot;, &quot;score2&quot;], [&quot;sample1&quot;, 100, 80], [&quot;sample2&quot;, 90, 70]], &quot;output&quot;: [[&quot;score1&quot;, &quot;sample1&quot;, 100], [&quot;score1&quot;, &quot;sample2&quot;, 90], [&quot;score2&quot;, &quot;sample1&quot;, 80], [&quot;score2&quot;, &quot;sample2&quot;, 70]] },<br>    { &quot;input&quot;: [[&quot;name&quot;, &quot;score1&quot;, &quot;score2&quot;, &quot;score3&quot;], [&quot;sample1&quot;, 100, 80, 70], [&quot;sample2&quot;, 90, 70, 80]], &quot;output&quot;: [[&quot;score1&quot;, &quot;sample1&quot;, 100], [&quot;score1&quot;, &quot;sample2&quot;, 90], [&quot;score2&quot;, &quot;sample1&quot;, 80], [&quot;score2&quot;, &quot;sample2&quot;, 70], [&quot;score3&quot;, &quot;sample1&quot;, 70], [&quot;score3&quot;, &quot;sample2&quot;, 80]] },<br>    { &quot;input&quot;: [[&quot;id&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;], [&quot;a&quot;, 1, 2, 3], [&quot;b&quot;, 4, 5, 6]], &quot;output&quot;: [[&quot;x&quot;, &quot;a&quot;, 1], [&quot;x&quot;, &quot;b&quot;, 4], [&quot;y&quot;, &quot;a&quot;, 2], [&quot;y&quot;, &quot;b&quot;, 5], [&quot;z&quot;, &quot;a&quot;, 3], [&quot;z&quot;, &quot;b&quot;, 6]] },<br>    { &quot;input&quot;: [[&quot;id&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;xx&quot;, &quot;yy&quot;, &quot;zz&quot;], [&quot;a&quot;, 1, 2, 3, 10, 20, 30], [&quot;b&quot;, 4, 5, 6, 40, 50, 60]], &quot;output&quot;: [[&quot;x&quot;, &quot;a&quot;, 1], [&quot;x&quot;, &quot;b&quot;, 4], [&quot;y&quot;, &quot;a&quot;, 2], [&quot;y&quot;, &quot;b&quot;, 5], [&quot;z&quot;, &quot;a&quot;, 3], [&quot;z&quot;, &quot;b&quot;, 6], [&quot;xx&quot;, &quot;a&quot;, 10], [&quot;xx&quot;, &quot;b&quot;, 40], [&quot;yy&quot;, &quot;a&quot;, 20], [&quot;yy&quot;, &quot;b&quot;, 50], [&quot;zz&quot;, &quot;a&quot;, 30], [&quot;zz&quot;, &quot;b&quot;, 60]] },<br>    { &quot;input&quot;: [[&quot;Fruit&quot;, &quot;2021&quot;, &quot;2022&quot;, &quot;2023&quot;], [&quot;apple&quot;, 100, 120, 150], [&quot;orange&quot;, 80, 90, 100]], &quot;output&quot;: [[&quot;2021&quot;, &quot;apple&quot;, 100], [&quot;2021&quot;, &quot;orange&quot;, 80], [&quot;2022&quot;, &quot;apple&quot;, 120], [&quot;2022&quot;, &quot;orange&quot;, 90], [&quot;2023&quot;, &quot;apple&quot;, 150], [&quot;2023&quot;, &quot;orange&quot;, 100]] }<br>  ],<br><br>  &quot;expandA1Notations&quot;: [<br>    { &quot;input&quot;: [&quot;A1:B5&quot;, &quot;C3:D7&quot;, &quot;E2:F10&quot;], &quot;output&quot;: [[&quot;A1&quot;, &quot;B1&quot;, &quot;A2&quot;, &quot;B2&quot;, &quot;A3&quot;, &quot;B3&quot;, &quot;A4&quot;, &quot;B4&quot;, &quot;A5&quot;, &quot;B5&quot;], [&quot;C3&quot;, &quot;D3&quot;, &quot;C4&quot;, &quot;D4&quot;, &quot;C5&quot;, &quot;D5&quot;, &quot;C6&quot;, &quot;D6&quot;, &quot;C7&quot;, &quot;D7&quot;], [&quot;E2&quot;, &quot;F2&quot;, &quot;E3&quot;, &quot;F3&quot;, &quot;E4&quot;, &quot;F4&quot;, &quot;E5&quot;, &quot;F5&quot;, &quot;E6&quot;, &quot;F6&quot;, &quot;E7&quot;, &quot;F7&quot;, &quot;E8&quot;, &quot;F8&quot;, &quot;E9&quot;, &quot;F9&quot;, &quot;E10&quot;, &quot;F10&quot;]] },<br>    { &quot;input&quot;: [&quot;A:B&quot;, &quot;C:D&quot;, &quot;E:F&quot;], &quot;output&quot;: [[&quot;A1&quot;, &quot;B1&quot;, &quot;A2&quot;, &quot;B2&quot;, &quot;A3&quot;, &quot;B3&quot;, &quot;A4&quot;, &quot;B4&quot;, &quot;A5&quot;, &quot;B5&quot;, &quot;A6&quot;, &quot;B6&quot;, &quot;A7&quot;, &quot;B7&quot;, &quot;A8&quot;, &quot;B8&quot;, &quot;A9&quot;, &quot;B9&quot;, &quot;A10&quot;, &quot;B10&quot;], [&quot;C1&quot;, &quot;D1&quot;, &quot;C2&quot;, &quot;D2&quot;, &quot;C3&quot;, &quot;D3&quot;, &quot;C4&quot;, &quot;D4&quot;, &quot;C5&quot;, &quot;D5&quot;, &quot;C6&quot;, &quot;D6&quot;, &quot;C7&quot;, &quot;D7&quot;, &quot;C8&quot;, &quot;D8&quot;, &quot;C9&quot;, &quot;D9&quot;, &quot;C10&quot;, &quot;D10&quot;], [&quot;E1&quot;, &quot;F1&quot;, &quot;E2&quot;, &quot;F2&quot;, &quot;E3&quot;, &quot;F3&quot;, &quot;E4&quot;, &quot;F4&quot;, &quot;E5&quot;, &quot;F5&quot;, &quot;E6&quot;, &quot;F6&quot;, &quot;E7&quot;, &quot;F7&quot;, &quot;E8&quot;, &quot;F8&quot;, &quot;E9&quot;, &quot;F9&quot;, &quot;E10&quot;, &quot;F10&quot;]] },<br>    { &quot;input&quot;: [&quot;A1:C5&quot;], &quot;output&quot;: [[&quot;A1&quot;, &quot;B1&quot;, &quot;C1&quot;, &quot;A2&quot;, &quot;B2&quot;, &quot;C2&quot;, &quot;A3&quot;, &quot;B3&quot;, &quot;C3&quot;, &quot;A4&quot;, &quot;B4&quot;, &quot;C4&quot;, &quot;A5&quot;, &quot;B5&quot;, &quot;C5&quot;]] },<br>    { &quot;input&quot;: [&quot;A:C&quot;], &quot;output&quot;: [[&quot;A1&quot;, &quot;B1&quot;, &quot;C1&quot;, &quot;A2&quot;, &quot;B2&quot;, &quot;C2&quot;, &quot;A3&quot;, &quot;B3&quot;, &quot;C3&quot;, &quot;A4&quot;, &quot;B4&quot;, &quot;C4&quot;, &quot;A5&quot;, &quot;B5&quot;, &quot;C5&quot;, &quot;A6&quot;, &quot;B6&quot;, &quot;C6&quot;, &quot;A7&quot;, &quot;B7&quot;, &quot;C7&quot;, &quot;A8&quot;, &quot;B8&quot;, &quot;C8&quot;, &quot;A9&quot;, &quot;B9&quot;, &quot;C9&quot;, &quot;A10&quot;, &quot;B10&quot;, &quot;C10&quot;]] },<br>    { &quot;input&quot;: [&quot;1:5&quot;, &quot;3:7&quot;, &quot;2:10&quot;], &quot;output&quot;: [[&quot;A1&quot;, &quot;B1&quot;, &quot;C1&quot;, &quot;D1&quot;, &quot;E1&quot;, &quot;F1&quot;, &quot;G1&quot;, &quot;H1&quot;, &quot;I1&quot;, &quot;J1&quot;, &quot;K1&quot;, &quot;L1&quot;, &quot;M1&quot;, &quot;N1&quot;, &quot;O1&quot;, &quot;P1&quot;, &quot;Q1&quot;, &quot;R1&quot;, &quot;S1&quot;, &quot;T1&quot;, &quot;U1&quot;, &quot;V1&quot;, &quot;W1&quot;, &quot;X1&quot;, &quot;Y1&quot;, &quot;Z1&quot;, &quot;A2&quot;, &quot;B2&quot;, &quot;C2&quot;, &quot;D2&quot;, &quot;E2&quot;, &quot;F2&quot;, &quot;G2&quot;, &quot;H2&quot;, &quot;I2&quot;, &quot;J2&quot;, &quot;K2&quot;, &quot;L2&quot;, &quot;M2&quot;, &quot;N2&quot;, &quot;O2&quot;, &quot;P2&quot;, &quot;Q2&quot;, &quot;R2&quot;, &quot;S2&quot;, &quot;T2&quot;, &quot;U2&quot;, &quot;V2&quot;, &quot;W2&quot;, &quot;X2&quot;, &quot;Y2&quot;, &quot;Z2&quot;, &quot;A3&quot;, &quot;B3&quot;, &quot;C3&quot;, &quot;D3&quot;, &quot;E3&quot;, &quot;F3&quot;, &quot;G3&quot;, &quot;H3&quot;, &quot;I3&quot;, &quot;J3&quot;, &quot;K3&quot;, &quot;L3&quot;, &quot;M3&quot;, &quot;N3&quot;, &quot;O3&quot;, &quot;P3&quot;, &quot;Q3&quot;, &quot;R3&quot;, &quot;S3&quot;, &quot;T3&quot;, &quot;U3&quot;, &quot;V3&quot;, &quot;W3&quot;, &quot;X3&quot;, &quot;Y3&quot;, &quot;Z3&quot;, &quot;A4&quot;, &quot;B4&quot;, &quot;C4&quot;, &quot;D4&quot;, &quot;E4&quot;, &quot;F4&quot;, &quot;G4&quot;, &quot;H4&quot;, &quot;I4&quot;, &quot;J4&quot;, &quot;K4&quot;, &quot;L4&quot;, &quot;M4&quot;, &quot;N4&quot;, &quot;O4&quot;, &quot;P4&quot;, &quot;Q4&quot;, &quot;R4&quot;, &quot;S4&quot;, &quot;T4&quot;, &quot;U4&quot;, &quot;V4&quot;, &quot;W4&quot;, &quot;X4&quot;, &quot;Y4&quot;, &quot;Z4&quot;, &quot;A5&quot;, &quot;B5&quot;, &quot;C5&quot;, &quot;D5&quot;, &quot;E5&quot;, &quot;F5&quot;, &quot;G5&quot;, &quot;H5&quot;, &quot;I5&quot;, &quot;J5&quot;, &quot;K5&quot;, &quot;L5&quot;, &quot;M5&quot;, &quot;N5&quot;, &quot;O5&quot;, &quot;P5&quot;, &quot;Q5&quot;, &quot;R5&quot;, &quot;S5&quot;, &quot;T5&quot;, &quot;U5&quot;, &quot;V5&quot;, &quot;W5&quot;, &quot;X5&quot;, &quot;Y5&quot;, &quot;Z5&quot;], [&quot;A3&quot;, &quot;B3&quot;, &quot;C3&quot;, &quot;D3&quot;, &quot;E3&quot;, &quot;F3&quot;, &quot;G3&quot;, &quot;H3&quot;, &quot;I3&quot;, &quot;J3&quot;, &quot;K3&quot;, &quot;L3&quot;, &quot;M3&quot;, &quot;N3&quot;, &quot;O3&quot;, &quot;P3&quot;, &quot;Q3&quot;, &quot;R3&quot;, &quot;S3&quot;, &quot;T3&quot;, &quot;U3&quot;, &quot;V3&quot;, &quot;W3&quot;, &quot;X3&quot;, &quot;Y3&quot;, &quot;Z3&quot;, &quot;A4&quot;, &quot;B4&quot;, &quot;C4&quot;, &quot;D4&quot;, &quot;E4&quot;, &quot;F4&quot;, &quot;G4&quot;, &quot;H4&quot;, &quot;I4&quot;, &quot;J4&quot;, &quot;K4&quot;, &quot;L4&quot;, &quot;M4&quot;, &quot;N4&quot;, &quot;O4&quot;, &quot;P4&quot;, &quot;Q4&quot;, &quot;R4&quot;, &quot;S4&quot;, &quot;T4&quot;, &quot;U4&quot;, &quot;V4&quot;, &quot;W4&quot;, &quot;X4&quot;, &quot;Y4&quot;, &quot;Z4&quot;, &quot;A5&quot;, &quot;B5&quot;, &quot;C5&quot;, &quot;D5&quot;, &quot;E5&quot;, &quot;F5&quot;, &quot;G5&quot;, &quot;H5&quot;, &quot;I5&quot;, &quot;J5&quot;, &quot;K5&quot;, &quot;L5&quot;, &quot;M5&quot;, &quot;N5&quot;, &quot;O5&quot;, &quot;P5&quot;, &quot;Q5&quot;, &quot;R5&quot;, &quot;S5&quot;, &quot;T5&quot;, &quot;U5&quot;, &quot;V5&quot;, &quot;W5&quot;, &quot;X5&quot;, &quot;Y5&quot;, &quot;Z5&quot;, &quot;A6&quot;, &quot;B6&quot;, &quot;C6&quot;, &quot;D6&quot;, &quot;E6&quot;, &quot;F6&quot;, &quot;G6&quot;, &quot;H6&quot;, &quot;I6&quot;, &quot;J6&quot;, &quot;K6&quot;, &quot;L6&quot;, &quot;M6&quot;, &quot;N6&quot;, &quot;O6&quot;, &quot;P6&quot;, &quot;Q6&quot;, &quot;R6&quot;, &quot;S6&quot;, &quot;T6&quot;, &quot;U6&quot;, &quot;V6&quot;, &quot;W6&quot;, &quot;X6&quot;, &quot;Y6&quot;, &quot;Z6&quot;, &quot;A7&quot;, &quot;B7&quot;, &quot;C7&quot;, &quot;D7&quot;, &quot;E7&quot;, &quot;F7&quot;, &quot;G7&quot;, &quot;H7&quot;, &quot;I7&quot;, &quot;J7&quot;, &quot;K7&quot;, &quot;L7&quot;, &quot;M7&quot;, &quot;N7&quot;, &quot;O7&quot;, &quot;P7&quot;, &quot;Q7&quot;, &quot;R7&quot;, &quot;S7&quot;, &quot;T7&quot;, &quot;U7&quot;, &quot;V7&quot;, &quot;W7&quot;, &quot;X7&quot;, &quot;Y7&quot;, &quot;Z7&quot;], [&quot;A2&quot;, &quot;B2&quot;, &quot;C2&quot;, &quot;D2&quot;, &quot;E2&quot;, &quot;F2&quot;, &quot;G2&quot;, &quot;H2&quot;, &quot;I2&quot;, &quot;J2&quot;, &quot;K2&quot;, &quot;L2&quot;, &quot;M2&quot;, &quot;N2&quot;, &quot;O2&quot;, &quot;P2&quot;, &quot;Q2&quot;, &quot;R2&quot;, &quot;S2&quot;, &quot;T2&quot;, &quot;U2&quot;, &quot;V2&quot;, &quot;W2&quot;, &quot;X2&quot;, &quot;Y2&quot;, &quot;Z2&quot;, &quot;A3&quot;, &quot;B3&quot;, &quot;C3&quot;, &quot;D3&quot;, &quot;E3&quot;, &quot;F3&quot;, &quot;G3&quot;, &quot;H3&quot;, &quot;I3&quot;, &quot;J3&quot;, &quot;K3&quot;, &quot;L3&quot;, &quot;M3&quot;, &quot;N3&quot;, &quot;O3&quot;, &quot;P3&quot;, &quot;Q3&quot;, &quot;R3&quot;, &quot;S3&quot;, &quot;T3&quot;, &quot;U3&quot;, &quot;V3&quot;, &quot;W3&quot;, &quot;X3&quot;, &quot;Y3&quot;, &quot;Z3&quot;, &quot;A4&quot;, &quot;B4&quot;, &quot;C4&quot;, &quot;D4&quot;, &quot;E4&quot;, &quot;F4&quot;, &quot;G4&quot;, &quot;H4&quot;, &quot;I4&quot;, &quot;J4&quot;, &quot;K4&quot;, &quot;L4&quot;, &quot;M4&quot;, &quot;N4&quot;, &quot;O4&quot;, &quot;P4&quot;, &quot;Q4&quot;, &quot;R4&quot;, &quot;S4&quot;, &quot;T4&quot;, &quot;U4&quot;, &quot;V4&quot;, &quot;W4&quot;, &quot;X4&quot;, &quot;Y4&quot;, &quot;Z4&quot;, &quot;A5&quot;, &quot;B5&quot;, &quot;C5&quot;, &quot;D5&quot;, &quot;E5&quot;, &quot;F5&quot;, &quot;G5&quot;, &quot;H5&quot;, &quot;I5&quot;, &quot;J5&quot;, &quot;K5&quot;, &quot;L5&quot;, &quot;M5&quot;, &quot;N5&quot;, &quot;O5&quot;, &quot;P5&quot;, &quot;Q5&quot;, &quot;R5&quot;, &quot;S5&quot;, &quot;T5&quot;, &quot;U5&quot;, &quot;V5&quot;, &quot;W5&quot;, &quot;X5&quot;, &quot;Y5&quot;, &quot;Z5&quot;, &quot;A6&quot;, &quot;B6&quot;, &quot;C6&quot;, &quot;D6&quot;, &quot;E6&quot;, &quot;F6&quot;, &quot;G6&quot;, &quot;H6&quot;, &quot;I6&quot;, &quot;J6&quot;, &quot;K6&quot;, &quot;L6&quot;, &quot;M6&quot;, &quot;N6&quot;, &quot;O6&quot;, &quot;P6&quot;, &quot;Q6&quot;, &quot;R6&quot;, &quot;S6&quot;, &quot;T6&quot;, &quot;U6&quot;, &quot;V6&quot;, &quot;W6&quot;, &quot;X6&quot;, &quot;Y6&quot;, &quot;Z6&quot;, &quot;A7&quot;, &quot;B7&quot;, &quot;C7&quot;, &quot;D7&quot;, &quot;E7&quot;, &quot;F7&quot;, &quot;G7&quot;, &quot;H7&quot;, &quot;I7&quot;, &quot;J7&quot;, &quot;K7&quot;, &quot;L7&quot;, &quot;M7&quot;, &quot;N7&quot;, &quot;O7&quot;, &quot;P7&quot;, &quot;Q7&quot;, &quot;R7&quot;, &quot;S7&quot;, &quot;T7&quot;, &quot;U7&quot;, &quot;V7&quot;, &quot;W7&quot;, &quot;X7&quot;, &quot;Y7&quot;, &quot;Z7&quot;, &quot;A8&quot;, &quot;B8&quot;, &quot;C8&quot;, &quot;D8&quot;, &quot;E8&quot;, &quot;F8&quot;, &quot;G8&quot;, &quot;H8&quot;, &quot;I8&quot;, &quot;J8&quot;, &quot;K8&quot;, &quot;L8&quot;, &quot;M8&quot;, &quot;N8&quot;, &quot;O8&quot;, &quot;P8&quot;, &quot;Q8&quot;, &quot;R8&quot;, &quot;S8&quot;, &quot;T8&quot;, &quot;U8&quot;, &quot;V8&quot;, &quot;W8&quot;, &quot;X8&quot;, &quot;Y8&quot;, &quot;Z8&quot;, &quot;A9&quot;, &quot;B9&quot;, &quot;C9&quot;, &quot;D9&quot;, &quot;E9&quot;, &quot;F9&quot;, &quot;G9&quot;, &quot;H9&quot;, &quot;I9&quot;, &quot;J9&quot;, &quot;K9&quot;, &quot;L9&quot;, &quot;M9&quot;, &quot;N9&quot;, &quot;O9&quot;, &quot;P9&quot;, &quot;Q9&quot;, &quot;R9&quot;, &quot;S9&quot;, &quot;T9&quot;, &quot;U9&quot;, &quot;V9&quot;, &quot;W9&quot;, &quot;X9&quot;, &quot;Y9&quot;, &quot;Z9&quot;, &quot;A10&quot;, &quot;B10&quot;, &quot;C10&quot;, &quot;D10&quot;, &quot;E10&quot;, &quot;F10&quot;, &quot;G10&quot;, &quot;H10&quot;, &quot;I10&quot;, &quot;J10&quot;, &quot;K10&quot;, &quot;L10&quot;, &quot;M10&quot;, &quot;N10&quot;, &quot;O10&quot;, &quot;P10&quot;, &quot;Q10&quot;, &quot;R10&quot;, &quot;S10&quot;, &quot;T10&quot;, &quot;U10&quot;, &quot;V10&quot;, &quot;W10&quot;, &quot;X10&quot;, &quot;Y10&quot;, &quot;Z10&quot;]] }<br>  ]<br>}</pre><h3>5. Sample scriptÂ 2</h3><p>Each function of the above sample script uses only one argument. When multiple arguments are used, the script is as follows. The sample function is asÂ follows.</p><ul><li><a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#splitarray">splitArray</a>: Split array every nÂ length.</li></ul><pre>function myFunction() {<br><br>  const apiKey = &quot;###&quot;; // Please set your API key.<br><br>  const functionObj = {<br>    splitArray: function splitArray(array, size) {<br>      /**<br>       * ### Description<br>       * Split array every n length.<br>       *<br>       * @param {Array} array 2 dimensional array.<br>       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.<br>       * @return {Array} Transposed array.<br>       */<br>      function splitArray(array, size) {<br>        if (!array || !size || !Array.isArray(array)) {<br>          throw new Error(&quot;Please give an array and split size.&quot;);<br>        }<br>        return [...Array(Math.ceil(array.length / size))].map((_) =&gt;<br>          array.splice(0, size)<br>        );<br>      }<br>      return splitArray(array, size);<br>    },<br>  };<br><br>  const g = GeminiWithFiles.geminiWithFiles({<br>    apiKey,<br>    response_mime_type: &quot;application/json&quot;,<br>    doCountToken: true,<br>  });<br><br>  const functions = Object.entries(functionObj)<br>    .map(<br>      ([k, v]) =&gt;<br>        `&lt;FunctionName&gt;${k}&lt;/FunctionName&gt;&lt;Function&gt;${v.toString()}&lt;/Function&gt;`<br>    )<br>    .join(&quot;&quot;);<br>  const jsonSchema = {<br>    title: &quot;5 input values for giving each function&quot;,<br>    description: `Proposal 5 input values for giving each function. ${functions} Don&#39;t propose &quot;empty&quot;, &quot;null&quot;, &quot;undefined&quot; as values.`,<br>    type: &quot;array&quot;,<br>    items: {<br>      type: &quot;object&quot;,<br>      properties: {<br>        functionName: { description: &quot;Function name&quot;, type: &quot;string&quot; },<br>        inputValues: {<br>          description: `Proposed 5 input values. Don&#39;t propose &quot;empty&quot;, &quot;null&quot;, &quot;undefined&quot; as values.`,<br>          type: &quot;array&quot;,<br>          items: {<br>            description: &quot;Proposed input value&quot;,<br>            type: &quot;array|object|string|number&quot;,<br>          },<br>        },<br>      },<br>      additionalProperties: false,<br>    },<br>  };<br>  let res = g.generateContent({ jsonSchema });<br>  if (typeof res == &quot;string&quot;) {<br>    try {<br>      res = JSON.parse(res);<br>    } catch ({ stack }) {<br>      console.error(stack);<br>      return;<br>    }<br>  }<br>  const result = res.reduce((o, { functionName, inputValues }) =&gt; {<br>    try {<br>      o[functionName] = [];<br>      inputValues.forEach((input) =&gt; {<br>        const temp = JSON.parse(JSON.stringify(input));<br>        const output = functionObj[functionName](...temp);<br>        o[functionName].push({ input, output });<br>      });<br>    } catch ({ stack }) {<br>      console.log(stack);<br>    }<br>    return o;<br>  }, {});<br>  console.log(JSON.stringify(result));<br>}</pre><p>When this script is run, the following result is obtained.</p><pre>{<br>  &quot;splitArray&quot;: [<br>    { &quot;input&quot;: [[1, 2, 3, 4, 5, 6], 2], &quot;output&quot;: [[1, 2], [3, 4], [5, 6]] },<br>    { &quot;input&quot;: [[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;], 2], &quot;output&quot;: [[&quot;a&quot;, &quot;b&quot;], [&quot;c&quot;, &quot;d&quot;], [&quot;e&quot;]] },<br>    { &quot;input&quot;: [[&quot;apple&quot;, &quot;orange&quot;, &quot;grape&quot;, &quot;banana&quot;, &quot;kiwi&quot;], 3], &quot;output&quot;: [[&quot;apple&quot;, &quot;orange&quot;, &quot;grape&quot;], [&quot;banana&quot;, &quot;kiwi&quot;]] },<br>    { &quot;input&quot;: [[true, false, true, false, true], 1], &quot;output&quot;: [[true], [false], [true], [false], [true]] },<br>    { &quot;input&quot;: [[1.2, 3.14, 2.71, 0.577], 2], &quot;output&quot;: [[1.2, 3.14], [2.71, 0.577]] }<br>  ]<br>}</pre><h3>Summary</h3><p>From the above result, we can confirm the possibility of reverse engineering using Gemini API. This also shows that Gemini API can be used to develop applications.</p><h3>Note</h3><ul><li>If an error occurs, please run the script again. Or, please adjust the description in the JSONÂ schema.</li><li>I believe that this approach will be able to be also used for other languages except for Google AppsÂ Script.</li><li>In the current stage, it seems that the class objects depending on Google Apps Script like SpreadsheetApp, DriveApp, and so on cannot be used as the inputÂ values.</li><li>The top abstract image was created byÂ <a href="https://gemini.google.com/app">Gemini</a>.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2ee8789f01db" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db">Leveraging Gemini 1.5 API for Automated Test Case Generation Reverse Engineering</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Googleâ€™s Gemini Function Calling to Fetch Real-Time Company News and Insights from Externalâ€¦]]></title>
            <link>https://medium.com/google-cloud/using-googles-gemini-function-calling-to-fetch-real-time-company-news-and-insights-from-external-d11b2da565e3?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/d11b2da565e3</guid>
            <category><![CDATA[calling-function]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[gemini]]></category>
            <dc:creator><![CDATA[Ishana Shinde]]></dc:creator>
            <pubDate>Thu, 09 May 2024 09:29:21 GMT</pubDate>
            <atom:updated>2024-05-09T09:29:21.687Z</atom:updated>
            <content:encoded><![CDATA[<h3>Using Googleâ€™s Gemini Function Calling to Fetch Real-Time Company News and Insights from ExternalÂ APIs</h3><p>In todayâ€™s fast-paced financial markets, professional and seasoned investors are constantly on the lookout for efficient ways to stay ahead. The sheer volume of financial data available can be overwhelming, but powerful generative models such as the <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview">Gemini model in Vertex AI</a> along with tool frameworks like <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling">Function Calling in Gemini</a> offer a promising solution. These tool frameworks not only automate the process of gathering financial insights but also revolutionizes the way investors can use generative models to interact with marketÂ data.</p><h3>Overview</h3><p>Function Calling is a native framework in Gemini that enables the generative model to call specific functions and perform tasks based on user prompts. It allows you to get output from the Gemini model in a structured way, trigger function calls that process data, retrieve information from external sources, or execute operations via API calls. This approach allows you to build AI agents that can perform complex workflows and opens up new possibilities for automation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vOZ_3hOAcKSo2wUaYNTqRw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DN0M1Sim-LMSCF892XZZOA.png" /></figure><p>When using Function Calling in Gemini, you can still communicate with Gemini through natural language prompts, which Gemini then transforms into structured data outputs. This interaction provides a flexible way to convert natural language into API calls that access real-time information from external APIs and quickly generate insights. Letâ€™s dive into a use case that illustrates how this technology can streamline news and financial data analysis of various companies.</p><h3>The Investorâ€™s Challenge</h3><p>Meet Jane! Sheâ€™s a busy investor whoâ€™s always on the lookout for the latest market trends and financial news. She needs information quickly and accurately, but sifting through endless articles and reports is time-consuming.</p><p>Janeâ€™s daily routine involves analyzing a flood of financial reports and market data to identify investment opportunities. By relying solely on Gemini, she faces significant challenges like frozen training data, making it difficult to access the latest market trends. Inconsistent responses further hinder her analysis, as the model struggles to provide accurate and relevant information consistently. Additionally, the limited functionality of generative models without function and tool framework prevents Jane from seamlessly integrating real-time data and executing complex financial calculations, leaving her with a fragmented and inefficient workflow.</p><h3>Discovering the Solution: Gemini FunctionÂ Calling</h3><p>The breakthrough came when Jane discovered Function Calling in Gemini. This framework uses native functionality in the Gemini model to translate natural language into structured data that we can use to help the generative model integrate with external data sources and fetch tailored financial and company insights. Itâ€™s not just about speed; itâ€™s about the relevance and precision of the information that weâ€™re sending to the Gemini model to augment the knowledge it was trained on with real-time information from externalÂ systems.</p><p><strong>How Jane Uses Gemini FunctionÂ Calling</strong></p><p>Hereâ€™s a look at how Jane uses Function Calling in Gemini to provide functions as tools to enhance her data analysis:</p><p><strong><em>Function Declaration</em></strong></p><pre>get_company_overview_func = FunctionDeclaration(<br>    name=&quot;get_company_overview&quot;,<br>    description=&quot;This function returns the company information, financial ratios, and other key metrics for the equity specified. Data is generally refreshed on the same day a company reports its latest earnings and financials.&quot;,<br>    parameters={<br>        &quot;type&quot;: &quot;object&quot;,<br>        &quot;properties&quot;: {&quot;ticker&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The symbol of the ticker of your choice. For example: symbol=IBM&quot;}},<br>    },<br>)</pre><p>In this code snippet, Jane defines a function called get_company_overview. This function is designed to fetch essential company information, financial ratios, and key metrics for a specific stock ticker. The description provides a clear explanation of the function&#39;s purpose, while the parameters specify the required input (the stock tickerÂ symbol).</p><p><strong><em>Prompt</em></strong></p><pre>Give me the company overview of Google?</pre><p>Jane poses a simple question to Gemini, requesting an overview of Googleâ€™s company information.</p><p><strong><em>API Call</em></strong></p><pre>def get_company_overview_from_api(content):<br>    url = f&#39;https://www.alphavantage.co/query?function=OVERVIEW&amp;symbol={content[&quot;ticker&quot;]}&amp;apikey=INSERT_YOUR_API_KEY&#39;<br>    api_response = requests.get(url)  # Get the JSON response from the external news sentiment API<br>    return api_response.text</pre><p>Behind the scenes, Gemini recognizes the intent of Janeâ€™s query and triggers the get_company_overview function. This function, in turn, constructs a URL to query the Alpha Vantage API, replacing the placeholder {content[&quot;ticker&quot;]} with &quot;GOOG&quot; (the ticker symbol for Google). The API call fetches the relevant company data in JSONÂ format.</p><p><strong><em>Gemini Response</em></strong></p><pre>## Google&#39;s Company Overview:<br>* **Alphabet Inc Class C (GOOG)** is a multinational technology conglomerate headquartered in Mountain View, California.<br>* It&#39;s the parent company of Google and several former Google subsidiaries.<br>* As of March 31, 2024, it boasts a market capitalization of over $2 trillion.<br>* Key financial ratios:<br>    * P/E Ratio: 25.39<br>    * PEG Ratio: 1.659<br>    * Price/Book Ratio: 6.99<br>* Recent performance:<br>    * Quarterly earnings growth year-over-year: 0.609<br>    * Quarterly revenue growth year-over-year: 0.154</pre><p>Gemini processes the JSON data received from the API and crafts a concise summary in natural language. This summary highlights key aspects of Googleâ€™s company overview, including its full name, market capitalization, financial ratios, and recent performance metrics.</p><p>As demonstrated in this example, Gemini Function Calling streamlines the process of accessing company information. By simply asking a question, Jane can instantly receive a comprehensive overview of Googleâ€™s financial standing, saving her valuable time andÂ effort.</p><p><strong>Impact and Advancements</strong></p><p>Integrating Gemini Function Calling has significantly reduced the time Jane spends on company research, allowing her to focus more on strategic analysis and less on operational tasks. Since Jane was also able to build a web app and share it with her team, the toolâ€™s efficiency in delivering real-time, actionable insights has notably improved her teamâ€™s responsiveness to marketÂ changes.</p><p><strong>Extend the Functionality</strong></p><p>You can take the above example and extend it to serve your own needs, including integrating different data sources or defining additional functions that the Gemini model can use as tools at runtime. These improvements could further refine data accuracy and expand its utility across different industries.</p><h3>Explore theÂ Code</h3><p>If youâ€™re interested in exploring Gemini Function Calling further, visit the <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/use_case_company_news_and_insights.ipynb">GoogleCloudPlatform/generative-ai</a> code repository on GitHub to view and run the sample notebook.</p><h3>Conclusion</h3><p>Function Calling in Gemini offers a powerful way for generative models to interact with external systems and APIs to automate complex tasks. By integrating with external APIs, you can quickly retrieve the latest financial news and stock information, enabling faster and more informed decision-making.</p><p>Beyond mere automation, Function Calling ensures consistent and structured responses - eliminating the ambiguity and inconsistencies often associated with generative models. Developers gain full control in tailoring functions to specific financial tasks, such as real-time portfolio analysis or risk assessment. This powerful feature also augments generative models with real-time information from diverse sources, including databases, document repositories, and any other system accessible via anÂ API.</p><p>Whether youâ€™re a financial analyst, data scientist, or technology enthusiast, this approach can significantly streamline your workflow, improve accuracy, and unlock new possibilities in the way you access and process financial data.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d11b2da565e3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/using-googles-gemini-function-calling-to-fetch-real-time-company-news-and-insights-from-external-d11b2da565e3">Using Googleâ€™s Gemini Function Calling to Fetch Real-Time Company News and Insights from Externalâ€¦</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Scale Data Quality effortlessly on Google Cloud: Building a federated DQ framework empowered byâ€¦]]></title>
            <link>https://medium.com/google-cloud/scale-data-quality-effortlessly-on-google-cloud-building-a-federated-dq-framework-empowered-by-d62a60f00a62?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/d62a60f00a62</guid>
            <category><![CDATA[dataplex]]></category>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[data-quality]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Mansi Maharana]]></dc:creator>
            <pubDate>Thu, 09 May 2024 09:28:09 GMT</pubDate>
            <atom:updated>2024-05-09T09:28:08.947Z</atom:updated>
            <content:encoded><![CDATA[<h3>Scale Data Quality effortlessly on Google Cloud: Building a federated DQ framework empowered by Dataplex AutoDQ andÂ BigQuery</h3><p>The realm of data management is rapidly evolving, and enterprises are grappling with the challenge of ensuring data quality at scale. Data quality is the cornerstone of reliable information, minimizing data incidents, and ensuring regulatory compliance. The Data Mesh approach, a recently adopted paradigm in data management, emphasizes uniform data quality as a crucial factor for success. However, many organizations struggle with consistent data quality due to siloed data, fragmented implementation, and a lack of self-service infrastructure which often impede the implementation of data quality atÂ scale.</p><p><a href="https://cloud.google.com/dataplex/docs/auto-data-quality-overview">Dataplex AutoDQ</a>, addresses these challenges by offering a comprehensive solution that integrates with Google Cloud tools like BigQuery and other services, providing a centralized, scalable, and self-service data quality framework. BigQueryâ€™s scalable storage and federation capability which helps connect with external data sources are conducive to the framework. Fig.1 below outlines comprehensive Auto DQ capabilities and features.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rwBpsi9r6cQ1mqTi" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OLDPDid1jJYvnUV3" /><figcaption>Fig 1: Dataplex Auto DQÂ Features</figcaption></figure><p>This aforementioned framework enables the definition of data quality objectives and measurable methodologies for each domain and offers the following key features:</p><ul><li><strong>Centralized Management:</strong> Consistent data quality standards and measurement methodologies are established for the organization.</li><li><strong>Self-Service Capabilities:</strong> Teams can independently define and manage their data quality processes and metrics. This eliminates the need for each team to maintain its own infrastructure or processes for data quality management.</li><li><strong>Scalability:</strong> The framework can accommodate growth and the integration of new dataÂ sources.</li></ul><p>In this article, we will delve into the technical aspects of the federated and scalable data quality framework empowered by Dataplex Auto DQ and BQ in conjunction with other Google cloud services.</p><h3>The Federated DQ Framework: The Balance is aÂ must</h3><p>A federated DQ framework can provide a solution by empowering organizations to manage DQ in a centralized and consistent manner, while still allowing for domain-specific autonomy.</p><p>A key aspect of a federated DQ framework is the balance between centralization and autonomy. On the one hand, it is important to have centralized infrastructure and service to ensure consistency and prevent data quality silos. On the other hand, it is also important to empower domain teams with the autonomy to define and manage DQ assessments specific to theirÂ domain.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dKVjOMs4hVsdV2-i" /><figcaption>Fig 2: Federated DQ Framework Conceptual Architecture</figcaption></figure><p><strong>Centralized Infrastructure andÂ Service</strong></p><p>The centralized infrastructure and service should provide core DQaaS functionalities such as: Data Profiling and analysis, Data Cleansing and transformation, Data Quality monitoring and reporting. This centralized infrastructure should be managed by a dedicated data quality team that is responsible for setting and enforcing data quality standards across the organization.</p><p><strong>Domain-Owned Data Assessments</strong></p><p>Domain teams should be empowered to define and manage DQ assessments specific to their domain. This allows them to tailor DQ rules to their unique needs while still adhering to overall data quality standards. The Domain team should also be responsible for monitoring and remediating data quality issues and will be measured across thoseÂ KPIs.</p><p><strong>Why This BalanceÂ Matters</strong></p><p>Inconsistency in data quality across domains can cripple effective data management and hinder organizational goals. A centralized yet democratized Data Quality Framework addresses thisÂ by:</p><ul><li>Establishing Common Data QualityÂ Metrics</li><li>Streamlining Data Quality Management</li><li>Supporting Federated Rule Management</li><li>Ensuring Scalability and Collaboration</li><li>AI assistance for DQ rules recommendation andÂ binding</li></ul><p>This balanced approach entrusts domain teams with ownership while mitigating administrative burdens and infrastructure complexities while ensuring consistent data quality throughout the organization.</p><h3>The Federated DQ Framework: Reference Architecture</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KTcGTpZRM1eKdcB5" /><figcaption>Fig 3: Federated DQ Framework Technical Reference Architecture</figcaption></figure><p>Fig 3 illustrates the technical end-to-end architecture of the federated Data Quality framework and incorporates the following components:</p><ol><li><strong>Defining/Developing DQ Rules </strong>Dataplex provides multiple interfaces to help data product owners define and modify data quality (DQ) rules. DQ rules can be specified using JSON or <a href="https://cloud.google.com/dataplex/docs/use-auto-data-quality#create-scan-using-gcloud">YAML files</a>, enabling users to focus on formalizing validation rules without writing code. Users can also define reusable rules based on a standard template.Currently, BigQuery Machine Learning (BQML) models have the ability to be embedded within data quality (DQ) rules. For instance, anomaly detection BQML models can be effortlessly integrated into DQ rules. Additionally, the reusability of rules can be achieved through the definition of common rules via user-defined functions (UDFs). AI will assist in making advanced rule recommendations in the future. Learn more about rule definitions <a href="https://cloud.google.com/dataplex/docs/auto-data-quality-overview#rule-definition">here</a>.</li><li><strong>Review &amp; Approval </strong>Data stewards ensure data quality (DQ) rules meet organizational requirements, especially for critical and sensitive data elements. They verify the presence of required DQ rules and ensure proper definition and adherence to best practices. AI can assist with validation in the future. Existing review and approval processes can be leveraged for thisÂ purpose.</li><li><strong>DQ-as-a-Code </strong>Data quality configurations can be managed as code using Terraform, GitHub, and Cloud Build. This enables version control and seamless transfer of rules between environments.<strong> </strong>You can learn more about itÂ <a href="https://cloud.google.com/dataplex/docs/manage-data-quality-rules-as-code">here</a>.</li><li><strong>Data Store</strong> Automatic Data Quality (DQ) is supported for BigQuery tables, views, BigLake, and external tables created on Google Cloud Storage (GCS) data. The extensive capabilities of BigQuery as a data repository allow it to efficiently connect to external data sources, making data movement unnecessary and thus facilitating seamless data interoperability. Data Quality also facilitates the execution of data quality rules on various data storage formats, such as Parquet, Avro, Iceberg, and others, throughÂ BigLake.</li><li><strong>Orchestration</strong> You can schedule data quality checks through the serverless scheduler in Dataplex, or use the Dataplex API through external schedulers like Cloud Composer for pipeline integration. The composer DAG, which can be created as part of the framework, is in charge of orchestrating the data quality execution and then tagging workflow based on the schedule/dependency details supplied by the data product owner. Incorporating a Dataplex DQ job as a component of your data pipelines is made simpler by the ease with which a data engineering job can be an upstream or downstream dependency. Remember that composer can be used as a federated component, allowing us to use either a central operations composer instance or a domain-specific composer instance.</li><li><strong>Data Quality execution engine</strong> Dataplex DQ engines execute data quality rules against data in Google Cloud Storage and BigQuery(extend to other data sources through federation), requiring no infrastructure setup. They are fully managed, serverless, and support incremental checks.</li><li><strong>Incident management &amp; Actions</strong> Dataplex DQ logs are forwarded to cloud logging. Failed logs can be monitored by cloud monitoring. The <a href="https://cloud.google.com/dataplex/docs/use-auto-data-quality#set-alerts">notification channel alerts </a>relevant parties through emails, pagers, slack, or web hooks. Additional actions such as bug filing and assignment can be done too. Learn more about monitoring and alertingÂ <a href="https://cloud.google.com/dataplex/docs/auto-data-quality-overview#monitoring-and-alerting">here</a>.</li><li><strong>Analysis &amp; reporting</strong> Data quality results, generated by the Data Quality service, can be <a href="https://cloud.google.com/dataplex/docs/use-auto-data-quality#table-schema">published to BigQuery </a>directly or extracted using the API. This enables the organization to democratize this data, making it available for programmatic reporting and further analysis, such as time series. Additionally, column and row level security controls can be implemented at the domain level to ensure appropriate accessÂ control.</li><li><strong>Actionable dashboard </strong>You can provide an actionable dashboard to all the end users so they can monitor and assess the data quality across multiple dimensions. We can use any visualization tool we want on this DQ result data in BigQuery. Google Cloud also offers <a href="https://datastudio.google.com/data">Looker Data Studio</a> which is a simpler solution, and free, so you can try it anyÂ time.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JeD7-BbyNlmeoXGL" /><figcaption><strong>Example 1: D</strong>ata quality statistics as part of your centralized data products dashboard.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*E6mw4GVkS3_CvgkM" /><figcaption><strong>Example2: </strong>Data quality statistics can provide more detailed and time-series-based insights.</figcaption></figure><p>10. <strong>Auto tagging for discovery</strong> Data quality (DQ) results can be disseminated to the catalog via Auto DQ, and indexed custom tags that are incorporated into the datasets can be created. This will facilitate the search for and discovery of high-quality data products. Incorporating data quality metrics as part of metadata can help foster trust in data and enable users to utilize it with complete confidence and effortless search-ability.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*I2AV6eC_HFq5nacW" /><figcaption>DQ tag displaying overall and dimesion specific qualityÂ scores</figcaption></figure><h3>The Federated DQ Framework: KeyÂ Benefits</h3><ul><li><strong>Improved data quality </strong>A federated DQ framework can help to improve data quality by ensuring that all data sources are subject to the same DQ standards. This can lead to improved decision making and better business outcomes.</li><li><strong>Reduced costs </strong>A federated DQ framework can help to reduce costs by eliminating the need for duplicate DQ tools and processes. This can also lead to improved operational efficiency.</li><li><strong>Increased agility </strong>A federated DQ framework can also help increase agility by enabling organizations to respond quickly to changing data quality requirements. This can give organizations a competitive advantage in todayâ€™s fast-paced business environment.</li></ul><h3>The Federated DQ Framework: GuidingÂ tenets</h3><p>As you start adapting a federated data quality approach, here are a few guiding principles to abideÂ by:</p><ul><li><strong>Create a unified data quality framework</strong> that serves as a guiding principle for all data initiatives within the organization. This framework should provide a consistent approach to data quality measurement, ensuring that all data is evaluated using the same standards andÂ metrics.</li><li><strong>Objectively measure data quality</strong> Data quality should be measured objectively using quantifiable metrics such as accuracy, completeness, consistency, and timeliness. This ensures that data quality is not based on subjective opinions or perceptions.</li><li><strong>Align data quality with business objectives</strong> Data quality objectives should be aligned with the organizationâ€™s overall business goals and objectives. This ensures that data quality efforts are focused on improving the areas that matter most to the business.</li><li><strong>Clearly communicate data quality objectives and metrics</strong> Data quality objectives and metrics should be clearly communicated to all stakeholders, including data producers, data consumers, and data stewards. This ensures that everyone is aware of the expectations for data quality and can take appropriate actions to meet those expectations.</li><li><strong>Provide training on data quality</strong> Data producers and consumers should be provided with training on data quality best practices. This training should cover topics such as data cleansing, data validation, and data standardization.</li><li><strong>Define clear data quality scores for each data subject or domain</strong> Data quality scores should be defined for each data subject or domain within the scope of the data mesh. These scores should be based on the data quality objectives and metrics that have been established.</li><li><strong>Use data quality scores to monitor progress</strong> Data quality scores can be used to monitor progress over time and identify areas where improvement is needed. This information can be used to make informed decisions about data quality initiatives.</li><li><strong>Central operations should provide a standard reusable, rules-based template</strong> Central operations should provide a standard reusable, rules-based template that allows domain teams to define customized data quality rules. This template should include a set of common data quality rules that can be applied to all data, as well as a mechanism for defining custom rules for specific dataÂ domains.</li><li><strong>Enable domain teams to define customized rules</strong> Domain teams should be able to define customized data quality rules that are specific to their needs. This flexibility is important for ensuring that data quality rules are tailored to the unique characteristics of each dataÂ domain.</li><li><strong>Each critical data element or field should have its own set of data quality rules</strong> Each critical data element or field should have its own set of data quality rules that are applied to rows or summarized at the table level. These rules should be designed to ensure that data is accurate, complete, consistent, andÂ timely.</li><li><strong>Data quality rules should be enforced at the point of data entry</strong> Data quality rules should be enforced at the point of data entry to prevent low-quality data from entering the system. This can be done through a variety of methods, such as data validation checks, data cleansing routines, and data standardization procedures.</li><li><strong>Aim for high-performing data stewardship within the organization</strong> High-performing data stewardship is essential for ensuring that data quality is maintained over time. Data stewards should be responsible for monitoring data quality, identifying and resolving data quality issues, and promoting a culture of data quality within their organization.</li><li><strong>Train data stewards per domain</strong> Data stewards should be trained on data quality best practices and the specific data quality requirements of their domain. This training will enable data stewards to effectively manage data quality within theirÂ domain.</li><li><strong>Central operations should work with domain teams to identify applicable data quality metrics</strong> Central operations should work with domain teams to identify the data quality metrics that are most relevant to their business needs. This collaboration is important for ensuring that data quality efforts are focused on the areas that matter most to the business.</li><li><strong>Support domain teams in defining rules for establishing a data quality baseline </strong>Central operations should support domain teams in defining the rules that will be used to establish a data quality baseline. This support can include providing guidance on data quality best practices and developing tools and templates to help domain teams define theirÂ rules.</li><li><strong>Facilitate the adoption of data quality improvements</strong> Central operations should facilitate the adoption of data quality improvements by providing training, resources, and support to domain teams. This support can help domain teams to implement data quality improvements quickly and effectively.</li><li><strong>Data quality is an ever-evolving process</strong> Data quality is an ever-evolving process that is influenced by changes in the data landscape. As such, it is important to adopt an iterative approach toÂ data</li></ul><h3>Conclusion</h3><p>The federated and scalable data quality framework is a powerful tool that can empowers enterprises to manage data quality at scale, reduce costs and increase agility. By leveraging the combined strengths of Dataplex Auto DQ, BigQuery, and other Google Cloud services, organizations can gain a comprehensive understanding of their data landscape, identify and resolve data quality issues efficiently, and drive better decisionÂ making.</p><h3>Whatâ€™s next?</h3><p>If you feel like you want to learn more about it, contact me at <a href="mailto:maharanam@google.com">manaswini.maharana@gmail.com</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d62a60f00a62" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/scale-data-quality-effortlessly-on-google-cloud-building-a-federated-dq-framework-empowered-by-d62a60f00a62">Scale Data Quality effortlessly on Google Cloud: Building a federated DQ framework empowered byâ€¦</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Yahoo Benchmarks Dataflow vs Self-managed Flink Efficiency for two Streaming use-casesâ€“ Which isâ€¦]]></title>
            <link>https://medium.com/google-cloud/yahoo-benchmarks-dataflow-vs-b189c809ff49?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/b189c809ff49</guid>
            <category><![CDATA[apache-flink]]></category>
            <category><![CDATA[google-dataflow]]></category>
            <category><![CDATA[google-cloud-plaatform]]></category>
            <category><![CDATA[real-time-streaming-data]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Ihaffa]]></dc:creator>
            <pubDate>Wed, 08 May 2024 02:42:55 GMT</pubDate>
            <atom:updated>2024-05-08T02:48:58.085Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RRE6HxnipVQRDSp5mnMPsg.png" /></figure><h3><strong>Yahoo Benchmarks Dataflow vs Self-managed Flink Efficiency for two Streaming use-casesâ€“ Which is More Cost-Effective?</strong></h3><p><em>This blog post is co-author by Abel Lamjiri, Sr Principal Software Engineer atÂ Yahoo</em></p><p><strong>Introduction</strong></p><p>While working with Yahoo, weâ€™re constantly seeking ways to optimize the efficiency of streaming large-scale data processing pipelines. In a recent project, we focused on benchmarking the cost and performance of two stack choices that Yahoo wanted to understand: running Apache Flink in a self managed environment and Google Cloud Dataflow on two specific Yahoo use cases. This post details our benchmark setup, methodology, the use cases in scope, key findings, and the Dataflow configurations that helped us streamline performance.</p><p><strong>Benchmark Setup</strong></p><p>We designed our benchmark to ensure a quick, fair and rough comparison on Yahoo typical use cases; we chose two representative workloads, one compute heavy and another IO heavy task; the result of this benchmark would indicate which platform to be recommended to Yahoo teams for streaming, including writing results to BigTable, GCS, and complex streaming pipeline discussed below.</p><p><strong>Test Setup Infrastructure Diagram:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/801/0*DzoWO3ZmrwNMjTVX" /></figure><p><strong>Metric: </strong>Our primary focus was compute cost per unit of throughputâ€Šâ€”â€Šthe volume of data processed per unit of time. We aimed to understand the costs of running Flink and Dataflow pipelines at a sustained throughput of 20,000 records per second. â€œCostâ€ for Flink excludes operational overhead of setting up and running the job in the respective platform (engineering hoursÂ used).</p><p><strong>Workload: </strong>We simulated a 10TB data stream from Pub/Sub, creating a backlog of 100+ million records to maintain a constantÂ load.</p><p><strong>Controlled Environment: </strong>To focus on core configuration impacts, we initially disabled autoscaling by fixing Flinkâ€™s resource allocation and setting a limit on Dataflow worker count. This allowed us to compare costs at a consistent throughput: i.e. cost-per-unit-of-throughput. Auto Scaling would be a dimension in the ease of operation category, which we did expand in this experiment. We verified Dataflow scaling and found it seamlessly though.</p><p><strong>Use-case details:</strong></p><ol><li><strong>Write Avro to Parquet: </strong>Streaming job which reads avro message which then get windowed to certain time frames (1â€“5 mins) and outputted to gcs asÂ parquet</li><li><strong>Data Enrichment and Calculation (Complex): </strong>Simulating active user analysis and event enrichment. Involved Beam state management, key reshuffling, and making external calls to anotherÂ service.</li></ol><p><strong>Notes:</strong></p><ul><li>Operational costs of managing infrastructure are excluded for both platforms.</li><li>If the benchmark would show similar cost for both Flink and Dataflow infra cost, we were ready to pick Dataflow because Dataflow is a managed service. We were ready to benchmark a third use-case if the two use-cases above were not conclusive.</li></ul><p><strong>Understanding TheÂ Result:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*s5Kxx3NJXtHI2d3H_h_v5A.png" /></figure><p>The table above shows that Dataflow is around 1.5â€“2 times more cost effective in comparison to Apache Flink on our test cases so letâ€™s understand in more detail how we get all this cost. The idea for the benchmark is to: calculate Flink/Dataflow cost to achieve similar throughput. The goal is to have as close as possible on the number of messages processed per second on each of the streaming applications. In the table above, for the Enrichment use case, the number of provisioned vCPU on GKE is approximately 13x higher compared to Dataflow. It is not because Flink is inefficient, but it is because in Dataflow, <a href="https://cloud.google.com/dataflow/docs/streaming-engine">Streaming Engine</a> would sent a lot of the heavy computation to Dataflow backend. Of course, there was some room to improve Flink utilization, but it turned out to make the job unstable, so we did not spend further time there and <em>calculated the cost for 32 vCPUs</em> (2 x n2d-standar-16 machines) as if utilization wasÂ ~75%.</p><p>You can think of the Dataflow backend as a GCP backend resource to do heavy computation (e.g shuffling) instead of the work being done on Dataflowâ€™s worker. This naturally makes Dataflow to require less vCPU, make it more robust and gives much more consistency throughput. It is critical for Yahoo use cases to leverage the streaming engine.</p><p>In the below image, our dataflow pipeline uses a new release <a href="https://cloud.google.com/dataflow/pricing#streaming-compute-units">cost billing feature</a> which calculates cost based on Stream Engine Processing Unit. From our testing, the new billing feature was able to optimize pipeline cost for our throughput based workloads. On the Flink side, we install telemetry and monitor pubsub throughput to check the amount of resource it is using. For Flink setup, we did not spend too much time tuning the job and therefore, assumed lowest cost if we had improved utilization; i.e. the cost is based on having 32 vCores assuming we can get around 75% cpu utilization at bestÂ case.</p><p>To see detail breakdown of the cost of Dataflow, you can simply goes to Dataflow Cost Tab, like shownÂ below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0OE6vgHT6htZfEZK" /></figure><p>Here is the Dataflow command that we use to deploy our settings (inÂ gradle):</p><blockquote>task dataflow(type:JavaExec) {</blockquote><blockquote>mainClass = â€œcom.google.benchmark.BenchmarkDataflowâ€</blockquote><blockquote>classpath = sourceSets.main.runtimeClasspath</blockquote><blockquote>args =Â [</blockquote><blockquote><strong>â€œâ€Šâ€”â€ŠenableStreamingEngine</strong>â€,</blockquote><blockquote>â€œâ€Šâ€”â€ŠworkerMachineType=n2-standard-8â€, //or n2-standard-2 with 2Â workers</blockquote><blockquote><strong>â€œâ€Šâ€”â€ŠdataflowServiceOptions</strong>=<strong>enable_streaming_engine_resource_based_billing</strong>â€,</blockquote><blockquote>â€œâ€Šâ€”â€ŠnumWorkers=1â€, // or 2Â workers</blockquote><blockquote>â€œâ€Šâ€”â€ŠmaxNumWorkers=1â€, // or 2Â workers</blockquote><blockquote>â€¦</blockquote><blockquote>â€œâ€Šâ€”â€ŠusePublicIps=falseâ€,</blockquote><blockquote>]</blockquote><blockquote>}</blockquote><p>The original Dataflow Streaming Engine usage is currently metered and billed based on the <a href="https://cloud.google.com/dataflow/pricing#streaming-data">volume of data processed</a> by Streaming Engine. With the resource-based billing model, jobs are metered based on the <em>resources that are consumed</em> and users are billed for the total resources consumed. In short, Streaming Engine Compute Units are used to calculate for â€œResource Based Billingâ€ while previously the streaming engine cost are calculated by the amount of data being sent/ process (Total GB of dataÂ process)</p><p><strong>How to tell the difference?</strong></p><ul><li>If you were to use â€œresource-based billing modelâ€, you will be charged based on the SKU (stock keeping unit) not by the amount of streaming data processed</li><li>In the â€œCostâ€ tab, you should see â€œ<strong>Streaming Engine Compute Unit</strong>â€ instead of â€œProcessed Dataâ€</li></ul><p>In addition to basic parameters, further optimization in Dataflow can be achieved by carefully customizing machine types to your specific workload. Dataflow Prime can also enhance both performance and cost-efficiency in some scenarios.</p><p><strong>Conclusion</strong></p><p>This benchmark highlights the importance of careful configuration and ongoing experimentation when optimizing Dataflow pipelines. The ideal setup for your Dataflow deployment is highly dependent on your specific workload, data velocity, and cost-performance tradeoffs youâ€™re willing to make. Understanding the various optimization tools within Dataflowâ€Šâ€”â€Šfrom worker configuration to features like autoscalingâ€Šâ€”â€Šis crucial for maximizing efficiency and minimizing cost.</p><p>When using Resource Based Billing, for the two use-cases tested in this study, we found Flink compute cost to be on par with Dataflow. Without this flag, Dataflow was 5X more expensiveâ€Šâ€”â€Šagain for the two use-cases in the scope of thisÂ study.</p><p>Finally, keep an eye out for new Dataflow features! The recent introduction of <a href="https://cloud.google.com/dataflow/docs/guides/streaming-modes#set-streaming-mode">at-least-once streaming mode</a> offers greater flexibility for use cases where occasional duplicates are acceptable in favour of lower cost and reducedÂ latency.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b189c809ff49" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/yahoo-benchmarks-dataflow-vs-b189c809ff49">Yahoo Benchmarks Dataflow vs Self-managed Flink Efficiency for two Streaming use-casesâ€“ Which isâ€¦</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>