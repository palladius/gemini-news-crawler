<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 16 Apr 2024 19:00:06 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Appsâ€¦]]></title>
            <link>https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/c9ce870dcb99</guid>
            <category><![CDATA[google-apps-script]]></category>
            <category><![CDATA[google-workspace]]></category>
            <category><![CDATA[google-spreadsheets]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-sheets]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 09:29:21 GMT</pubDate>
            <atom:updated>2024-04-16T09:29:21.906Z</atom:updated>
            <content:encoded><![CDATA[<h3>Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google AppsÂ Script</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*rqD6sT_95bXtlahR.jpg" /></figure><h3>Abstract</h3><p>Consolidate scattered cell references (A1Notation) in Google Sheets for efficiency. This script helps select cells by background color or update values/formats, overcoming limitations of large rangeÂ lists.</p><h3>Introduction</h3><p>When working with Google Spreadsheets, there might be a scenario where you need to process scattered A1Notations (cell addresses in the format â€œA1â€). This could involve selecting cells with specific background colors, updating cell values, or modifying cellÂ formats.</p><p>One approach to handle scattered A1Notations is to create a range list containing the individual cell coordinates and activate it. However, this method becomes inefficient when dealing with a large number of cells due to the high processing cost associated with activating each cell individually.</p><p>To address this limitation, consolidating scattered A1Notations into continuous ranges offers a significant performance improvement. While a previous report discussed expanding consolidated A1Notations back into individual cells Ref: <a href="https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/">https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/</a>, consolidating them for processing efficiency had not beenÂ covered.</p><p>During the development of a script to achieve consolidation, it became apparent that existing solutions were not straightforward. To ensure clarity and facilitate debugging in the initial stages, the script was created by splitting each step into smaller, testable functions. While this approach might appear less elegant, it prioritizes understandability during the development process.</p><p>The provided script offers a solution for consolidating scattered A1Notations, as illustrated in the demonstration image. By consolidating the notations, the script can efficiently select cells with a specific background color, reducing the overall processing cost.</p><p>Furthermore, the scriptâ€™s functionality can be extended to other use cases. For instance, it can be used to update the values or formats of scattered cells across the spreadsheet.</p><h3>Principle</h3><p>In this script, the process of consolidating A1Notations into rectangles is achieved by calculating the maximum rectangle size for all given A1Notations. This essentially combines scattered A1Notations into a single, most efficient rectangle.</p><p>The sample situation is asÂ follows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*53lMnCPL-j25cyO-.png" /></figure><p>The cells with a red background color are used in this example. When the A1Notations are retrieved from those cells, it is asÂ follows.</p><pre>[<br>  &quot;C2&quot;,<br>  &quot;D2&quot;,<br>  &quot;E2&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;C3&quot;,<br>  &quot;D3&quot;,<br>  &quot;E3&quot;,<br>  &quot;C4&quot;,<br>  &quot;D4&quot;,<br>  &quot;C6&quot;,<br>  &quot;D6&quot;,<br>  &quot;C7&quot;,<br>  &quot;D7&quot;,<br>  &quot;B8&quot;,<br>  &quot;C8&quot;,<br>  &quot;D8&quot;,<br>  &quot;E8&quot;,<br>  &quot;F8&quot;<br>]</pre><p>When these A1Notations are consolidated, it becomes asÂ follows.</p><pre>[<br>  &quot;C2:E3&quot;,<br>  &quot;C6:D8&quot;,<br>  &quot;C4:D4&quot;,<br>  &quot;E8:F8&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;B8&quot;<br>]</pre><p>Here, the maximum size of the rectangle is calculated starting from the top-left cell (C2 in this example). This approach determines the result values in the above outputÂ order.</p><p>Itâ€™s important to note that if the situation is changed, the maximum size of the rectangle might not always be obtainable using this method. While itâ€™s possible to modify the starting cell for calculation to ensure the maximum rectangle size is always found, this can significantly increase the processing cost. Therefore, this script adopts the top-left cell as the starting point for calculation to strike a balance between efficiency and accuracy.</p><p>The image provides a visual representation of the consolidation process applied to a sample set of A1Notations.</p><p>The script can be seen at <a href="https://github.com/tanaikech/UtlApp/blob/master/forStringProcessing.js#L466">my repository</a>.</p><h3>Usage</h3><h3>1. Create a Google Spreadsheet</h3><p>Please create a new Google Spreadsheet. And, please set the background color as the above image. In this sample, the background colors are set inÂ â€œB2:F8â€.</p><p>And, please open the script editor of this Spreadsheet.</p><h3>2. InstallÂ library</h3><p>In this case, the script is a bit complicated. So, I added this script to my existing library <a href="https://github.com/tanaikech/UtlApp">UtlApp</a>. By this, this library can expand and consolidate the A1Notations.</p><p>You can see how to install this library atÂ <a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#1-install-library">here</a>.</p><h3>3. Sample scriptÂ 1</h3><p>In this sample, the situation of the above image is used. The script is asÂ follows.</p><pre>function sampl1() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  Browser.msgBox(JSON.stringify(res));<br>}</pre><p>When this script is run, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/909/0*HGSpkIuucZ75_3Xh.gif" /></figure><h3>4. Sample scriptÂ 2</h3><p>In this sample, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/810/0*xTvZlCIhIhj0z6mH.gif" /></figure><p>The cells of the red background color are selected by this script. And, the background color of the selected cells is manuallyÂ changed.</p><p>The script is asÂ follows.</p><pre>function sampl2() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  sheet.getRangeList(res).activate();<br>}</pre><h3>IMPORTANT</h3><ul><li>Iâ€™m worried that this method might not be able to be used on a Google Spreadsheet with a large size because of the processÂ cost.</li></ul><h3>Note</h3><ul><li>The top abstract image was created by <a href="https://gemini.google.com/">Gemini</a> from the section of â€œIntroductionâ€.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c9ce870dcb99" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99">Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Appsâ€¦</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FHIR Whistle Data Mappings Validation]]></title>
            <link>https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/cd62c8613a92</guid>
            <category><![CDATA[healthcare-data-engine]]></category>
            <category><![CDATA[fhir-mapping]]></category>
            <category><![CDATA[whistle-data-mapping]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Ashwinshetty]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:06:38 GMT</pubDate>
            <atom:updated>2024-04-16T06:26:24.849Z</atom:updated>
            <content:encoded><![CDATA[<h3>Business Scenario</h3><p>Healthcare Data Engine(HDE) is a popular GCP based solution to help Healthcare stakeholders transition to FHIR (Fast Healthcare Interoperability Resources). HDE provides pipelines which helps convert non FHIR data to FHIR and reconciles them to form a single Longitudinal Patient Record, which then makes deriving insights from patient data easy andÂ quick.</p><p>One of the core components of HDE is the Data Mapping Language known as Whistle. This Open Source Data Mapping language is used for converting complex, nested data from one schema to another. For Example, from HL7 toÂ FHIR.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/538/1*ODs0XPOT-aUALiRqchds9Q.png" /></figure><p>This article talks about how to use Whistle to write sample mappings for HL7 data. Run the mapping code locally and then test the resultant converted FHIR format data against a FHIRÂ store.</p><h3><strong>What do weÂ need</strong></h3><p>We will test the Whistle Mappings to convert sample HL7 data to FHIR. We will be leveraging APIs provided by GCP Healthcare API to ingest some sample HL7 messages to a HL7 store provided by GCP Healthcare API. We will use the schematized variant of this HL7 message from HL7 store and run Whistle mapping code to convert it to FHIR on our local machines. We will then test this converted FHIR data by ingesting it into GCP Healthcare API FHIRÂ store</p><h3>Steps</h3><p><strong>Step 1</strong>â€Šâ€”â€ŠEnable GCP Cloud Healthcare API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/953/1*4QU0vG0lwQMn5RSS-yC2Fw.png" /></figure><p><strong>Step 2</strong>â€Šâ€”â€ŠFollow documentation in <a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">git repoâ€Šâ€”â€ŠHealthcare Data Harmonization</a> to set up Whistle Engine on our local machines. This would need us to install below softwares on our local machines, as we will be using â€˜<strong>gradle</strong>â€™ to run our Whistle engine application.</p><ul><li><a href="https://git-scm.com/">Git</a></li><li><a href="https://www.azul.com/downloads/?version=java-11-lts&amp;package=jdk#zulu">JDK 11.x</a></li><li><a href="https://gradle.org/next-steps/?version=7.6&amp;format=bin">Gradle 7.x</a></li></ul><p><strong>Step 3â€Š</strong>â€”â€ŠCreate a <a href="https://cloud.google.com/healthcare-api/docs/datasets#create-dataset">Healthcare API Dataset</a> and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2#creating_an_hl7v2_store">HL7 store</a> and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/fhir#creating_a_fhir_store">Fhir store</a> inside that dataset. We will use these resources for ourÂ testing.</p><p>Once created we should see an output like below, where â€˜<strong>datastore</strong>â€™ is our Healthcare API Dataset, â€˜<strong>hl7v2store</strong>â€™ is the HL7 store and â€˜<strong>fhirstore</strong>â€™ is the FHIRÂ store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/793/1*FfLztPEw6stASDDU3c6C3w.png" /></figure><p><strong>Step 4</strong>â€Šâ€”â€ŠLet us <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2-messages#ingesting_hl7v2_messages">ingest a sample HL7 message</a> into our HL7 store. Save the below sample message in a file named â€˜<strong>sample-hl7-msg.hl7</strong>â€™.</p><pre>MSH|^~\&amp;|FROM_APP|FROM_FACILITY|TO_APP|TO_FACILITY|20170703223000||ADT^A01|20170703223000|P|2.5|<br>EVN|A01|20210713083617|<br>PID|1||21004033^^^^MRN||SULIE^BRAN||19941208|M|||444 MAIN ST^^MOUNTAIN SPRINGS^CO^80444||1111111144|2222222244|<br>PV1||I|H44 RM4^1^^HIGHWAY 44 CLINIC||||5144^MARRIE QUINIE|||||||||Y||||||||||||||||||||||||||||20170703223000|</pre><p>The default segment separator in HL7v2 is a carriage return (\r). Most text editors use newline (\n) characters as segment separators. So we will use the below command to replace any \n withÂ \r.</p><pre>sed -z &#39;s/\n/\r/g&#39; sample-hl7-msg.hl7 &gt; sample-hl7-msg-fixed.hl7</pre><p>HL7 store expects input messages to be in base64 encoded string format. So let us use the below command to encode the sample HL7Â message.</p><pre>openssl base64 -A -in ./sample-hl7-msg-fixed.hl7 -out ./sample-hl7-msg-base64.txt</pre><p>Copy the encoded string from â€˜<strong>sample-hl7-msg-base64.txt</strong>â€™ in the below format and save it in a file named â€˜<strong>hl7v2-sample.jsonâ€™</strong>.</p><pre>{<br>  &quot;message&quot;: {<br>    &quot;data&quot;: &quot;&lt;base64-encoded-string&gt;&quot;<br>  }<br>}</pre><p>We will run the below CURL command in a terminal to ingest this message to an HL7Â store.</p><pre>curl -X POST      \<br>    -H &quot;Authorization: Bearer $(gcloud auth application-default print-access-token)&quot;      \<br>    -H &quot;Content-Type: application/json; charset=utf-8&quot;      \<br>    --data-binary @hl7v2-sample.json      \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages:ingest&quot;</pre><p>Once the command is successful, we will get a â€˜<strong>message.name</strong>â€™ field in the response as shownÂ below.</p><pre>{<br>  &quot;hl7Ack&quot;: &quot;&lt;base64-encoded-string&gt;&quot;,<br>  &quot;message&quot;: {<br>    &quot;name&quot;: &quot;&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;MESSAGE_ID&gt;&quot;,<br>    }<br>}</pre><p>Using the â€˜<strong>message.name</strong>â€™ field we will next fetch the schematized message into an output json file. This file will act as an input for our whistle mappings.</p><pre>curl -X GET \<br>     -H &quot;Authorization: Bearer &quot;$(gcloud auth print-access-token) \<br>     -H &quot;Content-Type: application/json; charset=utf-8&quot; \<br>     &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;message-name&gt;&quot; \<br>     | jq &#39;.schematizedData.data | fromjson&#39; &gt; &lt;output-filename.json&gt;</pre><p><strong>Step 5</strong>â€Šâ€”â€ŠLet us open any IDE or terminal. We will run below gradle command to trigger mapping, in the directory where github repo wasÂ cloned.</p><pre>gradle :runtime:run -q --args=&quot;-m $HOME/wstl_codelab/codelab.wstl -i $HOME/wstl_cod<br>elab/&lt;output-filename.json&gt;&quot; &gt; converted-fhir.json</pre><blockquote>Explanation of the above <strong>gradle</strong>Â command:</blockquote><blockquote><strong>gradle</strong>: This invokes the Gradle build automation tool.<br><strong>:runtime:run</strong>: This tells Gradle to execute the run task to start the Whistle application.<br><strong>-q</strong>: This flag tells Gradle to run in â€œquietâ€ mode, suppressing most of the output except for errors.<br>â€Š<strong>â€”â€Šargs</strong>: This introduces arguments that will be passed to the run task (and ultimately to the application it starts).<br><strong>-m $HOME/wstl_codelab/codelab.wstl</strong>: This argument specifies the path to a whistle file that the application will use for data mapping.<br><strong>-i $HOME/wstl_codelab/&lt;output-filename.json&gt;</strong>: This argument points to a JSON file containing input data for the WhistleÂ mapping.</blockquote><p><strong>Sample Patient WhistleÂ Mapping:</strong></p><blockquote>This code is just for demo purposes and does not represent the actual FHIR structure. It maps Patient fields like â€˜<strong>identifier</strong>â€™, â€˜<strong>name</strong>â€™ and â€˜<strong>address</strong>â€™ from the PID segment in our input file. These mappings are structured into functions like â€˜<strong>Build_Identifier</strong>â€™, â€˜<strong>Build_Name</strong>â€™ and â€˜<strong>Build_Address</strong>â€™ for better readability.</blockquote><pre>PID_Patient($root.ADT_A01.PID)<br><br>def PID_Patient(PID){<br>  identifier[]: Build_Identifier(PID.3[])<br>  name[]: Build_Name(PID.5[])<br>  address[]: Build_Address(PID.11[])<br>  active: true<br>  resourceType: &quot;Patient&quot;<br>}<br><br>def Build_Identifier(CX) {<br>  value: CX.1<br>}<br><br>def Build_Name(XPN) {<br>  family: XPN.1.1<br>  given[]: XPN.2<br>  given[]: XPN.3<br>}<br><br>def Build_Address(XAD) {<br>  line[]: XAD.2<br>  city: XAD.3<br>  state: XAD.4<br>  postalCode: XAD.5<br>}</pre><p><strong>Step 6</strong>â€Šâ€”â€ŠOnce we have the mapped output, we can check if all the fields were converted as per our requirements. Once confirmed, we can try and load this to a FHIR store using the belowÂ command.</p><pre>curl -X POST \<br>    -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>    -H &quot;Content-Type: application/fhir+json&quot; \<br>    -d @converted-fhir.json \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/fhirStores/&lt;fhirstore-name&gt;/fhir/Patient&quot;</pre><p>Post successful completion of the above command, we should be able to see the record in our FHIRÂ store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nGeHlfu01LH9IC1oCEeN7Q.png" /></figure><h3>Conclusion</h3><p>By following the steps outlined above, we explored a method to validate the HL7 to FHIR conversion workflow utilizing the Open Source Whistle Data Mapping repository. This approach can be readily adapted to validate data conversion workflows involving any other data format to FHIR. This technique proves useful for conducting quick tests, proofs of concept (POCs), or pilot projects for healthcare data conversion to FHIR. Engaging with this process offers a deeper understanding of the capabilities of the powerful Whistle Data Mapping Language.</p><h3>Reference Links</h3><p><a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">Whistle githubÂ repo</a></p><p><a href="https://cloud.google.com/healthcare-api/docs">Cloud Healthcare API documentation</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd62c8613a92" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92">FHIR Whistle Data Mappings Validation</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dazboâ€™s Google Cloud Next â€™24 Recap: Keynote]]></title>
            <link>https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6f5518238c9d</guid>
            <category><![CDATA[ai-agent]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[wrap-up]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud-next]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:05:10 GMT</pubDate>
            <atom:updated>2024-04-16T07:13:06.235Z</atom:updated>
            <content:encoded><![CDATA[<h3>Shall I? Shanâ€™tÂ I?</h3><p>Itâ€™s been a couple of days since Google Cloud Next â€™24 wrapped up, and Iâ€™ve seen recaps appear on Medium already. So I ask myself: <em>â€œShould I bother thisÂ year?â€</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*jvPtfzoKolvQBc2o.jpg" /><figcaption>To recap or not toÂ recapâ€¦</figcaption></figure><p>Iâ€™ve decided â€œYesâ€ for twoÂ reasonsâ€¦</p><ul><li>Iâ€™ve been doing recaps of these events for a few years, so Iâ€™d hate to break my streak! (Check out <a href="https://medium.com/google-cloud/google-next-2023-experience-and-favourite-sessions-fb00add5f59e">here</a>, andÂ <a href="https://docs.google.com/presentation/d/1gfgijQjlQlvn6CEd29j5uVRXgynNFoBwMz2bUIXvydA/edit#slide=id.g27956d63397_0_230">here</a>.)</li><li>I find writing stuff down helps me learn and remember. So even if no one else finds this useful, IÂ will!</li></ul><p>This yearâ€™s Google Cloud Next was in Las Vegas. Alas, this is the Next in the last few that I havenâ€™t been able to attend in person. ğŸ˜­ So, this wrap-up is based purely on watching the virtual content. And in case you werenâ€™t aware, you can view all the recorded sessions at <a href="https://cloud.withgoogle.com/next">cloud.withgoogle.com/next</a>.</p><h3>Summary of Announcements</h3><p>Iâ€™ll update this list as a view more sessions.</p><ul><li>New investments in sub-sea cabling and dataÂ centres.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: A3 Mega VMs, powered by NVIDIA H100 Tensorcore GPUs. Twice as powerful has the previous iteration.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: GA of TPU v5p. Googleâ€™s most powerful TPU yet. These have 4x the compute capacity of the previous generation ofÂ TPUs.</li><li>Preview: Hyperdisk MLâ€Šâ€”â€Šnext generation block storage optimised for AI workloads.</li><li>Vertex AI onÂ <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>GKE Enterprise support forÂ <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>AI Model support (including Gemma and Llama) onÂ <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>Preview: <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805">Google Axion</a>. A custom ARM-based CPU. Claims 50% better performance and 60% more energy efficient than comparable current-gen x86 VMs! Google are migrating many services toÂ Axiom.</li><li>Intel 5th Gen Xeon processors.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Gemini AI 1.5 Pro in Vertex AI</a>. Googleâ€™s multimodal foundational model. It can parse 1m tokens of information!</li><li>Gemini AI 1.5 Pro now integrated with Gemini CodeÂ Assist.</li><li>Supervised tuning for GeminiÂ models.</li><li>Preview: Gemini Cloud Assist, which helps with the entire development lifecycle, including design and optimisation.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Grounding of Gemini models with Google Search</a>! This significantly reduces hallucination.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder">Vertex AI Agent Builder</a>: rapidly speed up the creation of multi-modal AIÂ agents.</li><li><a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations">Google Vids</a> will be released to Workspace labs in June. This is an AI-powered collaborative video creation app, as part of Workspace.</li><li>Imagen 2.0 is now GA in Vertex AI. Googleâ€™s most advanced text-to-image model.</li><li>Public preview: Text-to-Live Image. This creates animated video-like images from a textÂ prompt.</li><li>Public preview: Gemini inÂ Looker.</li><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiantâ€™s frontline threat intelligence using using natural languageÂ prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths andÂ risks.</li></ul><h3>The Irony Isnâ€™t Wasted OnÂ Me</h3><p>Iâ€™ve watched the keynote, and Iâ€™m summarising it here. Manually. WithoutÂ AI.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/181/0*z3Pfn-gqrDOZ2lSc.gif" /></figure><h3>Opening Keynote: The New Way toÂ Cloud</h3><p>You can see the full keynoteÂ <a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">here</a>.</p><h4>Keynote QuickÂ Thoughts</h4><ul><li>Itâ€™s all about AI. Shocking.</li><li>The biggest announcements are around Gen AI capabilities.</li><li>I think the keynote mentioned AI agents 1,806,402 times. (Okay, Iâ€™m exaggerating slightly.)</li></ul><h4>Introduction</h4><blockquote>Google are at the forefront of the AI platform shift. More than 60% of funded Gen AI startups, and nearly 90% of Gen AI unicorns are Google Cloud customers.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5JXpAkiOWGrILB4z.jpg" /></figure><p>The keynote opens with an introductory video talking the power of AI today. (<em>â€œAI you say? Iâ€™m shocked. Shocked, I tell you!â€</em>) The video talks about things we can do with AI now,Â like:</p><ul><li>Using satellites to reduce methane emissions.</li><li>Turning DNA into code to makeâ€¦ Crop-resistant corn!</li><li>Spoting and filling potholes.</li><li>Spoting diseasesÂ earlier.</li><li>Scanning 100K lines of code in 2 minutes, in order to spot and fixÂ bugs.</li></ul><p>So this is <em>â€œThe new way toÂ Cloud.â€</em></p><p>So far, soÂ cool.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1PHwomsMjmMnmhFTPJ62dw.png" /><figcaption>Google has announced partnerships with 100s of leading AIÂ partners</figcaption></figure><p>The early keynote includes a brief introduction to some of the topics of this yearâ€™sÂ Next:</p><ul><li>Over 300 customers and partners will be sharing their <strong>Gen AI success stories</strong> at thisÂ event.</li><li>Some discussion around the launch of <strong>Gemini </strong>and the advancements since itsÂ launch.</li><li><strong>Google Distributed Cloud and Edge</strong>, to support highly confidential and edge workloads.</li><li><strong>Cross-cloud networking</strong> now provides secure, low-latency connectivity of Googleâ€™s AI services to any application on anyÂ cloud.</li><li><strong>Chrome Enterprise PremiumÂ Browser</strong>.</li><li><strong>Multimodal Gen AI Agents</strong> will transform how we interact with the applications and the web. Agents are intelligent entities to do things like: customer agents, to help a shopper find the perfect dress; or helping an employee pick the right health benefits.</li></ul><h4>The AIÂ Stack</h4><p>The keynote talks about <strong>Googleâ€™s AIÂ stack</strong>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FxbiOgARL1a4-Q__qFFqfA.png" /><figcaption>Googleâ€™s AIÂ stack</figcaption></figure><ul><li>Note the rebranding of Duet AI to â€œ<a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-for-google-cloud-is-here"><strong>Gemini for GoogleÂ Cloud</strong></a>â€.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture"><strong>AI Hypercomputer</strong></a>: an integrated AI infrastructure platform for offering AI at scale. There are a number of announcements related to GPUs, TPUs, and AI-optimised storage.</li></ul><p>The keynote includes announcements around:</p><ul><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization/unlock-ai-anywhere-with-google-distributed-cloud?e=48754805"><strong>Google Distributed Cloud</strong></a>, which has a number of capability enhancements around GKE, Vertex AI, and AI model support. GDC now has both â€œsecretâ€ and â€œtop secretâ€ accreditations. Mobile operator â€œOrangeâ€ referenced as an organisation running across 26 countries and using GDC to keep data localised to eachÂ country.</li><li><a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805"><strong>Google Axion</strong></a>. A new custom ARM-based CPU that offers considerably higher performance and lowe energy consumption than caparable current genÂ x86.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates"><strong>Gemini 1.5 Pro in public preview</strong></a>. It has the worldâ€™s largest context window. In a single shot, it can process: 1M tokens, 1 hour of video, 11 hours of audio, and over 30K lines ofÂ code.</li><li><strong>Grounding of Gemini models with Google Search!</strong> This significantly reduces hallucination. Or you can ground with data from your own databases.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder"><strong>Vertex AI Agent Builder</strong></a>â€Šâ€”â€Što rapidly speed up creating AI Agents. Gemino Pro can create free-flowing conversations with text, voice, images and video as inputs. But also, it can even provide real time interactions in voice! Natural language can be used to train the AI agents, e.g. to describe topics that are verboten. You can configure transcription and summarisation. And response quality can be improved using vector search. Also, modular extensions can be integrated to complete standard customer workflows, e.g. booking aÂ flight.</li></ul><h4>Shopping AIÂ Agents</h4><p>The keynote then demonstrates a <strong>shopping AI Agent</strong>, and the ability to upload a video and askÂ it:</p><blockquote>Find me a checkered shirt like the keyboard player is wearing. Iâ€™d like to see prices, where to buy it, and how soon can I be wearingÂ it?</blockquote><p>The response is near instantaneous on the website. And then we see a demo of interacting with a <em>voice </em>AI agent which continues the interaction and completes the transaction. Thatâ€™s prettyÂ cool!</p><h4>A Few Google Workspace Updates</h4><p>Then the keynote moves onto <strong>Gemini for Google Workspace</strong>. Use itÂ to:</p><ul><li>Answer questions.</li><li>Create notes in meetings.</li><li>Extract insights fromÂ reports.</li><li>Create images to insert in presentations.</li><li>Real-time translation.</li></ul><p>Announcements related to <strong>Google Workspace</strong>:</p><ul><li>A recent benchmarking study shows <strong>Google Meet now outperforms Zoom and MS Teams</strong> for overall video performance.</li><li>Chat summarisation and real time translation now available for GoogleÂ Meet.</li><li><strong>AI Security add-on</strong> can automatically classify and protect companyÂ data.</li><li><strong>Gemini in Google Chat</strong> can provide summaries of long conversations.</li></ul><p>We see a demo of reviewing proposals, comparing them, and asking questions, e.g.</p><blockquote>Does this offer comply with our compliance ruleÂ book?</blockquote><h4>Employee Agents</h4><p>Next, we talk about how to <strong>create a multi-modal AI employee agents using VertexÂ AI</strong>:</p><ul><li>Create a custom model with VertexÂ AI.</li><li>Connect the custom model to your company data and webÂ data.</li><li>Ground in enterprise truth, e.g. with BigQuery andÂ AlloyDB.</li></ul><p>Then we see a demo of how you can use a Vertex AI employee agent to summarise an employee benefits enrollment email, as well as a one hour benefits video. The agent is able to reason across text, video and the prompt, and provide a summary. Furthermore, the agent is able to compare the proposed plan to a previous plan, and make inferences.</p><h4>Creative AIÂ Agents</h4><p>Now we move on to <strong>Creative AI Agents</strong>. Carrefore are using Creative AI Agents for marketing; they built a new marketing studio using Vertex AI, in just five weeks. Now they can build personalised campaigns in just a fewÂ clicks.</p><p>Creative agents uses Gemino Pro to look at existing material, documents and brand images, to infer a brand identity. We can generate multi-modal content; we can create live images, and even podcasts!</p><p>Then there was the announcement of <a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations"><strong>Google Vids</strong></a>, the AI-powered collaborative video creation app, as part of Google Workspace. Aparna then demos creating a recap video of the Next event, using GoogleÂ Vids:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kZo1hMT6Ne-896WcxB3sGQ.png" /><figcaption>Creating a video recap in seconds, using GoogleÂ Vids</figcaption></figure><p>Then we have announcements of <strong>Imagen 2.0 Text-to-Image</strong>, including new editing modes to edit a generated image. And thereâ€™s the new <strong>Text-to-Live Image</strong>, which is now inÂ preview:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*K2OL48TfgPnzrPn6EXgqhw.gif" /><figcaption>Generating a live image from aÂ prompt</figcaption></figure><h4>Data Agents</h4><p>So manyÂ agents!!</p><p>AI Data Agents us to ask natural language questions of our data. <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-bigquery-at-next24"><strong>Gemini in BigQuery</strong></a> is now in Preview, and allows AI-powered data preparation, analysis and querying. BigQuery can be integrated directly with Vertex AI. So now we can perform multi-modal analysis across all of documents, images, videos, audio, and structured data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LlwikgKhm7Q-kjaSvSKqig.png" /><figcaption>Querying a dataÂ agent</figcaption></figure><p>One extremely cool thing about this demo was that the agent built a forecast dynamically, using BigQuery ML. And then uses vector embeddings to find products that look like a suppliedÂ image.</p><h4>Code Agents</h4><p>Surpriseâ€¦ MoreÂ agents.</p><p>Googleâ€™s AI code assistant is now called <strong>Gemini Code Assist</strong>. (No more DuetÂ AI.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/969/1*pizqI2uBk7FGn_0Y5TBAhg.png" /><figcaption>Benefits of using CodeÂ Assist</figcaption></figure><p>The keynote talks about how Gemini Code Assist can be used with a code base anywhereâ€¦ On-prem, GitLab, GitHub, BitBucket, etc. Furthermore, Gemini Code Assist supports data residency requirements in multiple regions. It is now integrated with Gemini 1.5 Pro, and can leverage the new 1-million token contextÂ window.</p><p>The demo was coolâ€¦ Show the visual mockup of a new UI to Gemini Code Assist, and it generates the code, leveraging our entire (huge) code base, and aligned to our code standards.</p><h4>Security Agents</h4><p>Pleaseâ€¦ No moreÂ agents!</p><p>These AI agents assist security operations teams, radically increasing the speed of security investigation and response.</p><p>There were a number of announcements relating to integration of Gemini into security products:</p><ul><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiantâ€™s frontline threat intelligence using using natural languageÂ prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths andÂ risks.</li></ul><h4>Wrap-Up</h4><p>Thomas Kurian wraps-up byÂ saying:</p><blockquote>Our open platform offers choice at everyÂ layer.</blockquote><ul><li>Chips (CPUs, TPUs, GPUs) for training andÂ serving.</li><li>Your choice ofÂ models.</li><li>Your choice of development environments.</li><li>Databases, including vector.</li><li>Your choice of business applications.</li></ul><blockquote>Weâ€™re creating a new era of generative AI agents, built on a new, truly open platform for AI. And weâ€™re reinventing infrastructure to supportÂ it.</blockquote><h3>Whatâ€™s Next?</h3><p>(See what I didÂ there?)</p><p>Iâ€™ll watch a bunch of sessions Iâ€™m interested in, and provides some useful nuggets and summaries soon. Iâ€™ll put these in some separate articles, rather than just adding to thisÂ one.</p><h3>Links</h3><ul><li><a href="https://cloud.withgoogle.com/next">Google Cloud NextÂ â€˜24</a></li><li><a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">Keynote</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/google-cloud-next-2024-wrap-up">All 218 things we announced at Google Cloud NextÂ â€˜24</a></li></ul><h3>Before YouÂ Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helpsÂ me!</li><li>Please give me claps! You know you clap more than once,Â right?</li><li>Feel free to <strong>leave a comment</strong>Â ğŸ’¬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you donâ€™t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on theseÂ icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*73hF99AvDUGryMuV.png" /><figcaption>Follow and Subscribe</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f5518238c9d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d">Dazboâ€™s Google Cloud Next â€™24 Recap: Keynote</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fine tuning Gemma with LoRA on GCP]]></title>
            <link>https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/5d25dbab9e0e</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[finetune-llm]]></category>
            <category><![CDATA[lora]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[gemma]]></category>
            <dc:creator><![CDATA[pritam sahoo]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:52 GMT</pubDate>
            <atom:updated>2024-04-16T00:04:52.701Z</atom:updated>
            <content:encoded><![CDATA[<p>My obsession with Gemma continues. Folks new to the Gemma model can revisit my previous blogÂ <a href="https://medium.com/google-cloud/gemma-open-models-from-google-0045263e53d2">link</a>.</p><p>In brief Gemma is the family of lightweight, state of the art (SOTA) open models powered by the same technology powering one of the most popular Google Cloud GeminiÂ models.</p><p>In this blog we will get started with fine tuning with Gemma withÂ LoRA.</p><p>Lets understand first a bit on fine tuning. One of the reasons finetuning is picking up is the reason Large language Models(LLMs) are not trained on specific tasks or domain related data. Primarily LLMs often called as foundational models are trained on internet scale massive corpus of data, texts etc. Doing a full training of pre-trained LLM models becomes technically challenging due to expensive computational resources as one of the major concerns.</p><p>Letâ€™s understand the benefits of FineÂ tuning.</p><ol><li>Fine Tuning pre-trained model is much faster and cost effective leading to less computational resources required.</li><li>Better Performances for domain specific tasks especially on industry use cases related to Financial services, InsuranceÂ , Healthcare etc.</li><li>Lets not forget about democratization of GenAI models for individual users i.e. developers and others who have less computational power.</li></ol><p>Lets understand Parameter efficient fine tuning <a href="http://a.ka">a.k.a</a>. PEFT. Itâ€™s a subset of fine tuning which effectively utilizes parameters/weights with efficient output. Instead of altering all the parameters of the model PEFT selects a subset of them thereby reducing computational and memory requirements. PEFT plays a major role in the fine tuning process thereby improving the performance of base/foundational LLMs on specific tasks. This is super useful when training LLM models like Gemini and its different variants, PALM,even open source Gemma models etc fromÂ Google.</p><p>We will explore fine tuning Gemma Models with <strong>LoRA</strong>. <strong>LoRA</strong> stands for Low Rank Adaptation of Large Language Models. Itâ€™s a technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights/parameters of the base model and introducing a small number of new weights into theÂ model.</p><p><strong>Crucial Point to consider</strong> In LoRA, the starting point hypothesis is super importantÂ . It assumes that the pre-trained modelâ€™s weights are already close to the optimal solution for the downstream tasks.</p><p>Advantages of using LoRA as fine tuning technique</p><ol><li>Reduces Parameter and memory footprint. LoRA significantly reduces the number of trainable parameters, making it much more memory-efficient and computationally cheaper.</li><li>Fine tuning and so does inference is faster ~ as it uses less parameters/weights.</li><li>Maintains performance: LoRA has been proved to maintain performance close to traditional fine-tuning methods in severalÂ tasks.</li></ol><p><strong>So letâ€™s get started with Fine tuning with LoRA on the GemmaÂ Model.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UkJdQkyIoYu3I-OH3bJYoA.jpeg" /></figure><p>For this demo I will be using Google Collab Notebook to get some horsepower with T4Â GPUs.</p><p><strong>Step 1: Get access toÂ Gemma</strong></p><p>To complete this collab, you will first need to complete the setup instructions at <a href="https://ai.google.dev/gemma/docs/setup">Gemma setup</a>. The Gemma setup instructions show you how to do the following:</p><ul><li>Get access to Gemma on <a href="https://kaggle.com/">kaggle.com</a>.</li><li>Select a Colab runtime with sufficient resources to run the Gemma 2BÂ model.</li><li>Generate and configure a Kaggle username and APIÂ key.</li></ul><p>After youâ€™ve completed the Gemma setup, move on to the next section, where youâ€™ll set environment variables for your Colab environment.</p><p><strong>Step 2Â : Select theÂ Runtime</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/468/0*6QanOb6XI087IwsU" /></figure><h4><strong>Step 3Â : Configure your secrets i.e. username and key in AccountÂ tab</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/834/0*m_Kg4FsICCsvEH73" /></figure><p><strong>Step 4Â : Select the Data for fine tuning from hugging face. </strong><a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k"><strong>Databricks Dolly 15k dataset</strong></a><strong>. </strong>This dataset contains 15,000 high-quality human-generated prompt / response pairs specifically designed for fine-tuning LLMs. Brief screenshot of theÂ datasets</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/997/0*wmgP3pZMciNG5Ix5" /></figure><p><strong>Step 5Â : Set the environment variables and run the below commands inÂ Collab</strong></p><p>import os</p><p>from google.colab importÂ userdata</p><p>os.environ[â€œKAGGLE_USERNAMEâ€] = userdata.get(â€˜usernameâ€™)</p><p>os.environ[â€œKAGGLE_KEYâ€] = userdata.get(â€˜keyâ€™)</p><p><strong>Step 6Â : Install the dependencies</strong></p><p>!pip install -q -U keras-nlp</p><p>!pip install -q -UÂ keras&gt;=3</p><p><strong>Step 7Â : Select the backend. You may choose from PyTorch or Tensorflow orÂ Jax</strong></p><p>os.environ[â€œKERAS_BACKENDâ€] =Â â€œjaxâ€.</p><p># Avoid memory fragmentation on JAXÂ backend.</p><p>os.environ[â€œXLA_PYTHON_CLIENT_MEM_FRACTIONâ€]=â€1.00&quot;</p><p><strong>Step 8Â : Import Packages i.e. Keras and KerasNLP.</strong></p><p>import keras</p><p>import keras_nlp</p><p><strong>Step 9Â : Load the dataset from huggingÂ face.</strong></p><p>!wget -O databricks-dolly-15k.jsonl <a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl">https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl</a></p><p><strong>Step 10Â : For this demo purpose I will be using a subset of 1000 examples instead of 15K examples. For better fine tuning you may use more examples.</strong></p><p>import json</p><p>data =Â []</p><p>with open(â€œdatabricks-dolly-15k.jsonlâ€) asÂ file:</p><p>for line inÂ file:</p><p>features = json.loads(line)</p><p># Filter out examples with context, to keep itÂ simple.</p><p>if features[â€œcontextâ€]:</p><p>continue</p><p># Format the entire example as a singleÂ string.</p><p>template = â€œInstruction:\n{instruction}\n\nResponse:\n{response}â€</p><p>data.append(template.format(**features))</p><p># Only use 1000 training examples, to keep itÂ fast.</p><p>data = data[:1000]</p><p><strong>Step 11Â : Now its time to Load the Gemma 2B base Model. You may try using the Gemma 7B baseÂ model.</strong></p><p>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(â€œgemma_2b_enâ€)</p><p>gemma_lm.summary()</p><p>You will see below summary output if everything is workingÂ fine.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/0*df55cDERngDI3ig_" /></figure><p><strong>Step 11: Lets Inference the Model before fineÂ tuning.</strong></p><p>Pass the below prompt i.e. â€œ What should I do on a trip toÂ Europe?â€</p><p>prompt = template.format(</p><p>instruction=â€What should I do on a trip to Europe?â€,</p><p>response=â€â€,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>You will see very generic blant and not so great output from the base model as mentioned below</strong></p><p>â€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”</p><p><strong>Instruction:</strong></p><p><strong>What should I do on a trip toÂ Europe?</strong></p><p><strong>Response:</strong></p><p><strong>Itâ€™s easy, you just need to follow theseÂ steps:</strong></p><p><strong>First you must book your trip with a travelÂ agency.</strong></p><p><strong>Then you must choose a country and aÂ city.</strong></p><p><strong>Next you must choose your hotel, your flight, and your travel insurance</strong></p><p><strong>And last you must pack for yourÂ trip.</strong></p><p><strong>â€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”</strong></p><p><strong>Step 12: Lets fine tuning using LoRA using Databricks Dolly 15KÂ dataset.</strong></p><p>LoRA rank. It controls the expressiveness and precision of the fine-tuning adjustments.Lower rank means which requirement of computational power and also less precision adaptation. You may start with 4,8 etc for demo/experimentation purposes.</p><p>&gt;&gt; gemma_lm.backbone.enable_lora(rank=4)</p><p>&gt;&gt; gemma_lm.summary()</p><p><strong>Total params: 2,507,536,384 (9.34Â GB)</strong></p><p><strong>Trainable params: 1,363,968 (5.20Â MB)</strong></p><p><strong>Non-trainable params: 2,506,172,416 (9.34Â GB)</strong></p><p><strong>While you run the below section in the collab notebook be patient as it will take some time and you will see reduction in losses.This step will reduce the number of trainable parameters significantly.Epoch = 1 means it will run for 1 time for 1000 datasets.</strong></p><p>gemma_lm.preprocessor.sequence_length =Â 512</p><p>optimizer = keras.optimizers.AdamW( // AdamW ~ optimizer for transformer models</p><p>learning_rate=5e-5,</p><p>weight_decay=0.01,</p><p>)</p><p>optimizer.exclude_from_weight_decay(var_names=[â€œbiasâ€, â€œscaleâ€])</p><p>gemma_lm.compile(</p><p>loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),</p><p>optimizer=optimizer,</p><p>weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],</p><p>)</p><p>gemma_lm.fit(data, epochs=1, batch_size=1)</p><p><strong>The output from the above step will show significant reduction in loss with just 1000 datasets.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qf-k1hlFro2K4DUh" /></figure><p><strong>Step 13: Letâ€™s get started with Inferencing post fineÂ tuning.</strong></p><p>Pass the below prompt again i.e. â€œ What should I do on a trip toÂ Europe?â€</p><p>prompt = template.format(</p><p>instruction=â€What should I do on a trip to Europe?â€,</p><p>response=â€â€,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>**** Let me know the results. Must be better than before finetuning.</strong></p><p>Thatsâ€™ it folks on Gemma fine tuning with LoRA. Stay tuned for more updates coming your way on QLoRAâ€¦â€¦..</p><p><strong>Signing offâ€¦.Â Pritam</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5d25dbab9e0e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e">Fine tuning Gemma with LoRA on GCP</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use Log Analytics for BigQuery Usage Analysis on Google Cloud]]></title>
            <link>https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/8f5454626c6c</guid>
            <category><![CDATA[log-analytics]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[observability]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[logs]]></category>
            <dc:creator><![CDATA[Xiang Shen]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:36 GMT</pubDate>
            <atom:updated>2024-04-16T12:18:26.756Z</atom:updated>
            <cc:license>https://creativecommons.org/publicdomain/mark/1.0/</cc:license>
            <content:encoded><![CDATA[<p>On Google Cloud you can use <a href="https://cloud.google.com/logging/docs/log-analytics#analytics">Log Analytics</a> to query and analyze your log data, and then you can view or <a href="https://cloud.google.com/logging/docs/analyze/charts">chart the queryÂ results</a>.</p><p><a href="https://cloud.google.com/bigquery">BigQuery</a> is Google Cloudâ€™s fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence.</p><p>While BigQuery offers built-in observability capabilities like the <a href="https://cloud.google.com/bigquery/docs/information-schema-intro">INFORMATION_SCHEMA</a> views, detailed logging remains crucial for in-depth usage analysis, auditing, and troubleshooting potential issues.</p><p>This article will walk you through how to analyze BigQuery logs using log analytics.</p><h3>Upgrade LogÂ bucket</h3><p>First, if you havenâ€™t, you need to configure Cloud Logging to upgrade all the existing log buckets with Log Analytics enabled.</p><p>To upgrade an existing bucket to use Log Analytics, do the following:</p><ol><li>In the navigation panel of the Google Cloud console, select <strong>Logging</strong>, and then select <strong>LogsÂ Storage.</strong></li><li>Locate the bucket that you want toÂ upgrade.</li><li>When the <strong>Log Analytics available</strong> column displays <strong>Upgrade</strong>, you can upgrade the log bucket to use Log Analytics. Click <strong>Upgrade</strong>.<br>A dialog opens. ClickÂ <strong>Confirm</strong>.</li></ol><h3>Perform BigQuery Activities</h3><p>Complete the following tasks to generate some BigQuery logs. In the tasks, the BigQuery command line tool <a href="https://cloud.google.com/bigquery/docs/reference/bq-cli-reference">bq</a> isÂ used.</p><p><strong>Task 1. CreateÂ datasets</strong></p><p>Use the <strong>bq mk</strong> command to create new datasets named <strong>bq_logs</strong> and <strong>bq_logs_test </strong>in yourÂ project:</p><pre>bq mk bq_logs<br>bq mk bq_logs_testbq mk bq_logs_test</pre><p><strong>Task 2. List theÂ datasets</strong></p><p>Use the <strong>bq ls</strong> command to list the datasets:</p><pre>bq ls</pre><p><strong>Task 3. Delete aÂ dataset</strong></p><p>Use the <strong>bq rm</strong> command to delete the a dataset (select <strong>y</strong> when prompted):</p><pre>bq rm bq_logs_test</pre><p><strong>Task 4. Create a newÂ table</strong></p><pre>bq mk \<br> --table \<br> --expiration 3600 \<br> --description &quot;This is a test table&quot; \<br> bq_logs.test_table \<br> id:STRING,name:STRING,address:STRING</pre><p>You should have a new empty table named <strong>test_table</strong> that has been created for yourÂ dataset.</p><p><strong>Task 5. Run some exampleÂ queries</strong></p><p>You can run a simple query like the following to generates a log entry. Copy and paste the following query into the BigQuery QueryÂ editor:</p><pre>bq query â€” use_legacy_sql=false â€˜SELECT current_dateâ€™</pre><p>The following query will leverage weather data from the <a href="https://cloud.google.com/blog/products/data-analytics/noaa-datasets-on-google-cloud-for-environmental-exploration">National Oceanic and Atmospheric Administration (NOAA)</a>. Copy the query into the BigQuery editor and clickÂ <strong>RUN</strong>.</p><pre>bq query --use_legacy_sql=false \<br>&#39;SELECT<br> gsod2021.date,<br> stations.usaf,<br> stations.wban,<br> stations.name,<br> stations.country,<br> stations.state,<br> stations.lat,<br> stations.lon,<br> stations.elev,<br> gsod2021.temp,<br> gsod2021.max,<br> gsod2021.min,<br> gsod2021.mxpsd,<br> gsod2021.gust,<br> gsod2021.fog,<br> gsod2021.hail<br>FROM<br> `bigquery-public-data.noaa_gsod.gsod2021` gsod2021<br>INNER JOIN<br> `bigquery-public-data.noaa_gsod.stations` stations<br>ON<br> gsod2021.stn = stations.usaf<br> AND gsod2021.wban = stations.wban<br>WHERE<br> stations.country = &quot;US&quot;<br> AND gsod2021.date = &quot;2021-12-15&quot;<br> AND stations.state IS NOT NULL<br> AND gsod2021.max != 9999.9<br>ORDER BY<br> gsod2021.min;&#39;</pre><h3>Perform logÂ analysis</h3><p>Now there are some log entries for BigQuery. You can run some queries using Log Analytics.</p><p><strong>Task 1. Open Log Analytics</strong></p><p>On the left side, under <strong>Logging</strong> click <strong>Log Analytics</strong> to access the feature. You should see something like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6PrYdj0eZePE3wF8" /></figure><p>If your query field is empty or you forget which table you want to use, you can click the <strong>Query</strong> button to get the sample queryÂ back.</p><p>Now you can run your own queries in the query field. Remember to replace <strong>[Your Project Id]</strong> with the project id you areÂ using.</p><p><strong>Task 2. To find the activities for BigQueryÂ datasets</strong></p><p>You can query the activities that a dataset is created orÂ deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;datasetservice%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QgplyJgYAw8L93Rn" /></figure><p><strong>Task 3. To find the activities for BigQueryÂ tables</strong></p><p>You can query the activities that a dataset is created orÂ deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;%TableService%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*REyZPbwK41Vfotxk" /></figure><p><strong>Task 4. To view the queries completed inÂ BigQuery</strong></p><p>Run the following query:</p><pre>SELECT<br> timestamp,<br> resource.labels.project_id,<br> proto_payload.audit_log.authentication_info.principal_email,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.query) AS query,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.statementType) AS statementType,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatus.error.message) AS message,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS startTime,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS endTime,<br> CAST(TIMESTAMP_DIFF( CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS TIMESTAMP), CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS TIMESTAMP), MILLISECOND)/1000 AS INT64) AS run_seconds,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalProcessedBytes) AS INT64) AS totalProcessedBytes,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalSlotMs) AS INT64) AS totalSlotMs,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.referencedTables) AS tables_ref,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalTablesProcessed) AS INT64) AS totalTablesProcessed,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.queryOutputRowCount) AS INT64) AS queryOutputRowCount,<br> severity<br>FROM<br> `[Your Project Id].global._Default._Default`<br>WHERE<br> log_id = &quot;cloudaudit.googleapis.com/data_access&quot;<br> AND proto_payload.audit_log.service_data.jobCompletedEvent IS NOT NULL<br>ORDER BY<br> startTime</pre><p>After the query completes, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yIhioPqh3mZwXguM" /></figure><p>Scroll through the results of the executedÂ queries.</p><p><strong>Task 5. To chart the queryÂ result</strong></p><p>Instead of using a table to see the results, Log Analytics also supports creating charts for visualization. For example, to view a pie chart for the queries that have run, you can click the <strong>Chart</strong> button in the result view, select <strong>Pie chart</strong> as the chart type and <strong>query</strong> as the column. You should see a chart similar to the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HoiWqDgj6gqLAp58" /></figure><p>Weâ€™ve only scratched the surface of BigQuery log analysis; you can explore many other queries and charts to enhance your understanding of BigQuery. Feel free to contribute and create samples in <a href="https://github.com/GoogleCloudPlatform/observability-analytics-samples/tree/main/samples/logging">GCPâ€™s sample GitHub repository</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f5454626c6c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c">Use Log Analytics for BigQuery Usage Analysis on Google Cloud</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Landing Zone Technical Onboardingâ€” the â€œHow-Toâ€ (Google Cloud Adoption Series)]]></title>
            <link>https://medium.com/google-cloud/landing-zone-technical-onboarding-the-how-to-google-cloud-adoption-series-9b7ba8710e83?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/9b7ba8710e83</guid>
            <category><![CDATA[technical-design]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud-partner]]></category>
            <category><![CDATA[landingzone]]></category>
            <category><![CDATA[project-team]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Mon, 15 Apr 2024 08:06:46 GMT</pubDate>
            <atom:updated>2024-04-15T13:45:31.192Z</atom:updated>
            <content:encoded><![CDATA[<p>Welcome to the latest installment in the <a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Google Cloud Adoption and Migration: From Strategy to Operation</a> series.</p><p>Previously, we covered the various considerations that are part of LZ design. In this part weâ€™llÂ cover:</p><ul><li>Establishing your <strong>LZ Core ProjectÂ Team</strong>.</li><li>Establishing the <strong>support </strong>youÂ need.</li><li><strong>Workshopping </strong>the design considerations.</li><li>Agreeing and documenting your <strong>LZ design decisions</strong> in the <strong>LZ Design Document</strong>.</li><li><strong>Deploy!</strong></li></ul><h3>Technical Onboarding?</h3><p>Google refers to the overall process of setting up a Cloud environment as <em>â€œTechnical Onboardingâ€</em>. Historically it was referred to as <em>â€œCloud Foundationâ€</em>. It includes:</p><ol><li>Kick-off, scoping andÂ planning</li><li>Understanding organisational capability needs</li><li>Workshopping</li><li>Implementation (deployment)</li></ol><p>But before you can execute any steps, youâ€™ll need a core projectÂ team.</p><h3>Establishing Your LZ ProjectÂ Team</h3><p>If youâ€™re working through the process of designing your LZ, then itâ€™s probably fair to say that your maturity with Google Cloud is quite low. If you were mature in Google Cloud, youâ€™d already have your well-designed LZ! So the first thing youâ€™ll need to do is <strong>establish the core LZ project team</strong>, who will be responsible for:</p><ul><li>Working through the various LZ design considerations that I have outlined in the previous fewÂ parts.</li><li>Capturing your LZ design decisions.</li><li>Deploying yourÂ LZ.</li></ul><p><strong>What should this LZ core team look like?</strong> My recommendations are that you include the following:</p><ul><li>A <strong>lead cloud architect</strong>. This needs to be someone who fully understands cloud, and your organisationâ€™s cloud strategy. Possibly, they were the enterprise cloud architect who created the organisationâ€™s cloud strategy in the first place. This person will ultimately own the technical deliverables, and have the final say on the design decisions.</li><li><strong>A project manager.</strong> For obviousÂ reasons!</li><li>A <strong>Platform / DevOps Lead</strong>. This is someone who has a strong understanding of Google Cloud, DevOps, and Terraform IaC. This person will ultimately be responsible for deployment of the LZ. They are likely also a key member of the <strong>Cloud Platform Team</strong>, which <em>may not yet formally exist at this point in the journey. </em>(And it is likely that this person will be a pivotal member of the new Cloud PlatformÂ Team.)</li><li>An <strong>SRE Lead</strong>. This person will be concerned with observability and embedding <a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE best practice</a>. They are likely a key member of the <strong>SRE Team</strong>, <em>which may not yet formallyÂ exist.</em></li><li>A <strong>network architect</strong>. Someone who understands your existing network topology, switching and routing, and your network strategy. This person will be well placed to describe the options and constraints, particularly when agreeing hybrid connectivity, DNS options,Â etc.</li><li>A <strong>security architect</strong>. Someone who understands the organisational security requirements, as well as existing capabilities and use cases for technologies like firewalls, IDS/IPS, WAF, and proxies. They will also be aware of the security strategy.</li><li><strong>Google Cloud SMEs</strong>. You need a some Google Cloud specialists who collectively have strong knowledge and experience of Google Cloud LZs, and of all the products and services that are fundamental the LZ, such as IAM, networking, GKE, monitoring and alerting, etc.</li></ul><p>Which brings us onÂ toâ€¦</p><h3>Support</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iwfCzdjGNwy9VRke" /></figure><p>Again: if youâ€™re in the process of designing and building your LZ, itâ€™s quite likely that your organisation doesnâ€™t yet have the necessary expertise and experience to design and deploy the LZ. Youâ€™re going to need someÂ help.</p><p>For this, I would recommend engaging with Google, or with a <a href="https://cloud.google.com/find-a-partner/">Google Cloud Partner</a> that offers a <strong>Landing Zone design and build service</strong>. During the LZ design and build phase, you can supplement your internal team with all the expertise (and experience) that you need. For example, the Google Cloud partner can supply youÂ with:</p><ul><li>A<strong> Google Cloud Architect</strong> who is an expert in Google Cloud and LZÂ design.</li><li><strong>Additional experts</strong> who can handle any specific queries that the architect doesnâ€™t have the answers to. These additional resources could be deployed into your core team, or simply be available as â€œbackground resourcesâ€ that your partner can tapÂ into.</li><li><strong>DevOps engineering capability</strong>, to either build and deploy your IaC, or to support and guide your own PlatformÂ Team.</li></ul><p>Additionally, such a partner can assist youÂ to:</p><ul><li>Build your <a href="https://medium.com/@derailed.dash/google-cloud-adoption-organisational-change-capabilities-upskilling-and-cloud-centre-of-15bc49ae7ae6">CCoE capability</a>.</li><li>Build your PlatformÂ Team.</li><li>Build your <a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE capability</a>.</li><li>Execute an initial migration PoC.</li><li>Help you build an application migration factoryÂ team.</li></ul><p>The partner can advise on organisational structure, provide resource augmentation until your organisation is self sufficient, and help upskill your internalÂ staff.</p><p>It would be remiss of me not to mention <a href="https://www.epam.com/?gad_source=1&amp;gclid=Cj0KCQjwlN6wBhCcARIsAKZvD5h7jf_UxQE04IrMPl5G9rxYkX2YhMjUgUVU_jPCnO9AWjz9fnPgXLkaAiX0EALw_wcB"><strong>EPAM</strong></a><strong> </strong>at this stage, because a) they are a <strong>Google Cloud Premier Partner</strong> that offers such a service; and b) I happen to work forÂ them!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*az-PumqTxzgAcYpE.png" /><figcaption>EPAMâ€Šâ€”â€ŠA Google premierÂ partner</figcaption></figure><p>They are multinational, with around 50000 engineers and consultants. They have over 1200 Google Cloud certified experts, and they have won <strong>Google Cloud Partner of the Year</strong> in 2018, 2023 andÂ 2024!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/868/1*gv58BYiOq362nTAF4flxgQ.png" /><figcaption>EPAMâ€™s credentials</figcaption></figure><p>Iâ€™ve worked with a lot of consultancies over my career (though typically, as the client), and I can honestly say: <strong>EPAM are the gold standard</strong>.</p><p>If you do want to engage with EPAM, you can reach out through the link above, or you can <a href="https://github.com/derailed-dash">connect with me directly</a>.</p><h3>Workshops</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RejtZ3_J4Zzn-rtG.jpg" /><figcaption>Tried-and-Tested</figcaption></figure><p>Now youâ€™ve got your project team and engaged with the support you need, itâ€™s time to crack on with some workshops!</p><p><strong>Googleâ€™s tried-and-tested approach is to hold a series of workshops</strong>, aligned to the various LZ design considerations that Iâ€™ve previously covered. (Google Cloud partners will typically use a similar approach.) I would recommend a 2 hour workshop for each of the following topics:</p><ul><li><strong>Initiation:</strong> LZ goals; current state; overall project scope; confirming roles and responsibilities; ways-of-working.</li><li><strong>Identity and Access Management:</strong> IAM; roles; master IdP decisions; Google Workspace superadmins; Google Cloud org admins; other groups; IdP integration; SSO andÂ MFA.</li><li><strong>Resource Hierarchy and Management:</strong> org and folder structure; org policies; tenants and project factory principles; environments and sandboxes.</li><li><strong>Network and Security: </strong>hybrid connectivity; shared VPC topology; VPC-SC; firewall; ingress and egress (including Internet connectivity) patterns; current network/appliance considerations; DDoS and WAF (e.g. with Cloud Armor); DNS; DR and region considerations; org policiesÂ revisit.</li><li><strong>Compute and GKE:</strong> GKE strategy; multitenant cluster design; fleet design; GKE address ranges; release channels; Workload Identity Federation; Anthos service mesh; IaaS OS management and upgrade/patching strategy.</li><li><strong>Operations and Visibility: </strong>monitoring, logging and alerting; predefined and custom dashboards; metrics, including GKE and Istio metrics, and Ops Agent; metrics scope (aggregation and isolation) design; SIEM considerations; audit logging; network service logging; logging aggegration and organisational log sinks; log archiving and exports; integration with other operations software (e.g. on-prem); SRE considerations.</li><li><strong>Billing and Cost Optimisation: </strong>billing roles; tenant/project cost visibility; billing exports; project budgets; sandbox budgets; budget alerts; labelling strategy and standards.</li><li><strong>Automation, GitOps and Foundation Enablement:</strong> IaC, GitOps and CI/CD; LZ IaC frameworks and accelerators (e.g. Google Fabric FAST and Google CFT); IaC policy enforcement; project factory; tenant onboarding processes and support. (Iâ€™m going to cover tenant enablement later in theÂ series.)</li></ul><p>So, thatâ€™s eight workshops with the core LZ team, and you can bring in specialists as required.</p><h3>Documenting Your LZÂ Design</h3><p>As you progress through the workshops, document the design decisions as you go. Capture your overall design and decisions in an artefact that Google calls the <strong>LZ Technical Design DocumentÂ (TDD)</strong>.</p><p>This doc shouldÂ include:</p><ul><li>Introduction and scope of the solution.</li><li>A summary of all design decisions. These should point to the relevant sections in the document.</li><li>Sections corresponding to each of the workshops.</li></ul><h3>Deploy!</h3><p>Iâ€™m going to cover this in the next installment!</p><h3>Before YouÂ Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helpsÂ me!</li><li>Please <strong>give me claps</strong>! You know you clap more than once,Â right?</li><li>Feel free to <strong>leave a comment</strong>Â ğŸ’¬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you donâ€™t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on theseÂ icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*fF62z2-FT03ui0O5.png" /><figcaption>Follow and Subscribe</figcaption></figure><h3>Links</h3><ul><li><a href="https://medium.com/google-cloud/landing-zones-on-google-cloud-b42b08e1abaa">Landing Zones on Google Cloud: What It Is, Why You Need One, and How to CreateÂ One</a></li><li><a href="https://cloud.google.com/find-a-partner/">Google CloudÂ Partners</a></li><li><a href="https://www.epam.com/?gad_source=1&amp;gclid=Cj0KCQjwlN6wBhCcARIsAKZvD5h7jf_UxQE04IrMPl5G9rxYkX2YhMjUgUVU_jPCnO9AWjz9fnPgXLkaAiX0EALw_wcB">EPAM</a></li><li><a href="https://cloud.google.com/architecture/framework">Google Cloud Architecture Framework</a></li><li><a href="https://cloud.google.com/architecture/security-foundations">Enterprise Foundations Blueprint</a></li></ul><h3>Series Navigation</h3><ul><li><a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Series overview and structure</a></li><li><a href="https://medium.com/@derailed.dash/google-cloud-adoption-organisational-change-capabilities-upskilling-and-cloud-centre-of-15bc49ae7ae6">Org change, upskilling and CCoE Establishment</a></li><li><a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE bestÂ practice</a></li><li>Previous: <a href="https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-4-iac-gitops-and-ci-cd-google-cloud-ae3f533c6dbd">Design your Landing Zoneâ€Šâ€”â€ŠDesign Considerations Part 4: IaC, GitOps andÂ CI/CD</a></li><li>Next: Technical Onboarding and Landing Zone Deployment</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9b7ba8710e83" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/landing-zone-technical-onboarding-the-how-to-google-cloud-adoption-series-9b7ba8710e83">Landing Zone Technical Onboardingâ€” the â€œHow-Toâ€ (Google Cloud Adoption Series)</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Potentials Network Security Issues In Google Cloud Platform You Need To Know]]></title>
            <link>https://medium.com/google-cloud/potentials-network-security-issues-in-google-cloud-platform-you-need-to-know-ef1fa64e01dd?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/ef1fa64e01dd</guid>
            <category><![CDATA[google-cloud-networking]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-security-operations]]></category>
            <category><![CDATA[google-cloud-security]]></category>
            <category><![CDATA[network-security]]></category>
            <dc:creator><![CDATA[Dolly Aswin]]></dc:creator>
            <pubDate>Mon, 15 Apr 2024 08:06:31 GMT</pubDate>
            <atom:updated>2024-04-15T13:55:43.507Z</atom:updated>
            <content:encoded><![CDATA[<h3>The Potentials Network Security Issues In Google Cloud Platform You Need ToÂ Know</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Cv7Zt-zcepDs_3bOb1fytg.png" /></figure><p><a href="https://cloud.google.com/">Google Cloud Platform</a> (GCP) offers several features that can make network management streamlined and efficient compared to traditional on-premise deployments. There are some benefits of using network management inÂ GCP:</p><ul><li><strong>Simplified Setup and Management<br></strong>Automation and pre-configured settings make network configuration faster and less error-prone.</li><li><strong>Scalability on Demand<br></strong>The platform automatically scales network resources to meet your applicationâ€™s fluctuating trafficÂ demands.</li><li><strong>Flexible Customization<br></strong>GCP offers a wide range of networking options and services to tailor your network environment to your specificÂ needs.</li><li><strong>Enhanced Security<br></strong>Built-in security features like firewalls and access controls provide a strong foundation for protecting yourÂ network.</li><li><strong>Reduced Operational Overhead<br></strong>Managed services handle complex network tasks like load balancing and route optimization, freeing up your teamâ€™sÂ time.</li></ul><p>Google Cloud Platform (GCP) strives to make network deployment and management as frictionless and efficient as possible, it<strong> </strong>empowers you to focus on your application development while ensuring a secure and efficient network environment.</p><h3>Potential Network Security Issues inÂ GCP</h3><p>There is no such thing as a perfect network security settings, not even on Google Cloud Platform (GCP). Security is an ongoing process that requires constant vigilance and adaptation. These caused of these following things:</p><ul><li><strong>The Evolving Threat Landscape<br></strong>Hackers are constantly developing new methods to exploit vulnerabilities. Security settings that might be sufficient today could become obsolete tomorrow as new threatsÂ emerge.</li><li><strong>Human Error<br></strong>Accidental mistakes during configuration or management can introduce vulnerabilities. Even with GCPâ€™s automation features, human oversight still plays aÂ role.</li><li><strong>Shared Responsibility Model<br></strong>In cloud environments like GCP, security is a shared responsibility. While GCP provides a secure platform, you are ultimately responsible for configuring and managing your resources securely.</li></ul><p>Here are some potentials network security issues you face when deploying an application to Google Cloud Platform (GCP) can be broadly categorized into four mainÂ areas:</p><h4><strong>1. Misconfiguration</strong></h4><ul><li><strong>Overly Permissive Access Control<br></strong>Granting excessive permissions through firewall rules or <a href="https://cloud.google.com/security/products/iam">Identity and Access Management (IAM)</a> can create vulnerabilities. Accidentally allowing access to more resources than necessary increases the attackÂ surface.</li><li><strong>Insecure Service Defaults<br></strong>Using default configurations for GCP services might expose vulnerabilities. Not customizing security settings for services can leave them susceptible toÂ attacks.</li><li><strong>Public IP Addresses<br></strong>Exposing your application directly to the public internet without proper access restrictions makes it more vulnerable to unauthorized access attempts.</li></ul><h4><strong>2. Unsecured Communication</strong></h4><ul><li><strong>Unencrypted Data Transfer<br></strong>Sensitive information transmitted over the network without encryption (<a href="https://en.wikipedia.org/wiki/HTTPS">HTTPS</a>) is vulnerable to interception by attackers. This includes communication between your application and users, as well as internal communication within your GCP environment.</li><li><strong>Lack of Internal Encryption<br></strong>If communication between different components of your application within GCP isnâ€™t encrypted, data might be exposed even within the platform.</li></ul><h4><strong>3. Outdated Security Practices</strong></h4><ul><li><strong>Unpatched Systems<br></strong>Failing to update software and operating systems with security patches leaves them susceptible to known exploits. Hackers can easily exploit these vulnerabilities to gain unauthorized access.</li><li><strong>Weak Password Management<br></strong>Reusing passwords or using weak passwords for user accounts, databases, or services significantly increases the risk of unauthorized access.</li><li><strong>Lack of Security Monitoring<br></strong>Not having proper tools in place to monitor network activity, user access logs, and system logs makes it difficult to detect suspicious behavior and potential attacks.</li></ul><h4><strong>4. Service Misconfigurations</strong></h4><ul><li><strong>Unintended Resource Sharing<br></strong>Accidentally sharing resources with other projects or users within GCP can lead to unauthorized access to your application data or resources.</li><li><strong>Misconfigured Security Groups<br></strong>Security groups act as firewalls within GCP. Incorrect configuration can leave resources exposed or restrict legitimate access.</li></ul><h3>Mitigating TheÂ Problem</h3><p>Mitigating the potentials network security issues in Google Cloud Platform (GCP) is possible. But thereâ€™s no one-size-fits-all recipe for mitigating network security issues in GCP because security is an ongoingÂ process.</p><p>However, there are best practices and strategies you can implement to significantly reduce risks and create a strong security posture inÂ GCP:</p><h4><strong>1. The Principle of Least Privilege</strong></h4><p>Grant users and services only the minimum permissions required to perform their tasks. This minimizes the potential damage if a security breachÂ occurs.</p><h4><strong>2. Access Control Enforcement</strong></h4><p>Utilize Identity and Access Management (IAM) policies and firewalls to restrict access to your resources. Define granular access controls to limit who can access what, when, and fromÂ where.</p><h4><strong>3. HTTPS Everywhere</strong></h4><p>Enforce HTTPS encryption for all communication within your application and between your application and users. This ensures data confidentiality by scrambling it during transmission.</p><h4>4. <strong>Patching Regularly</strong></h4><p>Maintain a regular patching schedule to keep your software and operating systems updated with the latest security fixes. These patches address known vulnerabilities that attackers mightÂ exploit.</p><h4><strong>5. Strong Password Management</strong></h4><p>Enforce strong password policies that require complex passwords and consider implementing <a href="https://en.wikipedia.org/wiki/Multi-factor_authentication">Multi Factor Authentication (MFA)</a> for added security.</p><h4><strong>6. Network Activity Monitoring</strong></h4><p>Utilize Cloud Monitoring and other security tools to monitor network activity, user access logs, and system logs for suspicious behavior. This helps you detect potential threats earlyÂ on.</p><h4><strong>7. SecurityÂ Audits</strong></h4><p>Conduct regular security audits to assess your overall security posture and identify any vulnerabilities that might exist. <a href="https://en.wikipedia.org/wiki/Penetration_test">Penetration testing</a>, where ethical hackers attempt to exploit vulnerabilities, can be particularly valuable.</p><h3><strong>GCP Features for EnhancedÂ Security</strong></h3><p>Beyond those best practices and strategies above, GCP also offers various built-in features that contribute to a secure network environment:</p><ul><li><strong>Automated Security Features<br></strong><a href="https://cloud.google.com/firewall/docs/firewalls">Firewalls</a> and access controls provide a strong foundation for network security.</li><li><strong>Security Command Center<br></strong><a href="http://loud.google.com/security/products/security-command-center">This central hub</a> provides visibility into security threats and helps you manage security posture across your GCP resources.</li><li><strong>Managed Services<br></strong>GCP offers managed services like <a href="https://cloud.google.com/security/products/security-key-management">Cloud Key Management Service (KMS) </a>for secure key storage and <a href="https://cloud.google.com/identity">Cloud Identity</a> for centralized identity management, reducing your administrative burden.</li></ul><p>Security is a shared responsibility in GCP. While Google provides a secure platform, you are ultimately responsible for configuring and managing your resources securely. By implementing these strategies, leveraging GCPâ€™s security features, and staying informed about evolving threats, you can significantly reduce network security risks in your cloud environment.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ef1fa64e01dd" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/potentials-network-security-issues-in-google-cloud-platform-you-need-to-know-ef1fa64e01dd">Potentials Network Security Issues In Google Cloud Platform You Need To Know</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kubeflow Summit Europe 2024 âœ¨]]></title>
            <link>https://medium.com/google-cloud/kubeflow-summit-europe-2024-ed60cada2f03?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/ed60cada2f03</guid>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[kubeflow]]></category>
            <category><![CDATA[cloud-native]]></category>
            <dc:creator><![CDATA[Malek ZAAG]]></dc:creator>
            <pubDate>Fri, 12 Apr 2024 12:26:03 GMT</pubDate>
            <atom:updated>2024-04-12T12:26:02.994Z</atom:updated>
            <content:encoded><![CDATA[<p>In this blog, we are going to highlight some keynotes of the Kubeflow Summit Europe 2024 which was held this year at Paris. Unfortunately, i couldnâ€™t assist physically but i watched lately the cncf playlist on youtube and tried to do a small wrapÂ up.</p><h3>ğŸ’»What is KubeflowÂ ?</h3><p>Kubeflow is a Kubernetes-native, open-source framework for developing, managing, and running machine learning (ML) workloads. Kubeflow is an AI/ML platform that brings together several tools covering the main AI/ML use cases: data exploration, data pipelines, model training, and modelÂ serving.</p><h3>What is Kubeflow usedÂ for?</h3><p>Kubeflow solves many of the challenges involved in orchestrating machine learning pipelines by providing a set of tools and <a href="https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces">APIs</a> that simplify the process of training and deploying ML models atÂ scale.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/271/1*rYv0ZLAHY6tYZydMto3eBw.png" /><figcaption>Kubeflow pipeline</figcaption></figure><p>Now we defined what is Kubeflow, letâ€™s start talking about the keynotes and what they brang to us this yearÂ :</p><h3>ğŸ¤–Scalable Platform for Training and Inference Using Kubeflow atÂ CERN</h3><p>The <strong>European Organization for Nuclear Research</strong>, known as <strong>CERN</strong> is an intergovernmental organization that operates the largest particle physics laboratory in theÂ world.</p><p>This talk will go into the details of how a kubeflow based machine learning platform handles all the steps from data preparation, interactive analysis, distributed training and inference.</p><p>The requirements at CERNÂ :</p><ul><li>The platform should manage the full machine learning lifecycle Using multiple services can be confusing and hard to integrate.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/703/1*G5s1ZtE7MPx3xDGqwQcwdw.png" /><figcaption>MLOps lifecycle</figcaption></figure><ul><li>The platform needs to be integrated with CERN systems Auth, storage systems,Â etcâ€¦</li><li>The platform should be centralized to ensure easy and efficient access to GPUs and other accelerators.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/697/1*YtYQ7G-HtoFD9KrjjFlbgQ.png" /><figcaption>Reasons for centralizing resources</figcaption></figure><ul><li>The platform should be easy to use many scientists are not infrastructure experts.</li></ul><h4>âš›How MLOPS and Kubeflow are used at CERNÂ ?</h4><p>ATLAS is one of two general-purpose detectors at the Large Hadron Collider (LHC). It investigates a wide range of physics, from the Higgs boson to extra dimensions and particles that could make up darkÂ matter.</p><blockquote>I want to find Higgs bosons in the recorded collisions to studyÂ them.</blockquote><p>And this was their pipeline workflow to study the Higgs bosons particles.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uyu37xBGm6rGxCFU11_IYg.png" /><figcaption>CERN AtlasÂ pipeline</figcaption></figure><p><strong>Salt: </strong>General-purpose software to train multi-modal, multi-task transformer models.</p><p><strong>Katib:</strong> Used within Kubeflow to tune model Hyperparmeters.</p><p><strong>Kubeflow Notebooks:</strong> Store notebooks to be run in containers.</p><p><strong>Ceph: </strong>an open-source, distributed storageÂ system.</p><h3>Transforming Data Science at PepsiCo: The Kubeflow Revolution</h3><p>Kubeflow is also used at Pepsi and this is for many reasonsÂ :</p><ul><li>We already have K8S clusters and infrastructure team to<br>maintainÂ it</li><li>Lots of data deserves lots ofÂ models</li><li>Hyperparameters tuning -&gt;Â Katib</li><li>Serve models -&gt;Â KServe</li><li>Model training -&gt; training operators</li></ul><h4>The need for KubeflowÂ ?</h4><p>There was several reasons for using kubeflow at PepsiCoÂ :</p><ul><li>Production isÂ PAINFUL</li><li>With all the gaps Data Science was left to fend for themselves.</li><li>A lot of non-efficient work, going to production (or even staging) is aÂ slog.</li></ul><p>This led to creating multiple solutions that works with kubeflow to bring the best to the AI/ML ecosystem like the Monorepo for all of Data Science/AI projectÂ :</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/541/1*qHcDzBczY0PuyoPinSss-w.png" /><figcaption>Monorepo benefits</figcaption></figure><p>Or even the Prometheus CLI that is built on top of the <strong>kfp</strong>Â SDK:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/444/1*p8yRdZztyx5OlA-p5UASkA.png" /><figcaption>Prometheus cliÂ features</figcaption></figure><h4>ğŸ”„Culture Shift atÂ PepsiCo</h4><ul><li>None of the code we built matters without rethinking our relationship to Kubeflow.</li><li>If all we built was better tooling for a broken workflow, there would be no fundamental change.</li></ul><h3>The Good, the Bad, and the Missing Parts ofÂ Kubeflow</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/326/1*cP606VqXOXJkKPWdIZBjxg.png" /><figcaption>Kubeflow</figcaption></figure><h4>ğŸ˜„The goodÂ parts:</h4><ul><li>pipelines</li><li>notebooks, katib,Â kserve</li></ul><h4>ğŸ˜The badÂ parts:</h4><ul><li>documentation, tutorials, installation</li></ul><h4>ğŸ¤”The missingÂ parts:</h4><ul><li>Monitoring models</li><li>Model registry</li><li>Initial setup</li></ul><h4>Whatâ€™s coming for kubeflowÂ ?</h4><ul><li>finish cncf graduation</li><li>establish a TOC (technical oversigh tcommitte)</li><li>arm64 support</li><li>conformance testing</li></ul><h3>ğŸ§ AutoML &amp; Training Working GroupÂ Updates</h3><p>AutoML working group (WG) is responsible for all aspects of AutoML features on Kubeflow with Katib as the sub-project. Katib is a Kubernetes-native project with rich support for HyperParameter tuning, Neural Architecture Search, and Early Stopping algorithms.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/769/1*NJyayYjUBsFkBpzEGVvvmQ.png" /><figcaption>Katib features</figcaption></figure><h4>Katib architecture</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DFrIJ9Std7EY_gkp7RePHA.png" /><figcaption>Katib architecture</figcaption></figure><h4>Katib futureÂ ?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/635/1*nDdJCyqqquAxV_FviVpf0g.png" /></figure><h4>Training operatorÂ overview</h4><p>Kubeflow Training Operator is a Kubernetes-native project for fine-tuning and scalable distributed training of machine learning (ML) models created with various ML frameworks such as PyTorch, TensorFlow, XGBoost, andÂ others.</p><p>User can integrate other ML libraries such as <a href="https://huggingface.co/">HuggingFace</a>, <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, or <a href="https://github.com/NVIDIA/Megatron-LM">Megatron</a> with Training Operator to orchestrate their ML training on Kubernetes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/790/1*Cv8dwiSuJZaPcVLHF7j80Q.png" /><figcaption>Training OperatorÂ features</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/719/1*gkEN7mITUsh_BdYoTdTghg.png" /><figcaption>Example of distributed training forÂ PyTorch</figcaption></figure><h4>Training Operator RoadmapÂ :</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/756/1*Yz5Mr_HJfvACp_jTazPUBQ.png" /></figure><h4>ConclusionÂ :</h4><p>With this AI trend and need for performant and cost effective deployment strategies for ML models, kubeflow can be an interesting option for companies that havenâ€™t already migrated to cloud-native environments.</p><p>â€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”Â â€”</p><p>Was this helpful? Confusing? If you have any questions, feel free to contactÂ me!</p><p>Before youÂ leave:</p><p>ğŸ‘ Clap for theÂ story</p><p>ğŸ“° Subscribe for more posts like this @malek.zaag âš¡ï¸</p><p>ğŸ‘‰ğŸ‘ˆ Please follow me: <a href="https://github.com/Malek-Zaag">GitHub </a>|Â <a href="https://www.linkedin.com/in/malekzaag/">LinkedIn</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ed60cada2f03" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/kubeflow-summit-europe-2024-ed60cada2f03">Kubeflow Summit Europe 2024 âœ¨</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building an intelligent alerting with Gemini & Function Calling]]></title>
            <link>https://medium.com/google-cloud/building-an-intelligent-alerting-with-gemini-function-calling-c1981d38fa94?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/c1981d38fa94</guid>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gemini]]></category>
            <dc:creator><![CDATA[CC]]></dc:creator>
            <pubDate>Fri, 12 Apr 2024 01:29:03 GMT</pubDate>
            <atom:updated>2024-04-12T01:29:03.730Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>We use alerting to send an alarm to the operation team to solve or mitigate problems as soon as possible. Itâ€™s normally going to take a relatively long time to investigate issues and then come up with clear solutions to combat the issues. Iâ€™ve always imagined it would be nicer to have a altering along side a solution, in turn helping the engineer quickly work on problem solving rather than tedious process. The Large Language Models (LLMs) provide us with this capability to move towards this direction. This article is going to explore how we can leverage Gemini and native features to address theseÂ issues.</p><p>Letâ€™s assume our real world scenarios are the following:</p><ul><li>GKE as production environment</li><li>Microservices are running on top ofÂ GKE</li><li>Using Google Operation Suite as a centralized monitoring platform</li><li>Weâ€™re getting an alerting message triggered by breaching the metric - <strong>kubernetes.io/container/restart_count</strong> from a GKEÂ cluster.</li></ul><p>In traditional practice when an operational engineer was paged by an alert, what probably would have happened:</p><ol><li>Interpreted theÂ message.</li><li>Configured the credential to the target GKE cluster in order to collect information.</li><li>Ran proper commands or tools in order to collect whatever information is necessary.</li><li>Analyzed variety of collected information</li><li>Composed an incident report with a potential solution</li><li>Sent it to a related person in Chat to fix or further investigate.</li></ol><p>The LLM can help with analyzing and reasoning the alert, and work as a centralized control plane to handle various issues. However as a LLM, which doesnâ€™t have access to real time data or any external APIs and services, they are constrained to the information and knowledge that they were trained on. This can lead to frustration for end users who are trying to use the LLM to work with the most up-to-date-information from external systems. <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling">Function calling</a> is a feature of Gemini 1.0 Pro models, which allows developers to do exactly this and helps you connect generative models to real-world data via APIÂ calls.</p><h3>Function call: The bridge to externalÂ world</h3><p>Function calling allows developers to define custom functions that can output structured data from generative models and invoke external APIs. This enables LLMs to access real-time information and interact with various services, such as SQL databases, customer relationship management systems, document repositories, and anything else with anÂ API.</p><p>The following diagram illustrates a sequence of interactions between the user, the application, the model, and the function API. It represents a complete text modality set of interactions or a single conversation turn of a chat modality.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/516/0*_yK5piU17sWjI_9V" /></figure><p>If you want to know more about how function calling works, hereâ€™s a <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#how-works">detailedÂ link</a>.</p><p>Letâ€™s dive into the detail how we are going to implement this idea to leverage the power ofÂ LLMs.</p><h3>Architecture overview</h3><p>This is a high level diagram of how the workflow looks like regarding to our previousÂ vision.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aJZ7D70lyjp8-R6vJa669g.png" /></figure><p>The major sequence is composed of the following steps:</p><ol><li>Altering is triggered by the breach threshold of the metric which could be native metrics or customized one.</li><li>The Cloud Run service will call the model to determine what the proper solution would be as well as mapped functions are going to beÂ invoked.</li><li>Execute each step as per proposed solution.</li><li>The model will summarize the result of executing the solution and aggregated aÂ report.</li><li>The Cloud Run service will push the report toÂ Chat.</li></ol><h3>Build an intelligent alertingÂ handler</h3><p>At the heart of intelligent alerting is the handler interpreting the alerting message and taking necessary actions, such as collecting logs, events, etc. Then put things together for further solution.</p><p>Firstly, we define each individual tool as a Function Declaration in order to help us configure credentials to the GKE cluster and collect information about the pod. Here are the declarations:</p><pre># Function to get GKE credential <br>get_credentail_func = FunctionDeclaration(<br>    name=&quot;get_credential&quot;,<br>    description=&quot;Configure the credential and connect to the GKE cluster.&quot;,<br>    parameters={<br>        &quot;type&quot;: &quot;object&quot;,<br>        &quot;properties&quot;: {<br>            &quot;cluster_name&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The name of the Kubernetes cluster&quot;<br>            },<br>            &quot;region&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The region of the Kubernetes cluster&quot;<br>            },<br>            &quot;project_id&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The project ID of the Kubernetes cluster&quot;<br>            },<br>            &quot;isZonal&quot;: {<br>                &quot;type&quot;: &quot;boolean&quot;,<br>                &quot;description&quot;: &quot;If the cluster is zonal, set this to True, otherwise set this to False&quot;<br>            }<br>        },<br>        &quot;required&quot;: [<br>            &quot;cluster_name&quot;,<br>            &quot;region&quot;,<br>            &quot;project_id&quot;,<br>            &quot;isZonal&quot;<br>        ],<br>        &quot;required&quot;: [<br>            &quot;cluster_name&quot;,<br>            &quot;region&quot;,<br>            &quot;project_id&quot;<br>        ]<br>    }<br>)<br><br># Function to collect information of issued pod and analyse<br>collect_pod_information_fun = FunctionDeclaration(<br>    name=&quot;collect_pod_information&quot;,<br>    description=&quot;&quot;&quot;<br>        Collect pod information from the GKE cluster.<br>    &quot;&quot;&quot;,<br>    parameters={<br>        &quot;type&quot;: &quot;object&quot;,<br>        &quot;properties&quot;: {<br>            &quot;namespace_name&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The namespace where the pod is located, which is a lowercase RFC 1123 label must consist of lower case alphanumeric characters or &#39;-&#39;, and must start and end with an alphanumeric character (e.g. &#39;my-name&#39;,  or &#39;123-abc&#39;, regex used for validation is &#39;[a-z0-9]([-a-z0-9]*[a-z0-9])?&#39;)&quot;<br>            },<br>            &quot;pod_name&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The name of the pod to get by&quot;<br>            },<br>            &quot;kubernetes_context&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The kubernetes context to use, defaults to the current context&quot;<br>            }<br>        },<br>        &quot;required&quot;: [<br>            &quot;kubernetes_context&quot;<br>        ],         <br>    },<br>)</pre><p>Secondly, weâ€™re going to put tools together as a toolset and leverage Function Calling of Gemini to determine which tool should be used in order to achieve theÂ goal.</p><pre># Function calling<br>gke_cluster_tool = Tool (<br> function_declarations= [<br> get_credentail_func, <br> collect_pod_information_fun<br> ]<br>)<br><br>...<br><br># Setup model and temperature<br>model = GenerativeModel(&quot;gemini-1.0-pro-002&quot;,<br>generation_config={&quot;temperature&quot;: 0.5}, tools=[gke_cluster_tool])<br>chat = model.start_chat(response_validation=False)</pre><p>Lastly we prepare a prompt to instruct the model on what to do when an alerting message isÂ hitting.</p><pre>@app.post(&quot;/alerting&quot;)<br>def analyse_alerting(message: Union[str, dict]) -&gt; dict:<br> <br> prompt = &quot;&quot;&quot;<br> You are a Kubernetes expert and highly skilled in all Google Cloud services, Linux, and shell scripts. <br> Your task is to troubleshoot the problematic pod as per CONTEXT with the following steps: <br> <br> 1. Configure the credential and connect to the GKE cluster.<br> 2. Collect the pod information from the GKE cluster.<br> 3. Provide a summary of pod, a concise explanation of the issue and follow by step by step solutions to address the issues. <br> <br> Only use information that provided, do not make up information.<br> CONTEXT:<br> {}<br> &quot;&quot;&quot;.format(message[&quot;incident&quot;][&quot;resource&quot;][&quot;labels&quot;])<br> <br> response = chat.send_message(prompt)<br> ...</pre><p>Finally we see how the magic happens in your Chat when the alerting is triggered. Here is what IÂ had:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9v8MRt5P8zIcd3BH" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GV6rVG6QPAiU1gQ8" /></figure><h3>Summary</h3><p>To deploy the reference architecture that this document describes, see the <a href="https://github.com/cc4i/llm-alerting">Building an intelligent alerting with Gemini &amp; Function calling</a> on Google Cloud GitHub repository.</p><p>The foundational model can help you with reasoning as well as aggregate a proper solution from provided functions. Gemini is a very powerful foundational model you can harness to build more complex functions than what was built in this demo. I believe an end to end automated process can make the operational engineersâ€™ lives so muchÂ easier.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c1981d38fa94" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/building-an-intelligent-alerting-with-gemini-function-calling-c1981d38fa94">Building an intelligent alerting with Gemini &amp; Function Calling</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloud SQL for PostgreSQL Optimization during Migration using Database Migration Service.]]></title>
            <link>https://medium.com/google-cloud/cloudsql-for-postgresql-optimization-during-migration-using-database-migration-service-68ba35ec3040?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/68ba35ec3040</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[cloud-sql]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[google-cloud-partner]]></category>
            <category><![CDATA[database-migration]]></category>
            <dc:creator><![CDATA[Somdyuti]]></dc:creator>
            <pubDate>Fri, 12 Apr 2024 01:28:01 GMT</pubDate>
            <atom:updated>2024-04-12T19:07:08.270Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/430/1*wTbr8sMHT1BlgQDIINWymQ.jpeg" /><figcaption>DMS now performs Parallel Full Load and Parallel CDC from PostgreSQL</figcaption></figure><p><a href="https://cloud.google.com/database-migration">Database Migration Service</a>(DMS) makes it easier for you to migrate your data to Google Cloud. This service helps you lift and shift your MySQL and PostgreSQL workloads into Cloud SQL and AlloyDB for PostgreSQL. In addition, you can lift and modernize your Oracle workloads into Cloud SQL for PostgreSQL or AlloyDB for PostgreSQL.</p><p>In this document we will discuss how you can optimize the DMS Initial Load and CDC when migrating to Cloud SQL for PostgreSQL Instance. The source can be either Oracle or PostgreSQL.</p><h4>The Cloud SQL for PostgreSQL Parameters</h4><p><strong><em>The suggested parameters need to be properly tested and verified against the chosen target Cloud SQL Instance type. They should not be set in your actual production workload.</em></strong></p><p>1. <strong>max_wal_size</strong>= 20GB.</p><p>This will make sure that Database checkpoints happen when 20GB worth of WAL data is generated. If 20 GB worth of WAL data is generated &gt; 5 minutes then checkpoints will happen every 5 minutes as per checkpoint_timeout setting.</p><blockquote>Given that, during DMS Load with the default max_wal_size which is 1.5GB for Cloud SQL PostgreSQL Enterprise and 5GB for Enterprise Plus editions, checkpoints are happening every few seconds which increases the I/O and CPU. Higher value will increase the checkpoint frequency which will reduce the I/O footprint.</blockquote><p>Also monitor the following wait events from Cloud SQL Console <strong>System Insights</strong> <strong>â€œWALWriteâ€</strong>- and the <strong>event_type</strong> will be <strong>â€œLWlockâ€</strong>. Be aware that <strong><em>WALWrite can be both a LWLock and an IO event_type.</em></strong> For frequent checkpoints it will manifest as LWLock event type as the CKPT process will wait for a latch on the WAL Segments to write the Checkpoint_change# (aka Oracleâ€™s redo latches). High commits will manifest WALWrite as an IO wait event_type where the WAL Writer will be busy writing changes from WAL buffers to WALÂ Files.</p><p>There can be waits on <strong>â€œDataFileWriteâ€</strong> and <strong>â€œDataFileFlushâ€</strong> events also during very frequent and aggressive checkpoints.</p><p>2. <strong>commit_delay</strong> = 1000 (start with this and go uptoÂ 50000)</p><p>commit_delay sets the <strong>delay in microseconds between transaction commit and flushing WALs to disk</strong>. Basically it will help improve transaction throughput by performing batch commits during Bulk Inserts as it delays the WAL flush (which by default happens in every transaction commit) by 1000 microseconds after transaction commitsÂ , provided load is high enough to accumulate more transactions in that delay (which will be the case during DMS initialÂ load)</p><blockquote>Monitor the following wait events in <strong>System Insights</strong> <strong>â€œWALSyncâ€Â , â€œWALWriteâ€ </strong>which are IO wait event_types for waits related to high commits and also the <strong>â€˜Transaction countâ€™</strong> metric in System Insights.</blockquote><p>3. <strong>wal_buffers</strong> = 32â€“64 MB in 4 vCPU machines and 64â€“128 MB in 8â€“16 vCPU machines. It can be set to even 256MB for higher vCPUÂ targets.</p><p>Smaller wal_buffers increase commit frequency, so increasing the value will help in initialÂ load.</p><p>Again monitor the wait events as mentioned in (2)Â above.</p><p>4. <strong>Parallelism</strong>- As Postgres <strong>does not support parallel DMLs</strong> Bulk Inserts will notÂ benefit.</p><p>5. <strong>autovacuum</strong>- Turn it toÂ off</p><p>Note after the Initial Load is complete, make sure the autovacuum is turned On after running manualÂ vacuum.</p><p>But run a <strong><em>manual vacuum first before releasing the database for actual production usage</em></strong> and set the following to make manual vacuum fast as it will have a lot of work to do firstÂ time.</p><p><strong>max_parallel_maintenance_workers=4</strong> (set it to number of vCPUs of the Cloud SQL Instance)</p><p><strong>maintenance_work_mem=10GB</strong></p><p>Note that manual vacuum will take memory from maintenance_work_mem.</p><p><strong>Subsequently to make autovacuum faster,</strong>Â set</p><p><strong><em>autovacuum_work_mem to 1GB, otherwise autovacuum workers will consume memory from maintenance_work_mem</em></strong></p><p>From Cloud SQL PostgreSQL database parameter perspective we need to <strong>tune Checkpoints and Commits during DMS Initial Load </strong>(in general for any Bulk Load operations) as they significantly affect IOs and also to an extentÂ CPU.</p><blockquote>6. The below recommendation is very specific when the source is PostgreSQL as DMS now supports Parallel Full Load and Parallel CDC when migrating from PostgreSQL to Cloud SQL PostgreSQL or AlloyDB- <a href="https://cloud.google.com/database-migration/docs/postgres/create-migration-job#specify-source-connection-profile-info">faster PostgreSQL migrations</a></blockquote><p>The following parameter settings will help in more optimized Initial Data Copy and CDC when using PGMS (PostgreSQL Multiple Subscriptions)</p><p>In the source PostgreSQL database</p><p><strong>max_replication_slots</strong>- Set it to at least 20. It must be set to at least the number of subscriptions expected to connect which is max 10 subscriptions when DMS Parallelism is configured to Maximum (4 subscriptions per database), plus some reserve for table synchronization.</p><p><strong>max_wal_senders</strong>- Set it to higher value, preferably 20 and at least same as max_replication_slots.This controls the maximum number of concurrent connections from the target Cloud SQL PostgreSQL. With DMS Parallelism configured to Maximum there can be 4 subscriptions created per database with a max of 10 subscriptions for the PostgreSQL Cluster.</p><p>Assuming the target Instance has enough vCPU and memory available</p><p><strong>max_worker_processes</strong>- Set it to number of vCPUs in theÂ target.</p><p>max_replication_slots- Set it to 20. It must be set to at least the number of subscriptions that will be added to the subscriber which can be upto 10, plus some reserve for table synchronization.</p><blockquote>Even with PGMS(PostgreSQL Multiple Subscriptions), when the subscription is initializedÂ , there can be only one synchronization worker per table.(which means a table cannot be copied in parallel). Tables are copied in parallel across the subscriptions/replication sets.</blockquote><blockquote>max_logical_replication_workers and max_sync_workers_per_subscription will not affect the DMS Parallelism as these parameters influence Native Logical Replication and DMS uses pglogical.</blockquote><blockquote>7. This is very specific when you are migrating from Oracle that has many and large LOB segments. If your target Cloud SQL PostgreSQL or AlloyDB is in Version 14 and above. To make the initial load faster by 3x times, <br>change the default_toast_compression in target Cloud SQL PostgreSQL or AlloyDB toÂ LZ4.</blockquote><p>The CLOBs and BLOBs in Oracle are converted to TEXT and BYTEA respectively in PostgreSQL. If the LOBs are large then it is extremely likely that the size of tuple/row &gt; 2KB and they will be spilled to TOAST segments (store out-of-line) in PostgreSQL (they will be stored in pg_toast schema as pg_toast_&lt;OIDoftable&gt;). TOAST data is compressed/decompressed while being inserted/queried. The default compression technique that PostgreSQL uses is PGLZ which is CPU Intensive and not as performant as LZ4 which is available from PostgreSQL 14 onwards. Using LZ4 the SELECTs speed is close to that of uncompressed data, and the speed of data insertion is upto 80% faster compared to PGLZ. Additionally you will get faster performance duringÂ SELECTs.</p><h4>Target Cloud SQL PostgreSQL Instance Sizing andÂ Storage</h4><p>More resources you give to the target Cloud SQL Instance, the better the performance of DMS willÂ be.</p><p><strong>Network Throughput, Disk Throughput and Disk IOPS</strong>. Network Throughput is limited by the number of vCPUs- we get <strong>250MBps Network throughput per vCPU </strong>and the <strong>Disk Throughput (0.48MBps per GB</strong>) is limited by Network Throughput. For <strong>Disk IOPS</strong> we get <strong>30Â IOPS/GB</strong>.</p><p>So, the <strong><em>correct Instance size, along with the storage size will help you improve the DMS Initial Load performance</em></strong>. In general DMS will need more IOPS and decent Disk throughput and you can configure your disk size in such a way that you utilize as much as Network throughput Bandwidth for Disk Throughput (as most of the network bandwidth consumed will be from Database VM to underlying storage).</p><p>Take for example, for a 4 vCPU Cloud SQL Enterprise Instance you will get 1000 MB/s as Network throughput. So if you allocate a 600GB Disk you will get Disk Throughput close to 300 MB/s and 18000 IOPS. (I am not taking into account your Database size, of course you need to allocate more storage than your databaseÂ size)</p><blockquote>So do not size the initial storage based on the source database size only, take into account the Throughput and IOPS requirement of the workload.</blockquote><p><strong><em>You can always later reduce storage using either a request with Google Support team or via Self Service Storage Shrink which is in Preview mode now. Target Cloud SQL Instance can be downscaled before the application cut-over.</em></strong></p><h4>Few MoreÂ Tips</h4><p>Do not create a Regional Cloud SQL Instance during the Migration time. Enable High Availability, if you need so, after the migration is done and before application cut-over.</p><p>Do not enable Automated Backups during the time of migration.</p><p>DMS does not create Secondary Indexes and Constraints during Initial Load; it creates after the initial load completes and beforeÂ CDC.</p><p>Install<strong> pg_wait_sampling extension</strong> which will be helpful in diagnosing wait events related to PostgreSQL slow performance during Migration and even after production cut-over. Query <strong>pg_stat_bgwriter</strong>, <strong>pg_stat_wal</strong> for information on Checkpoints and Commits which can be used to diagnose further. Enable the log based alerts and log based Metrics related to Frequent checkpoints.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=68ba35ec3040" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/cloudsql-for-postgresql-optimization-during-migration-using-database-migration-service-68ba35ec3040">Cloud SQL for PostgreSQL Optimization during Migration using Database Migration Service.</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>