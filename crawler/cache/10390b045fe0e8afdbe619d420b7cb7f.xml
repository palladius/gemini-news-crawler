<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 14 May 2024 17:51:23 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Optimising Media Delivery with Google‚Äôs Media CDN]]></title>
            <link>https://medium.com/google-cloud/optimising-media-delivery-with-googles-media-cdn-18203a966c96?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/18203a966c96</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[cdnmedia]]></category>
            <category><![CDATA[devops]]></category>
            <category><![CDATA[cdn]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Divya Kurothe]]></dc:creator>
            <pubDate>Tue, 14 May 2024 04:22:21 GMT</pubDate>
            <atom:updated>2024-05-14T04:22:20.949Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*I5ssp6CdPIZ_KCelrWhIPQ.jpeg" /></figure><h4>Introduction</h4><p>Media CDN offers a robust solution for optimising media delivery, specifically tailored for high-throughput egress workloads such as streaming video and large file downloads. Central to Media CDN‚Äôs effectiveness is the concept of edge caching, which strategically places content closer to end users within Google‚Äôs global infrastructure. By doing so, Media CDN significantly reduces latency and relieves pressure on origin servers, enhancing overall performance.</p><p>What sets Media CDN apart is its remarkable adaptability in fetching content from various sources. Whether the content resides in Cloud Storage, another cloud provider, or on-premises infrastructure, Media CDN seamlessly integrates with existing¬†setups.</p><h4>Media CDN Capabilities</h4><p>Media CDN offers a range of powerful features to optimise content delivery:</p><p><strong>SSL (TLS) Certificate Support</strong>: Serve encrypted traffic from your own domain without additional charges.</p><p><strong>Google Cloud Armor Support</strong>: Control access to content using IP address allow-lists/deny-lists and geographic filtering.</p><p><strong>Extensibility</strong>: Customise request-response processing with Service Extensions plugins.</p><p><strong>Origin Authentication</strong>: Securely access private Cloud Storage¬†buckets.</p><p><strong>Advanced Routing Features</strong>: Map traffic to specific edge configurations and¬†origins.</p><p><strong>Client Connectivity Features</strong>: Support modern networking protocols like HTTP/2 and¬†QUIC.</p><p><strong>Cache Invalidation</strong>: Invalidate cached content by host, URL path, URL prefix, cache tags,¬†etc.</p><p><strong>Custom HTTP Header</strong>s: Specify custom headers for various purposes.</p><p><strong>Integration with Cloud Logging</strong>: Log each HTTP request for analysis and monitoring.</p><p><strong>Signed Requests</strong>: Use signed cookies and URLs for content authentication.</p><blockquote>Media CDN is by default not enabled in GCP projects. To inquire about accessing Media CDN, please reach out to your designated Google Cloud sales representative or your account¬†team.</blockquote><p>Here‚Äôs a step-by-step guide on configuring Media CDN origin and service in Google Cloud Platform¬†(GCP):</p><ol><li><strong>Create a Cloud Storage bucket to store your¬†content:</strong></li></ol><p>Create a private GCS and add some objects (images and videos) to¬†it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XaerO17hZLq0LlXsCYBLpg.png" /><figcaption>Private GCS¬†Bucket</figcaption></figure><p><strong>2. Grant Permission:</strong></p><p>Grant the ‚ÄúStorage Object Viewer‚Äù permission to the Media CDN service¬†account.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FZiorFFTW7IIOPwh_KZY6Q.png" /></figure><p><strong>3. Enable the required services:</strong></p><p>To configure and deploy Media CDN services, you need to enable APIs using the following gcloud command:<br><em>$ gcloud services enable </em><a href="https://www.google.com/url?q=http://networkservices.googleapis.com&amp;sa=D&amp;source=buganizer&amp;usg=AOvVaw1kZF4avsJwmQZXBLOaa8gj"><em>networkservices.googleapis.com</em></a><em><br>$ gcloud services enable </em><a href="https://www.google.com/url?q=http://certificatemanager.googleapis.com&amp;sa=D&amp;source=buganizer&amp;usg=AOvVaw28faR00t1oXq-tydCYqWlC"><em>certificatemanager.googleapis.com</em></a><em><br>$ gcloud services enable </em><a href="https://www.google.com/url?q=http://edgecache.googleapis.com&amp;sa=D&amp;source=buganizer&amp;usg=AOvVaw2AHFA9GixBPQXWzejH_pVR"><em>edgecache.googleapis.com</em></a></p><p><strong>4. Create and EdgeCache Origin:</strong></p><ul><li>Create an origin that points to your Cloud Storage bucket by selecting the same for the Origin address. If you have an external origin instead, replace Origin Address with the FQDN or IP¬†address.</li><li>Here, I am using HTTPS protocol with 443 port, you can also use HTTP protocol at 80¬†port.</li><li>Optional: Select a failover origin to try in case this origin becomes unreachable. You can update this field later.<br>For <strong>Max attempts</strong>, select the maximum number of attempts to fill the cache from this¬†origin.</li><li>Optional: Specify the following timeout values:<br>- For <strong>Connect timeout</strong>, select the maximum duration to wait for the origin connection to be established.<br>- For <strong>Response timeout</strong>, select the maximum duration to allow for a response to complete.<br>- For <strong>Read timeout</strong>, select the maximum duration to wait between reads of a single HTTP connection or¬†stream.</li></ul><p>You can either create it using console or using the following gcloud command:<br>gcloud edge-cache origins create ORIGIN --origin-address=&quot;ADDRESS&quot;<br>where:<br>ORIGIN: the name of the new origin<br>ADDRESS: the bucket name, gs://my-bucket<br>If you have an external origin instead, replace ADDRESS with the FQDN or IP¬†address.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YnuropQiUpHbUaruHNSy8w.png" /><figcaption>Media CDN¬†Origin</figcaption></figure><p>This is the same whether the bucket is multi-regional, dual-region, or regional.</p><p><strong>5. Creating a Media CDN¬†Service:</strong></p><p>In the Google Cloud console, go to the Media CDN page and create the service in the Service¬†Tab.</p><ul><li>Enter a unique name for your service‚Ää‚Äî‚Ääfor example, my-service.</li><li>In the Routing section, Add host rule, and then enter one or more host domain names. <br>- For Hosts, enter a host domain name‚Ää‚Äî‚Ääfor example, web.example.com.<br>- Add route rule and for Priority, specify 1.<br>- Add match condition, for Path match, specify¬†/</li><li>Select Fetch from an Origin, and then select the origin that you configured.</li><li>In Add-on actions. Add Response header.<br>- For Header Key, specify x-cache-status, and for Value, specify {cdn_cache_status}.</li><li>For Route action, Add an item: <br>- For Type, select CDN policy.<br>- For Cache mode, select FORCE_CACHE_ALL.</li><li>Save and create¬†service.</li></ul><p><strong>6. Create a DNS authorization</strong></p><p>First, you must create a DNS authorization to demonstrate ownership of the domain before you can issue certificates for it using the following command:</p><p><em>gcloud certificate-manager dns-authorizations create &lt;Domain-name-auth&gt;‚Ää‚Äî‚Äädomain= ‚Äú&lt;Domain-name&gt;‚Äù</em></p><p><em>gcloud certificate-manager dns-authorizations describe</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Tw4sstnBYzPxCtTStwaR_Q.png" /></figure><p><strong>Now in your DNS Provider, add the name of dnsResourceRecord (for example: _acme-challenge.example.com.‚Äô) as a CNAME record and add the IP address of the service as A¬†record.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*a-5NxKOQFJqqKFpJFIpT7w.png" /></figure><p><strong>7. Create a certificate</strong></p><p>After you have added the DNS record demonstrating ownership of the domain(s) you want to create a certificate for, you can issue a certificate creation request using the¬†command:</p><p><em>gcloud certificate-manager certificates create &lt;certificate-name&gt;‚Ää‚Äî‚Äädomains=‚Äù&lt;domain-name&gt;‚Äù‚Ää‚Äî‚Äädns-authorizations=‚Äù&lt;Domain-name-auth&gt;‚Äù‚Ää‚Äî‚Ääscope=EDGE_CACHE</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ULWUvlxqBFOCo9g7dixmTg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*H6lgtcIhpW1EXCfjcez_3w.png" /></figure><p>Now edit the Media CDN service and add the created certificate</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fWc3csGDEjgu3qMcqmJQjQ.png" /></figure><p>Now open the domain in the browser along with the object path as follows:<br><strong><em>https://&lt;domain_name&gt;/&lt;path_of_the_object&gt;</em></strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*x6noCCVS-7qixdaBEa-S9w.png" /></figure><p>Media CDN also utilises Google Cloud Armor for regulating content access, enabling users to grant or restrict permissions. Media CDN integrates with <strong>Google Cloud Armor‚Äôs Edge Security Policies</strong> to facilitates IP address allowlists and denylists, geographical filtering controls and operating on country and region¬†codes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CTF90lkx2XEFF1Ihy3mhDQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*K_eLsu11lZzFd9-MaWXr0g.png" /></figure><p>Media CDN effortlessly integrates with Edge Security Policy, offering IP and geolocation controls. Moreover, it provides the flexibility to incorporate custom code into the request-response processing path via Service Extensions plugins (Preview). This customisation unlocks a plethora of lightweight use cases, including header normalisation and custom tokenization.</p><p><strong>Thank you for reading¬†:)</strong></p><p>Questions?<br>If you have any questions, I‚Äôll be happy to read them in the comments.<br>You can also follow me on LinkedIn: <a href="https://www.linkedin.com/in/divya-kurothe/">Divya¬†Kurothe</a></p><p>Reference:<br><a href="https://cloud.google.com/blog/products/networking/introducing-media-cdn">https://cloud.google.com/blog/products/networking/introducing-media-cdn</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=18203a966c96" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/optimising-media-delivery-with-googles-media-cdn-18203a966c96">Optimising Media Delivery with Google‚Äôs Media CDN</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Connect to Non-PSC AlloyDB or Non-PSC Cloud SQL from a different VPC]]></title>
            <link>https://medium.com/google-cloud/connect-to-non-psc-alloydb-or-non-psc-cloud-sql-from-a-different-vpc-3f8eeed51d2a?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/3f8eeed51d2a</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[alloydb]]></category>
            <category><![CDATA[cloud-sql]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Harinderjit Singh]]></dc:creator>
            <pubDate>Tue, 14 May 2024 04:21:54 GMT</pubDate>
            <atom:updated>2024-05-14T14:10:54.619Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>When it comes to managed RDBMS solutions, Google Cloud Platform (GCP) offers two powerful solutions: Cloud SQL for managed relational databases and AlloyDB for PostgreSQL compatibility with high performance. Understanding how to connect to these databases is essential for any developer or administrator working with¬†GCP.</p><blockquote>Note¬†: For brevity, we‚Äôll focus on AlloyDB, as networking concepts apply similarly to CloudSQL.</blockquote><p>This blog dives into the world of private connectivity for AlloyDB, focusing on the benefits of Private Service Connect (PSC). Before we delve into the specific steps of creating a PSC endpoint for non-PSC enabled instances, let‚Äôs explore the scenarios where utilizing PSC with AlloyDB proves advantageous. Understanding these use cases will equip you with the foundational knowledge necessary to grasp the methods employed in specific situations.</p><p>Firstly, we‚Äôll differentiate between PSA (Private Service Access) and PSC, highlighting the evolution of private connectivity options for AlloyDB. This clarifies why PSC is the preferred approach for modern deployments.</p><h3><strong>Networking Essentials for Connecting Applications to¬†AlloyDB</strong></h3><p>Connecting your applications to AlloyDB involves careful consideration of your network architecture. Let‚Äôs break down the key scenarios and how you can establish secure and reliable connections. We will only discuss methods to access the database using Private¬†IP.</p><h4><strong>Scenario 1: Application and AlloyDB in the Same Customer VPC¬†Network</strong></h4><p>This is the most straightforward setup. You can <a href="https://cloud.google.com/alloydb/docs/configure-connectivity#gcloud">use PSA to connect to your AlloyDB Instance</a>. <strong>Private services access</strong> is implemented as a <a href="https://cloud.google.com/vpc/docs/vpc-peering">VPC peering connection</a> between your <a href="https://cloud.google.com/vpc/docs/vpc">VPC network</a> and the <em>underlying Google Cloud VPC network</em> where your AlloyDB instance resides. Any service in Customer VPC network can use the Private IP of AlloyDB to connect to¬†it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/415/1*UwDvQ2oqjwyDWzPsJZFrpw.png" /><figcaption>Fig1: Simplified depiction of simplest Architecture</figcaption></figure><h4><strong>Scenario 2: Application and Cloud SQL in a Shared VPC¬†Network</strong></h4><p><a href="https://cloud.google.com/vpc/docs/shared-vpc">Shared VPC networks</a> are designed to enable resource sharing across projects within an organization. This is the most commonly used network architecture in larger, multi-project environments.</p><p>This simplifies the network architecture. Similar to the same VPC scenario, PSA is configured and services within Customer Shared VPC network can use the Private IP address, language connectors or AlloyDB Auth¬†Proxy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/635/1*F6StsaFJAqoHXiGNAH9VJw.png" /><figcaption>Fig2: Shared¬†VPC</figcaption></figure><p>From above Illustration, We can see that GKE which is a separate Customer project (AlloyDB is in separate customer project) is able to access the AlloyDB using Private IP (PSA) as we are using shared VPC network and Shared VPC network is Peered to Google Managed AlloyDB VPC¬†network.</p><h4><strong>Scenario 3: Application and Cloud SQL in Different VPC Networks with VPC¬†Peering</strong></h4><p>This is when you want to manage separate VPC networks for separate projects or may be multiple Shared VPC networks.</p><p>For example consider the diagram¬†below</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/625/1*vCXHqR5GmzlItj9_uZ5tHw.png" /><figcaption>Fig 3 Separate customer VPC networks for Application and¬†AlloyDB</figcaption></figure><p><strong>How would GKE in Customer GKE VPC connect to AlloyDB in AlloyDB VPC (PSA configured)¬†?</strong></p><p>The first solution that comes to mind to VPC Network¬†peering.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/625/1*3vTqq8Mh8P4Tb1e_h3TdDA.png" /><figcaption>Fig 4 Transitive VPC Network¬†Peering</figcaption></figure><p>Even If we peer Customer GKE VPC network to Customer AlloyDB VPC which is already peered to Google AlloyDB VPC through PSA, <strong>GKE pods using Customer GKE VPC network won‚Äôt be able to connect to AlloyDB Private IP because VPC Network Peering isn‚Äôt transitive</strong>.</p><p>In Such a setup (Fig4. ), you can use the following ways to connect your AlloyDB instance to multiple VPCs using private IP¬†:</p><ul><li><a href="https://cloud.google.com/sql/docs/mysql/connect-multiple-vpcs#custom-route">Connect using custom route advertisements</a></li><li><a href="https://cloud.google.com/sql/docs/mysql/connect-multiple-vpcs#intermediate-proxy">Connect using an intermediate proxy (SOCKS5)</a> or Connect using a TCP proxy such as <a href="https://manpages.ubuntu.com/manpages/trusty/man1/simpleproxy.1.html">simpleproxy</a></li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/665/1*uz0NBlOxXS3LTy1EIbtgXQ.png" /><figcaption>Fig 5 TCP proxy on a VM hosted in Customer AlloyDB¬†VPC</figcaption></figure><blockquote>You may use AlloyDB Auth proxy instead of TCP proxy, but that will encrypt the data in transit only from Customer AlloyDB VPC network to Google AlloyDB VPC¬†network.</blockquote><p>While setting up a proxy for PSC with AlloyDB is straightforward, managing a dedicated GCE VM within the customer‚Äôs AlloyDB project (VPC network) introduces additional operational overhead.</p><blockquote>You can‚Äôt use TCP Proxy if you intend to use AlloyDB Auth Proxy or language connectors for encryption and IAM authentication. Because Auth proxies work only in VPC networks where the DB Instance IP is directly routable.</blockquote><h4>Scenario 4: Application and Cloud SQL in Different VPC Networks without VPC¬†Peering</h4><p>This scenario is the main motivation for this post. PSC solves this challenge for us. PSC provides another connectivity option for AlloyDB users, with improvements on the legacy Service Networking (or PSA) framework, such¬†as:</p><ul><li>Ability to make direct connections from multiple projects easily into AlloyDB resources, enabling new architectures.</li><li>Efficient use of IP address space, as a single IP address is required from a consumer VPC to connect to an AlloyDB instance. PSA requires a minimum of a /24 IP¬†range.</li><li>More secure as consumer and producer VPCs are isolated, and only inbound connectivity to AlloyDB is allowed. PSA requires bidirectional connectivity between the consumer VPC and AlloyDB by default, which is a blocker for some customer use¬†cases.</li></ul><p><strong>Private Service Connect (PSC) for¬†AlloyDB</strong></p><p><a href="https://cloud.google.com/vpc/docs/private-service-connect">Private Service Connect</a> provides a powerful way to consume Google-managed services privately, even if they reside in a different project or network. PSC creates an internal DNS alias for your Cloud SQL instance. Your application can access it using this alias, and traffic is routed securely over Google‚Äôs private¬†network.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/685/1*x6NJv3NHKO2mf97BZ-oeFA.png" /><figcaption>Fig6 Connect to AlloyDB using Private service¬†connect</figcaption></figure><p>In Fig6, You can see resources from Customer GKE VPC network and Customer AlloyDB VPC network can connect to AlloyDB using a Private service connect endpoints (forwarding rules) in respective VPC networks. These forwarding rules use the service attachment which is created as part of Private service connect. We can whitelist projects where our applications or clients will reside, in the AlloyDB service attachment.</p><p>This doesn‚Äôt requires Private service Access setup in Customer AlloyDB Project. AlloyDB Instance can be created with either PSC or PSA¬†enabled.</p><p>As of May 2024, both methods of private connectivity can‚Äôt be configured simultaneously. Same is true for CloudSQL as well. You can‚Äôt switch between PSC and PSA as of May¬†2024.</p><blockquote>You can also use <a href="https://cloud.google.com/alloydb/docs/configure-private-service-connect">Private service connect for AlloyDB</a> when Customer VPC networks don‚Äôt have PSA configured i.e. for all the above scenarios.</blockquote><p>You must create this endpoint in each customer VPC network where database access is¬†needed.</p><p>Private Service Connect endpoints that are used to access services are regional resources. However, you can make an endpoint available in other regions by <a href="https://cloud.google.com/vpc/docs/manage-endpoints-published-services#enable-global-access">configuring global¬†access</a>.</p><p><strong>Can you enable PSC for existing AlloyDB Instance?</strong></p><p>So if you have an existing AlloyDB Instance with PSA enabled and you have an application in a separate VPC which is not peered to Customer AlloyDB VPC network. And you want to connect to AlloyDB using private IP, what options you do¬†have?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/625/1*vCXHqR5GmzlItj9_uZ5tHw.png" /><figcaption>Fig 7 (same as¬†Fig3)</figcaption></figure><p>You may do what we did in Scenario 3 i.e. create VPC peering between Customer GKE VPC and Customer AlloyDB VPC and use TCP Proxy or use custom route advertisements. But there is an overhead of managing routes or GCE VM running¬†proxy.</p><p>A solution like PSC enabled AlloyDB would be a good fit for such a scenario. As you can‚Äôt simply switch between PSA and PSC, you have below¬†options:</p><ul><li>You can create a DMS job to migrate data between PSA enabled AlloyDB to PSC enabled AlloyDB. That means some extra work in setting up DMS and a small downtime as¬†well.</li><li>You can export all you data and import into a new PSC enabled Instance and that means downtime.</li><li><strong>You can create a PSC endpoint for a Non-PSC (PSA enabled) AlloyDB Instance. That will require some work but you will be able to use both PSA and PSC endpoints to connect the you AlloyDB Instance.</strong></li></ul><h3>Create PSC endpoint for a Non-PSC (PSA enabled) AlloyDB¬†Instance</h3><p>In this section, we will discuss what are the ways we can use to<strong> create a PSC endpoint for a PSA enabled AlloyDB Instance</strong>.</p><h4>Two Methods</h4><ul><li><strong>Method 1</strong>: Create a service attachment in Customer AlloyDB VPC network which already has PSA enabled. This service attachment will use a producer forwarding rule in Customer AlloyDB VPC network that has a ‚Äútarget instance‚Äù as backend and that target instance points to the VM where we have a TCP proxy or SOCKS Proxy running (Fig 8.). Create a forwarding rule in Customer GKE VPC network with service attachment in Customer AlloyDB VPC network as target. Applications deployed on GCE/GKE in Customer GKE VPC network can connect to AlloyDB Instance using the private IP assigned to outgoing fowarding rule.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/625/1*kgYwDyuLFp8jyBz8ik4L6Q.png" /><figcaption>Fig 8 Service attachment with tcp proxy running on a¬†VM</figcaption></figure><ul><li><strong>Method 2</strong>: Create a service attachment in Customer AlloyDB VPC network which already has PSA enabled. This service attachment will use a producer forwarding rule in Customer AlloyDB VPC network which has a backend service with Zonal hybrid network endpoint groups as targets. Zonal NEG has AlloyDB Private IP and port as endpoint. Create a forwarding rule in Customer GKE VPC network with service attachment in Customer AlloyDB VPC network as target. Applications deployed on GCE/GKE in Customer GKE VPC network can connect to AlloyDB Instance using the private IP assigned to outgoing fowarding rule.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/625/1*pFwi78U_e74ckZSczhOMdg.png" /><figcaption>Fig 9 Service attachment connected to LB with zonal NEG as¬†backend</figcaption></figure><p><strong>Method 2</strong> is a better option¬†because</p><ul><li>It uses all the managed services rather than a Proxy running on¬†VM.</li><li>You can use AlloyDB Auth proxy for encryption and IAM authentication</li></ul><h4>Assumptions before Implementation</h4><ul><li>You have a AlloyDB Cluster with primary Instance in Customer AlloyDB project which has PSA¬†enabled</li><li>You have a customer AlloyDB project with customer AlloyDB VPC network and a Customer GKE/GCE project with Customer GKE/GCE VPC¬†network</li><li>Your account has appropriate privileges to create and managed resources</li></ul><h4>Steps to implement Method¬†2</h4><ul><li>Read for environment variables</li></ul><pre><br>read -p &quot;region : &quot; REGION<br>read -p &quot;projectid : &quot; DB_PROJECT<br>read -p &quot;GCE_SUBNET : &quot; GCE_SUBNET<br>read -p &quot;DB_VPC_NET : &quot; DB_VPC_NET<br>read -p &quot;CIDR_TCPNAT : &quot; CIDR_TCPNAT<br>read -p &quot;clientprojectid : &quot; CLIENT_PROJECT<br>read -p &quot;AlloyDB Cluster : &quot; ADB_CLUSTER<br>read -p &quot;AlloyDB Instance : &quot; ADB_INSTANCE<br>read -p &quot;CLIENT_VPC_NET : &quot; CLIENT_VPC_NET<br>read -p &quot;GCE_SUBNET_CLIENT : &quot; GCE_SUBNET_CLIENT<br>read -p &quot;Resrverip : &quot; ADDR<br>read -p &quot;PORT :&quot; PORT</pre><p><em>DB_VPC_NET‚Ää‚Äî‚ÄäCustomer AlloyDB VPC Network name <br>DB_PROJECT‚Ää‚Äî‚ÄäCustomer AlloyDB VPC Project ID<br>GCE_SUBNET‚Ää‚Äî‚ÄäSubnet in Customer AlloyDB VPC Network<br>CIDR_TCPNAT‚Ää‚Äî‚ÄäCIDR for PSC subnet in Customer AlloyDB VPC Network<br>CLIENT_PROJECT‚Ää‚Äî‚ÄäCustomer GKE/GCE VPC Project ID<br>ADB_CLUSTER‚Ää‚Äî‚ÄäAlloyDB Cluster name<br>ADB_INSTANCE‚Ää‚Äî‚ÄäAlloyDB Instance<br>REGION‚Ää‚Äî‚ÄäRegion in which your AlloyDB Instance is created<br>CLIENT_VPC_NET‚Ää‚Äî‚ÄäCustomer GKE/GCE VPC Network name <br>GCE_SUBNET_CLIENT‚Ää‚Äî‚ÄäSubnet in Customer AlloyDB VPC Network <br>ADDR‚Ää‚Äî‚ÄäIP address to be used by outgoing Forwarding rule<br>PORT‚Ää‚Äî‚ÄäPORT on which DB or Auth proxy is listening</em></p><ul><li>Authenticate and create a subnet for Private service connect in Customer AlloyDB VPC¬†Network</li></ul><pre><br># Authenticate <br>gcloud auth login<br><br># Create a TCP NAT subnet.<br>gcloud compute networks subnets create dms-psc-nat-${REGION}-tcp \<br>--network=${DB_VPC_NET} \<br>--project=${DB_PROJECT} \<br>--region=${REGION} \<br>--range=${CIDR_TCPNAT} \<br>--purpose=private-service-connect</pre><ul><li>Create a Zonal Network endpoint group of Hybrid type and add an endpoint which is private IP of the AlloyDB Instance. Port can be either be AlloyDB port (5432) or AlloyDB Auth proxy port(5433).</li><li>For Cloud SQL ports would be different. Cloud SQL Auth Proxy uses¬†3307.</li></ul><pre>### create NEG<br>gcloud compute network-endpoint-groups create neg-$(date +%d%m%Y) --default-port=$PORT --network=${DB_VPC_NET} \<br>--network-endpoint-type=non-gcp-private-ip-port \<br>--project=${DB_PROJECT} \<br>--zone=${REGION}-a <br><br>### get private IP for AlloyDB or cloudSQL<br>DB_PRIVATE_IP=$(gcloud beta alloydb instances describe $ADB_INSTANCE --cluster=$ADB_CLUSTER --region=$REGION  --format json  --project=${DB_PROJECT} | jq .ipAddress|tr -d &#39;&quot;&#39;)<br><br>#neg endpoint<br>gcloud compute network-endpoint-groups update neg-$(date +%d%m%Y) \<br>    --zone=${REGION}-a  \<br>--add-endpoint=&quot;ip=&quot;${DB_PRIVATE_IP}&quot;,port=&quot;$PORT --project=${DB_PROJECT}</pre><ul><li>Configure the load balancer i.e. a backend service with Hybrid NEG as backend, TCP proxy, and regional health check for the backends.</li></ul><pre><br>### Health check probes for hybrid NEG backends originate from Envoy proxies in the proxy-only subnet.<br>gcloud compute health-checks create tcp lb-hc-$(date +%d%m%Y) \<br>    --region=${REGION} \<br>    --use-serving-port --project=${DB_PROJECT}<br><br>##### Create a backend service.<br><br>gcloud compute backend-services create bs-lb-$(date +%d%m%Y) \<br>   --load-balancing-scheme=INTERNAL_MANAGED \<br>   --protocol=TCP \<br>   --region=${REGION} \<br>   --health-checks=lb-hc-$(date +%d%m%Y) \<br>   --health-checks-region=${REGION} --project=${DB_PROJECT}<br>   <br>   <br>## Add the hybrid NEG backend to the backend service.<br><br>gcloud compute backend-services add-backend bs-lb-$(date +%d%m%Y) \<br>   --network-endpoint-group=neg-$(date +%d%m%Y) \<br>   --network-endpoint-group-zone=${REGION}-a \<br>   --region=${REGION} \<br>   --balancing-mode=CONNECTION \<br>   --max-connections=100 --project=${DB_PROJECT}<br>   <br>### For MAX_CONNECTIONS, enter the maximum concurrent connections <br>### that the backend should handle.<br><br>###Create the target TCP proxy.<br><br>gcloud compute target-tcp-proxies create tcp-proxy-$(date +%d%m%Y) \<br>   --backend-service=bs-lb-$(date +%d%m%Y) \<br>   --region=${REGION} \<br>   --project=${DB_PROJECT}<br><br></pre><ul><li>Create the forwarding rule which has target tcp proxy which we created in previous step. The forwarding rule only forwards packets with a matching destination port.</li></ul><pre>## create incoming forwarding rule which acts a frontend for LB<br>gcloud compute forwarding-rules create fr-psc-$(date +%d%m%Y) \<br>   --load-balancing-scheme=INTERNAL_MANAGED \<br>   --network=${DB_VPC_NET} \<br>   --subnet=${GCE_SUBNET} \<br>   --ports=$PORT \<br>   --region=${REGION} \<br>   --target-tcp-proxy=tcp-proxy-$(date +%d%m%Y) \<br>   --target-tcp-proxy-region=${REGION}  --project=${DB_PROJECT}</pre><ul><li>Create Service attachment in Customer AlloyDB VPC network which points to the forwarding rule we created in previous step. We whitelisted Customer AlloyDB VPC network and Customer GCE/GKE VPC network for the service attachment.</li></ul><pre># Create a service attachment.<br>gcloud compute service-attachments create dms-psc-svc-att-${REGION} \<br>--project=${DB_PROJECT} \<br>--region=${REGION} \<br>--producer-forwarding-rule=fr-psc-$(date +%d%m%Y) \<br>--connection-preference=ACCEPT_MANUAL \<br>--nat-subnets=dms-psc-nat-${REGION}-tcp \<br>--consumer-accept-list=${DB_PROJECT}=2000,${CLIENT_PROJECT}=2000</pre><ul><li>Create Firewall rule to allow ingress from service attachment subnet</li></ul><pre>gcloud compute \<br>--project=${DB_PROJECT} firewall-rules create fwr-dms-allow-psc-tcp \<br>--direction=INGRESS \<br>--priority=1000 \<br>--network=${DB_VPC_NET} \<br>--action=ALLOW \<br>--rules=all \<br>--source-ranges=${CIDR_TCPNAT}  \<br>--enable-logging</pre><ul><li>Reserve an internal IP from GCE_SUBNET_CLIENT subnet. This will be used by forwarding rule. This IP will be used by client in Customer GCE/GKE VPC network to connect to¬†AlloyDB.</li></ul><blockquote>If you want to use AlloyDB Auth proxy or language connector then this IP has to be same as PSA Private IP. That means you need to have a Subnet in Customer GCE VPC network with CIDR overlapping with part of your Private Service Access Allocated IP range in your Customer AlloyDB VPC¬†network.</blockquote><pre>### Reserve a Private IP address<br>gcloud compute addresses create addr-$(date +%d%m%Y) \<br>--project=${CLIENT_PROJECT} \<br>--region=${REGION} \<br>--subnet=${GCE_SUBNET_CLIENT} \<br>--addresses=${ADDR}</pre><ul><li>Create a forwarding rule in Customer GCE/GKE VPC network with target as service attachment in Customer AlloyDB VPC¬†network</li></ul><pre>## create PSC endpoint (forwarding rule)<br>gcloud compute forwarding-rules create fr-client-$(date +%d%m%Y) \<br>--address=addr-$(date +%d%m%Y) \<br>--project=${CLIENT_PROJECT} \<br>--region=${REGION} \<br>--network=${CLIENT_VPC_NET} \<br>--target-service-attachment=projects/${DB_PROJECT}/regions/${REGION}/serviceAttachments/dms-psc-svc-att-${REGION}</pre><ul><li>To test connectivity, Create a GCE VM in Customer GCE/GKE project using GCE_SUBNET_CLIENT of Customer GCE/GKE VPC network. Install postgresql-client</li></ul><pre>## create a Client VM <br>gcloud compute instances create instance-$(date +%d%m%Y)  \<br>    --project=${CLIENT_PROJECT} \<br>    --zone=${REGION}-a \<br>    --image-family=debian-12 \<br>    --image-project=debian-cloud \<br>    --network-interface=network-tier=PREMIUM,stack-type=IPV4_ONLY,subnet=${GCE_SUBNET_CLIENT} \<br> --metadata=startup-script=&#39;#! /bin/bash<br>apt-get install postgresql-client wget -y<br>wget https://storage.googleapis.com/alloydb-auth-proxy/v1.7.1/alloydb-auth-proxy.linux.amd64 -O alloydb-auth-proxy<br>chmod +x alloydb-auth-proxy<br>&#39;</pre><ul><li>On the GCE VM in Customer GKE/GCE VPC¬†network</li></ul><pre>### If Using AlloyDB Auth Proxy<br>gcloud beta alloydb instances describe alloydb-ins-primary-$(date +%d%m%Y) --project=${DB_PROJECT} --cluster=alloydb-cls-001  --region=${REGION} --format json | jq .name<br><br>./alloydb-auth-proxy INST_URI </pre><ul><li>If you are configuring for Cloud SQL, <a href="https://cloud.google.com/sql/docs/postgres/connect-auth-proxy#start-proxy">use this public document to start Cloud SQL Auth¬†Proxy</a>.</li><li>On the GCE VM in Customer GKE/GCE VPC¬†network</li></ul><pre>### If using AlloyDB Auth Proxy<br>psql -h 127.0.0.1 -U postgres postgres<br><br>### If using Private IP<br>psql -h &lt;Private IP of forwarding rule in Customer GCE/GKE VPC network&gt; -U postgres postgres</pre><p>This method works well. I have tested this multiple¬†times.</p><p>Performance testing is advised before production deployment.</p><h3>Other notable key¬†points</h3><ul><li>Prefer managed PSC enabled AlloyDB over such a manual¬†solution</li><li>NEG used as backed service‚Äôs backend are hybrid zonal NEG, so in case of a zone failure, you have to add new zonal NEG to backend service of load balancer.</li><li>You can use DMS service to migrate a PSA enabled AlloyDB to PSC¬†AlloyDB.</li></ul><h3>References</h3><ul><li><a href="https://cloud.google.com/alloydb/docs/connect-external">Connect to a cluster from outside its VPC | AlloyDB for PostgreSQL | Google Cloud</a></li><li><a href="https://cloud.google.com/sql/docs/mysql/connect-multiple-vpcs">Connect your instance to multiple VPCs | Cloud SQL for MySQL | Google Cloud</a></li><li><a href="https://cloud.google.com/sql/docs/mysql/configure-private-service-connect">Connect to an instance using Private Service Connect | Cloud SQL for MySQL | Google Cloud</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3f8eeed51d2a" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/connect-to-non-psc-alloydb-or-non-psc-cloud-sql-from-a-different-vpc-3f8eeed51d2a">Connect to Non-PSC AlloyDB or Non-PSC Cloud SQL from a different VPC</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optimize Your Athletic Performance with Gemini and Vertex AI]]></title>
            <link>https://medium.com/google-cloud/optimize-your-athletic-performance-with-gemini-and-vertex-ai-eae42d014659?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/eae42d014659</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[sports-analytics]]></category>
            <category><![CDATA[generative-ai]]></category>
            <dc:creator><![CDATA[Alok Pattani]]></dc:creator>
            <pubDate>Mon, 13 May 2024 14:34:51 GMT</pubDate>
            <atom:updated>2024-05-13T14:34:20.191Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>Co-Author:</strong> <a href="https://medium.com/@zackakil">Zack¬†Akil</a></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F8xO0-b_XmJA%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D8xO0-b_XmJA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F8xO0-b_XmJA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/49b30e151d1e06040298c0547df7c65b/href">https://medium.com/media/49b30e151d1e06040298c0547df7c65b/href</a></iframe><p>Many of us like to participate in sports, and if you‚Äôre competitive at all, you likely want to get better in some way: run faster, be more accurate, have more power, vanquish your opponents‚Ää‚Äî‚Ääand maybe even look good while doing¬†it.</p><p>One challenge in improving is that some aspects of our performance are hard to measure, and getting access to high quality data and personalized coaching can be difficult‚Ää‚Äî‚Ääwe‚Äôre not all on track to be professional athletes, after all. But what if we can get some of that insightful data and analysis with the help of¬†AI?</p><p>Enter one of Google Cloud‚Äôs most popular in-person demos that‚Äôs gone from London to Berlin to Las Vegas, and is coming to <a href="https://io.google/2024/">Google I/O</a> this week: AI Penalty Challenge! It‚Äôs a unique end-to-end interactive soccer coaching experience that uses the latest in Generative AI on Google Cloud, combined with other Cloud and Android technologies. The demo has stolen the show at various events over the last several months and continues to inspire participants to build amazing things with¬†Google.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Xq7COOKJBg3yDWir" /></figure><p><em>(Sidenote: For those outside the US, please feel free to read the word ‚Äúsoccer‚Äù as ‚Äúfootball‚Äù and forgive us for the ‚Äúmisspelling‚Äù throughout this article¬†</em>üòÜ<em>.)</em></p><p>The AI Penalty Challenge setup is pretty straightforward, as shown in the video above and explained in <a href="https://youtu.be/Z1wCyhmgMyU?si=rLOVAPUs_aPPMEyB">this one</a>: each participant takes three penalty kicks at a soccer goal, aiming for one of the yellow boxes in the top¬†corners.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*sDwreEMn-oFJtFf1" /></figure><p>The magical part is that each kick is evaluated on power, accuracy, and style within seconds, and afterwards, participants receive personalized feedback from an AI soccer coach and a unique player card as takeaways.</p><p>Let‚Äôs dive into the different aspects of this experience in more detail to understand how it¬†works.</p><h3>Recording the Penalty¬†Kick</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3K7W0ZxL5f_FBeeH" /></figure><p>The first step is to actually record the kicks, which is done using the cameras on six Pixel phones placed in different spots around pitch: one focusing on the goal, one focused on the ball path, one from each side focused on the kicker, and one in each top corner¬†box.</p><p>The camera timing is synchronized using a ‚Äúreferee‚Äù tablet that has an app that starts and stops recording between each session. The raw videos are uploaded to <a href="https://cloud.google.com/storage">Cloud Storage</a> almost immediately after each kick, and are used in replays on screens around the¬†pitch.</p><h3>Triggering Video Analysis in the¬†Cloud</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*K7sHWz-ct-AeL6_P" /></figure><p>When a video hits the specified Cloud Storage bucket, <a href="https://cloud.google.com/firestore/docs/eventarc">a Firestore trigger for Eventarc</a> routes the video through a <a href="https://cloud.google.com/functions">Cloud Function</a> that triggers Python scripts that run the analysis in a serverless way. Results are then sent to <a href="https://firebase.google.com/products/firestore">Cloud Firestore</a> (stats) and Cloud Storage (images).</p><h3>Measuring Speed and¬†Accuracy</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*d_TFuwIFP1Vw-ict" /></figure><p>Diving more into the actual analysis, custom object detection models trained with <a href="https://cloud.google.com/vision-ai/docs">Vertex AI Vision</a> are used to track the ball and the goal. Task-specific models shine in cases like these where there is enough labeled training data (images with the types of soccer balls and goals used in the demo) and the desired results are precise position estimates of objects in¬†frame.</p><p>Tracking the ball across frames enables the speed calculation that makes up the power score, and how close the ball is to the closest bin determines the accuracy score. All of this logic is called within a Cloud Function, with the calculated scores going to Cloud Firestore and some visualizations of the kick going to Cloud¬†Storage.</p><h3>Analyzing Style</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BD79t0uO5IWPnsy4" /></figure><p>The most subjective of the three attributes is style. Leveraging <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini">Gemini in Vertex AI</a>‚Äôs multimodality and reasoning ability, a prompt asks the model to look at frames of the kick, analyze how good or creative the kicker‚Äôs technique is, and return a style score with the reasoning behind it. With no additional soccer-specific training data (unlike what might be required for a more traditional ML model), Gemini still returns insightful results and explanations about¬†style.</p><p>The style score and reasoning are sent to Cloud Firestore, where the three scores can be averaged together to get a total score for each¬†kick.</p><h3>Coaching Feedback from a¬†Pro</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*N9_BWTt4zBLmIWwg" /></figure><p>Once all three kicks for a given participant have scores, another Firestore trigger for Eventarc initiates another Cloud Function, this time prompting <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview#1.5-pro-use-cases">Gemini 1.5 Pro</a> (with extended reasoning ability) with the player‚Äôs scores and specific instructions to generate some encouraging constructive feedback in the voice of a soccer coach. The coaching feedback is converted to audio using the <a href="https://cloud.google.com/text-to-speech">Text-to-Speech API</a>, and, with the help of <a href="https://cloud.google.com/text-to-speech/custom-voice/docs">Custom Voice</a>, comes out sounding like it came from a professional soccer¬†coach!</p><p>Below are the images and the coaching feedback I received for the three kicks I took at <a href="https://cloud.withgoogle.com/next">Google Cloud Next </a>‚Äô24 in Las¬†Vegas.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cTVOkGQXEm62uRWC" /></figure><p>He‚Äôs right: I should focus on driving through the ball more to get more¬†power!</p><h3>Generating a Player¬†Card</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kR6v_RKV47XmuJFt" /></figure><p>In addition to the practical coaching feedback, participants get another cool takeaway from this experience: a player card featuring their image and stats from their best kick, with a cool AI-generated background. Participants pick their background theme from a variety of types‚Ää‚Äî‚Ääforest, cosmic, heroic, etc.‚Ää‚Äî‚Ääand then <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview">Imagen on Vertex AI</a> takes the kicker image and specific prompt to do <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/image/edit-inpainting">mask-based editing</a>, generating a new image that gets inserted into the player¬†card.</p><p>The result is often something spectacular, like what I got in back in¬†April.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*-bH8gix7azGpz2o5" /></figure><p>What a cool bonus to help memorialize my penalty kick experience!</p><h3>Highlighting Top Performers on a Leaderboard</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4DzcgsmZSlglZsL2" /></figure><p>No interactive demo experience where people get scores would be complete without a leaderboard, so of course all AI Penalty Challenge events have one. In this case, there‚Äôs a web client that renders the top performers along with some stats like total number of kicks for the day or overall event, backed by tables in <a href="https://cloud.google.com/bigquery/">BigQuery</a> with all the data for every¬†kick.</p><p>After each kick gets analyzed, Firestore triggers a Cloud Function that injects the complete kick stats as a row in BigQuery, so that the leaderboard updates in near real-time. My 80 in Las Vegas wasn‚Äôt enough to make the screen, but I‚Äôll console myself knowing that it‚Äôs hard to crack the top 20 when there were more than 1000 kicks a day at that particular event!</p><h3>Putting It All¬†Together</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*J4e3pCulpXQjdtLg" /></figure><p>So that‚Äôs it: 15+ Google products including some of Cloud‚Äôs most exciting AI tools like Gemini, Imagen, and custom text-to-speech‚Ää‚Äî‚Ääall part of the <a href="https://cloud.google.com/vertex-ai">Vertex AI</a> platform‚Ää‚Äî‚Ääcome together organically to power this one-of-a-kind experience.</p><p>If this gets you excited, look out for the AI Penalty Challenge at an event near you‚Ää‚Äî‚Ääincluding Google I/O in Mountain View later this week. And even if you can‚Äôt make it to the actual demo, <a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform">get started with Vertex AI</a> and stay tuned for more detailed architecture breakdowns and code coming soon‚Ää‚Äî‚Äähopefully to serve as inspiration to build the next great AI-powered experience with Google¬†Cloud!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eae42d014659" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/optimize-your-athletic-performance-with-gemini-and-vertex-ai-eae42d014659">Optimize Your Athletic Performance with Gemini and Vertex AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AsyncAPI gets a new version 3.0 and new operations]]></title>
            <link>https://medium.com/google-cloud/asyncapi-gets-a-new-version-3-0-and-new-operations-013dd1d6265b?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/013dd1d6265b</guid>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[software-engineering]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[event-driven-architecture]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Mon, 13 May 2024 08:32:53 GMT</pubDate>
            <atom:updated>2024-05-13T12:07:34.632Z</atom:updated>
            <content:encoded><![CDATA[<p>Almost one year ago, I talked about <a href="https://www.asyncapi.com/">AsyncAPI</a> 2.6 and how confusing its publish and subscribe operations can be in my <a href="https://atamel.dev/posts/2023/05-18_asyncapi_publishsubscribe_refactor">Understanding AsyncAPI&#39;s publish &amp; subscribe semantics with an example</a>¬†post.</p><p>Since then, a new 3.0 version of AsyncAPI has been released with breaking changes and a totally new send and receive operations.</p><p>In this blog post, I want to revisit the example from last year and show how to rewrite it for AsyncAPI 3.0 with the new send and receive operations.</p><h3>AsyncAPI 3.0</h3><p>AsyncAPI 3.0 was released in December 2023. Since it&#39;s a major version, it has some breaking changes. These two pages does a good job explaining the changes and the rationale behind¬†them:</p><ul><li><a href="https://www.asyncapi.com/blog/release-notes-3.0.0">AsyncAPI 3.0.0 Release¬†Notes</a>.</li><li><a href="https://www.asyncapi.com/docs/migration/migrating-to-v3">Migrating to¬†v3</a>.</li></ul><p>I won‚Äôt go through all the changes. For me, the biggest change is the separation of operations and channels, and changing publish and subscribe operations to send and¬†receive.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/693/0*JRRNdhOa9QSlg9u_.png" /></figure><h3>Recap: Publish and subscribe operations in AsyncAPI¬†2.6</h3><p>As a recap, AsyncAPI 2.6 has the following publish and subscribe operations with these semantics:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/783/1*qt82k4Q2nNRNfxRTiEz9xA.png" /></figure><p>In 2.6, publish and subscribe operations are from <strong>user&#39;s perspective</strong>.</p><h3>New: Send and receive operations in AsyncAPI¬†3.0</h3><p>In 3.0, the publish and subscribe operations are replaced with send and receive operations. In <a href="https://www.asyncapi.com/docs/migration/migrating-to-v3">Migrating to v3</a> page, the rationale is given as¬†follows:</p><blockquote>In v2, the publish and subscribe operations consistently caused confusion, even among those familiar with the intricacies.</blockquote><blockquote>When you specified publish, it implied that others could publish to this channel since your application subscribed to it. Conversely, subscribe meant that others could subscribe because your application was the one publishing.</blockquote><blockquote>In v3, these operations have been entirely replaced with an action property that clearly indicates what your application does. That eliminates ambiguities related to other parties or differing perspectives.</blockquote><p>While I agree that publish and subscribe were confusing, I&#39;m not sure if send and receive are less confusing. You still need to talk about whose perspective when defining these operations. AsyncAPI docs talk about <em>application</em> but that&#39;s not clear either. Does application refer to the code sending the message (user) or the code receiving the message (server)?</p><p>In Async 3.0, send and receive operations are from <strong>server&#39;s perspective</strong>. An example will¬†clarify.</p><h3>Account and Email¬†Services</h3><p>Let‚Äôs revisit our example from last year. You have two microservices: Account Service emits an userSignedUp event and Email Service receives that¬†event:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*9Ydxd0aMTSxeLya1" /></figure><p>How do you define such an architecture in AsyncAPI 2.6 vs¬†3.0?</p><h3>Account Service</h3><p>For Account Service, in 2.6, you had to define a channel with subscribe operation because the <strong>user</strong> has to subscribe to receive messages:</p><pre>asyncapi: 2.6.0<br><br>channels:<br>  user/signedup:<br>    subscribe:<br>      operationId: publishUserSignedUp<br>      message:<br>        $ref: &#39;#/components/messages/userSignedUp&#39;</pre><p><a href="https://github.com/meteatamel/asyncapi-basics/blob/main/samples/account-email-services/account-service-2.6.yaml">account-service-2.6.yaml</a></p><p>In 3.0, the channel does not have an operation anymore. Instead, there&#39;s a new publishUserSignedUp operation with send action that refers to the channel. This is because the <strong>server</strong> sends the¬†message:</p><pre>asyncapi: 3.0.0<br><br>channels:<br>  user/signedup:<br>    address: user/signedup<br>    messages:<br>      publishUserSignedUp.message:<br>        $ref: &#39;#/components/messages/userSignedUp&#39;<br><br>operations:<br>  publishUserSignedUp:<br>    action: send<br>    channel:<br>      $ref: &#39;#/channels/user~1signedup&#39;<br>    messages:<br>      - $ref: &#39;#/channels/user~1signedup/messages/publishUserSignedUp.message&#39;</pre><p><a href="https://github.com/meteatamel/asyncapi-basics/blob/main/samples/account-email-services/account-service-3.0.yaml">account-service-3.0.yaml</a></p><h3>Email Service</h3><p>Similarly, in Email Service, in 2.6, you had to define a publish operation because the <strong>user</strong> had to publish a message to the¬†server:</p><pre>asyncapi: 2.6.0<br><br>channels:<br>  user/signedup:<br>    publish:<br>      operationId: receiveUserSignedUp<br>      message:<br>        $ref : &#39;#/components/messages/userSignedUp&#39;</pre><p><a href="https://github.com/meteatamel/asyncapi-basics/blob/main/samples/account-email-services/email-service-2.6.yaml">email-service-2.6.yaml</a></p><p>However, in 3.0, the <strong>server</strong> receives a message, so the operation has receive¬†action:</p><pre>asyncapi: 3.0.0<br><br>channels:<br>  user/signedup:<br>    address: user/signedup<br>    messages:<br>      receiveUserSignedUp.message:<br>        $ref: &#39;#/components/messages/userSignedUp&#39;<br><br>operations:<br>  receiveUserSignedUp:<br>    action: receive<br>    channel:<br>      $ref: &#39;#/channels/user~1signedup&#39;<br>    messages:<br>      - $ref: &#39;#/channels/user~1signedup/messages/receiveUserSignedUp.message&#39;</pre><p>In this blog post, I explored a small part of AsyncAPI and explained the differences between 2.6 operations publish and subscribe and 3.0 operations send and receive. In a nutshell, in 2.6, you need to think from user&#39;s perspective and in 3.0 in server&#39;s perspective when you define these operations.</p><p>If you‚Äôre interested in learning more, I have a talk on CloudEvents and AsyncAPI and a repo with some¬†samples:</p><ul><li><a href="https://speakerdeck.com/meteatamel/open-standards-for-building-event-driven-applications-in-the-cloud">Open standards for building event-driven applications in the¬†cloud</a>.</li><li><a href="https://github.com/meteatamel/asyncapi-basics/">asyncapi-basics</a></li></ul><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/05-13_asyncapi_30_send_receive/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=013dd1d6265b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/asyncapi-gets-a-new-version-3-0-and-new-operations-013dd1d6265b">AsyncAPI gets a new version 3.0 and new operations</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Securing Anthos Workload With Chronicle Backstory‚Ää‚Äî‚ÄäA comprehensive approchg]]></title>
            <link>https://medium.com/google-cloud/securing-anthos-workload-with-chronicle-backstory-a-comprehensive-approchg-fcf4a9a3a78b?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/fcf4a9a3a78b</guid>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[gcp-security-operations]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[cybersecurity]]></category>
            <dc:creator><![CDATA[Imran Roshan]]></dc:creator>
            <pubDate>Mon, 13 May 2024 02:17:57 GMT</pubDate>
            <atom:updated>2024-05-13T02:17:57.011Z</atom:updated>
            <content:encoded><![CDATA[<h3>Securing Anthos Workloads With Chronicle Backstory‚Ää‚Äî‚ÄäA comprehensive approach</h3><p>Implementation process, threat detection strategies, and remediation workflows to get¬†started.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/900/0*RpN2FuNx2P5E8wai.jpg" /></figure><p>How cool would it be to have a single glass-pane to secure Anthos as well as GKE clusters on the go? Helping implement automation to be granular security findings before it escalates?</p><p>The world of multiple clouds and hybrid systems poses distinct security challenges. The Anthos platform from Google Cloud makes it easier to deploy applications in a variety of environments, but securing these workloads calls for a thorough strategy. With the recent integration of Chronicle Backstory, a threat detection and investigation tool, with Anthos, we can now leverage extended detection and response (XDR) capabilities throughout this intricate environment. This blog delves deeply into the technical aspects of using Chronicle Backstory to secure Anthos workloads, including data ingestion, threat hunting queries, and utilizing the integrations that are already built¬†in.</p><h3>Implementing Backstory with¬†Anthos</h3><p>Chronicle Backstory uses pre-existing data sources to identify potential threats. Our main goal in integrating it with Anthos will be to consume information from two main¬†sources:</p><p><strong>Cloud Audit Logs:</strong> Your GCP projects‚Äô administrative activity, including that of Anthos clusters, is recorded in these logs.<br><strong>GKE Logs: </strong>The Kubernetes Engine (GKE) logs offer valuable information about the activities of containers and possible security incidents that occur in your Anthos workloads.</p><h4>Configuring cloud audit logging for Backstory</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*e6I_qtdeaCqpVhuqPvmYXg.gif" /><figcaption><a href="https://images.app.goo.gl/dM7fV7uT2ZPAD6ZD9">https://images.app.goo.gl/dM7fV7uT2ZPAD6ZD9</a></figcaption></figure><p>We start with enabling cloud audit logs for our GCP project as starters, run the command below to add an IAM binding to the Chronicle ingestor service¬†account.</p><pre>gcloud projects add-iam-policy-binding PROJECT_ID \<br>  --member=&quot;serviceAccount:chronicle-ingestor@backstory.iam.gserviceaccount.com:user&quot; \<br>  --role=&quot;roles/logging.logWriter&quot;</pre><p>Now that we have a log ingestor in place we can go ahead and create a logging sink to backstory. With this setup, Anthos cluster activity from Cloud Audit Logs can be ingested by Backstory.</p><pre>gcloud logging sinks create backstory-sink \<br>  --log-filter=&quot;resource.type=cluster&quot; \<br>  --destination=&quot;projects/PROJECT_ID/sinks/backstory-sink&quot; \<br>  --destination-type=backstory</pre><p>Enabling GKE logging for backstory includes two¬†steps:</p><ul><li>Enable Stackdriver Kubernetes Engine Monitoring for your Anthos¬†cluster.</li><li>Create a sink within Stackdriver Monitoring to export logs to Backstory:</li></ul><pre># Configure the Stackdriver Logging Agent<br>apiVersion: logging.k8s.io/v2<br>kind: LoggingDeployment<br>metadata:<br>  name: backstory-agent<br>spec:<br>  sinkRefs:<br>  - name: &quot;backstory-sink&quot;<br>    namespace: &quot;logging&quot;<br>  # Replace with your Backstory ingestion endpoint<br>  outputDataset: &quot;projects/your-project-id/datasets/anthos-logs&quot;<br>  # Filters to select relevant container logs<br>  selectors:<br>  - expression: &quot;resource.type=k8s_container&quot;<br>---<br># Define the Stackdriver Logging Sink to route logs to Backstory<br>apiVersion: logging.k8s.io/v2<br>kind: LoggingSink<br>metadata:<br>  name: backstory-sink<br>spec:<br>  # Replace with your Backstory ingestion credentials<br>  secretRef:<br>    name: backstory-credentials<br>  destination: <br>    # Configure secure HTTPS destination for Backstory<br>        destination: &quot;https://your-backstory-endpoint.google.com/v2/ingest&quot;<br>    # Define the log format for Backstory ingestion<br>    outputFormat: &quot;json&quot;</pre><p>With this setup, Backstory receives GKE logs from your Anthos workloads that show container activity.</p><h3>Threat Detection With Backstory Queries</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*mkKqN1LfvAgBx4jO.jpg" /></figure><p>Using Chronicle Query Language, Backstory is highly proficient in threat detection (CQL). Here are a few instances:</p><h4>Detecting Suspicious APIs</h4><p>This query finds instances of unauthorized users within Anthos clusters making API calls to the Secrets¬†API.</p><pre>SELECT resource.labels.cluster_name, <br>       timestamp, <br>       protoPayload.methodName, <br>       protoPayload.request.principalEmail <br>FROM audit_log <br>WHERE protoPayload.methodName LIKE &#39;%/v1/secrets%&#39; <br>  AND NOT protoPayload.request.principalEmail LIKE &#39;%admin@yourdomain.com&#39;<br>ORDER BY timestamp DESC;</pre><h4>Unusual Container Activity Detection</h4><p>This query finds containers that crash frequently in Anthos workloads, which may be a sign of suspicious activity.</p><pre>SELECT resource.labels.cluster_name, <br>       container.name, <br>       timestamp, <br>       jsonPayload.reason <br>FROM container <br>WHERE jsonPayload.reason LIKE &#39;%CrashLoopBackOff%&#39; <br>ORDER BY timestamp DESC;</pre><pre># Find container executions with unusual resource usage<br>SELECT process.name, container.name, resource.usage.cpu.usage_in_cores, resource.usage.memory.usage_in_bytes<br>FROM logs<br>WHERE resource.type = &quot;k8s_container&quot;<br>AND resource.usage.cpu.usage_in_cores &gt; (AVG(resource.usage.cpu.usage_in_cores) + 3 * STDDEV(resource.usage.cpu.usage_in_cores))<br>OR resource.usage.memory.usage_in_bytes &gt; (AVG(resource.usage.memory.usage_in_bytes) + 3 * STDDEV(resource.usage.memory.usage_in_bytes))<br>ORDER BY resource.usage.cpu.usage_in_cores DESC, resource.usage.memory.usage_in_bytes DESC</pre><h4>Suspicious Login¬†Attempts</h4><p>This query looks for login attempts made during a specified time period from odd locations. Additional filtering options include user accounts or unsuccessful login attempts.</p><pre>SELECT user_email, source_ip, timestamp<br>FROM events<br>WHERE event_type = &#39;login.attempt&#39; AND<br>timestamp &gt;= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1d) AND<br>geo_country(source_ip) NOT IN (&#39;US&#39;, &#39;GB&#39;)  -- Replace with trusted countries</pre><h4>Potential Lateral Movements</h4><p>This query looks for user activity that may indicate lateral movement across clusters involving multiple GCP resources. Events can be narrowed down by particular resource kinds or activities that take place within a constrained time¬†frame.</p><pre>SELECT user_email, resource_type, resource_name, timestamp<br>FROM events<br>WHERE event_type IN (&#39;resource.create&#39;, &#39;resource.access&#39;)<br>GROUP BY user_email, resource_type, resource_name<br>HAVING COUNT(*) &gt; 5  -- Adjust threshold based on expected activity</pre><h4>Unusual File¬†Access</h4><p>This query looks for file access events coming from source IP addresses or unexpected user accounts. Additional filters can be applied based on particular file types or attempts to access data after business¬†hours.</p><pre>SELECT user_email, source_ip, file_path, timestamp<br>FROM events<br>WHERE event_type = &#39;file.access&#39;<br>AND (user_email NOT IN (&#39;admin@example.com&#39;, &#39;service_account@project.com&#39;)  -- Trusted accounts<br>OR geo_country(source_ip) NOT IN (&#39;US&#39;))  -- Trusted location </pre><h3>Remediation?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x9a8vmb4z1_JhlwL" /></figure><p>For automated remediation, Backstory integrates with a number of tools. Here are some examples using Cloud Functions (because that‚Äôs what I found closest at¬†hand).</p><p>Isolating infected workloads on the cluster requires a cloud function to be triggered via Backstory findings after which we can add the function:</p><pre>def isolate_workload(data, context):<br>  # Extract cluster name and pod details from Backstory alert.<br>  cluster_name = data[&#39;resource&#39;][&#39;labels&#39;][&#39;cluster_name&#39;]<br>  pod_name = data[&#39;container&#39;][&#39;name&#39;]<br><br>  # Use Kubernetes API to cordon the infected node.<br>  from kubernetes import client, config<br>  config.load_kube_config()<br>  v1 = client.AppsV1Api()<br>  v1.patch_namespaced_daemon_set(<br>      &quot;kube-system&quot;, &quot;kube-dns&quot;, body={&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;taints&quot;: [{&quot;effect&quot;: &quot;NoSchedule&quot;, &quot;key&quot;: &quot;infected&quot;}]}}}}<br>  )</pre><p>With the addition of a taint to stop additional pod scheduling, this Cloud Function automatically isolates the compromised node.</p><p>Further, you can implement something like:</p><pre>def remediate_backstory_finding(data, context):<br>  &quot;&quot;&quot;Cloud Function triggered by Backstory detection.&quot;&quot;&quot;<br>  # Parse the Pub/Sub message data<br>  pubsub_message = json.loads(data)<br>  backstory_finding = json.loads(pubsub_message[&quot;data&quot;])<br><br>  # Extract relevant details from the detection<br>  finding_name = backstory_finding[&quot;findingName&quot;]<br>  threat_type = backstory_finding[&quot;externalSystems&quot;][0][&quot;threatType&quot;]<br><br>  # Implement logic for remediation based on threat type<br>  if threat_type == &quot;MALWARE&quot;:<br>    # Example: Isolate the affected workload<br>    print(f&quot;Isolating workload associated with finding: {finding_name}&quot;)<br>    # Replace with your specific isolation workflow (e.g., API call to Anthos)<br>  elif threat_type == &quot;PORT_SCAN&quot;:<br>    # Example: Block suspicious IP addresses<br>    print(f&quot;Blocking suspicious IP addresses from finding: {finding_name}&quot;)<br>    # Replace with your specific IP blocking workflow (e.g., firewall rule update)<br>  else:<br>    print(f&quot;Unrecognized threat type: {threat_type} for finding: {finding_name}&quot;)<br>    # Implement logic for handling unknown threats or sending notifications<br><br></pre><p>A Pub/Sub message with the Backstory detection details in JSON format initiates the function. After parsing the message data, the threat type and finding name are extracted.<br>The function carries out particular remediation actions based on the type of threat. Including examples of workload isolation for malware and IP blocking for port scans in this¬†case.</p><h3>Conclude</h3><p>These are but a few simple instances. Depending on your unique Anthos environment, security posture, and the threats you want to find, you‚Äôll need to modify the queries. As the integration develops, it‚Äôs also advised to refer to the official Backstory documentation for the most recent syntax and functionalities.</p><h3>Get in¬†touch??</h3><p><a href="https://linktr.ee/imranfosec">imranfosec | Instagram | Linktree</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fcf4a9a3a78b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/securing-anthos-workload-with-chronicle-backstory-a-comprehensive-approchg-fcf4a9a3a78b">Securing Anthos Workload With Chronicle Backstory‚Ää‚Äî‚ÄäA comprehensive approchg</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Talk to pdf using Bigquery, GPT4-Turbo & langchain with memory]]></title>
            <link>https://siddoncloud.medium.com/talk-to-pdf-using-bigquery-vectors-gpt4-turbo-langchain-4eac63140211?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/4eac63140211</guid>
            <category><![CDATA[langchain]]></category>
            <category><![CDATA[chatgpt]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Sid]]></dc:creator>
            <pubDate>Mon, 13 May 2024 08:41:17 GMT</pubDate>
            <atom:updated>2024-05-13T10:07:28.538Z</atom:updated>
            <content:encoded><![CDATA[<p>In this brief article, i am going to show you how to leverage the langchain framework with OpenAI (gpt-4) to work with Google clouds BigQuery vector search offering.</p><p>We are going to use a PDF file which provides a comprehensive overview of trends in AI research and development as of 2023. It covers various aspects of AI advancements including the growth in AI publications, the evolution of machine learning systems, and significant trends in AI conference attendance and open-source AI software. Key highlights include detailed statistics on AI journal, conference, and repository publications categorized by type, field of study, and geographic area.</p><p>This PDF will be converted to text embeddings after which i will show you how to retrieve them using langchain‚Äôs <strong>ConversationalRetrievalChain with memory </strong>by creating a retriever object which will point to the embeddings and eventually talk to the PDF using simple search¬†queries.</p><p>So lets¬†begin.</p><p><strong>Note</strong>- You need an active GCP account for this tutorial, even a trial account will¬†do.</p><h4>Step-1: Install the necessary modules in your local environment</h4><blockquote>pip3 install‚Ää‚Äî‚Ääupgrade langchain langchain_google_vertexai</blockquote><blockquote>pip3 install‚Ää‚Äî‚Ääupgrade‚Ää‚Äî‚Ääquiet google-cloud-storage</blockquote><blockquote>pip3 install¬†pypdf</blockquote><h4>Step-2: Create a BigQuery Schema and download credentials file from GCP¬†Account</h4><p>Head over to bigquery, open up an editor and create a schema. Call it <strong>bq_vectordb </strong>and this is the schema where the table which will store our vector embeddings will be¬†created.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-LkhEbUf5INjqNH7uA5jjQ.png" /></figure><p>Now, navigate to <strong>IAM</strong> from the GCP console and select <strong>Service Accounts </strong>from the left navigation. Here we will create and download the permissions json file containing the private key which we will use in the Python script. This json file grants our local environment access to the services in our GCP account on a project¬†level.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*prwgQH8LWrlaiDLv5rO76g.png" /></figure><p>Click on <strong>Manage keys</strong> and then select <strong>ADD KEY</strong> followed by <strong>Create new key. </strong>Thats it, select the key type as JSON and a file will be automatically downloaded to your¬†system.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tMZn_SLrTwt2BRYUpQieEQ.png" /></figure><p>Rename and copy this file to your current working directory.</p><p>That was as far as the environment setup goes, now we can get to the execution part.</p><h4>Step-3: Create and Ingest Embeddings using VertexAIEmbeddings, GCSFileLoader &amp; BigQueryVectorSearch</h4><p>First, we need to create embeddings from the PDF File: <strong>example.pdf</strong> using <strong>VertexAIEmbeddings</strong>. To do that, we load this pdf file from a GCS bucket using <strong>GCSFileLoader</strong> from <strong>langchain</strong> and use the <strong>RecursiveCharacterTextSplitter</strong> to split this pdf into several chunks with an overlap size set to¬†100.</p><p><strong>NOTE: </strong>Before you execute the below code, make sure to upload example.pdf to a gcs bucket and change the path values accordingly.</p><blockquote>from langchain_google_vertexai import VertexAIEmbeddings</blockquote><blockquote>from langchain_community.vectorstores import BigQueryVectorSearch</blockquote><blockquote>from langchain.document_loaders import GCSFileLoader</blockquote><blockquote>from langchain_community.document_loaders import PyPDFLoader</blockquote><blockquote>from langchain.text_splitter import RecursiveCharacterTextSplitter</blockquote><blockquote>import os</blockquote><blockquote>os.environ[‚ÄòGOOGLE_APPLICATION_CREDENTIALS‚Äô] = ‚Äòyour-json-filename.json‚Äô</blockquote><blockquote>PROJECT_ID = ‚Äú{project-id}‚Äù</blockquote><blockquote>embedding = VertexAIEmbeddings(</blockquote><blockquote>model_name=‚Äùtextembedding-gecko@latest‚Äù, project=PROJECT_ID</blockquote><blockquote>)</blockquote><blockquote>gcs_bucket_name = ‚Äúyour-bucket-name‚Äù</blockquote><blockquote>pdf_filename = ‚Äútest_data/example.pdf‚Äù</blockquote><blockquote>def load_pdf(file_path):</blockquote><blockquote>return PyPDFLoader(file_path)</blockquote><blockquote>loader = GCSFileLoader(</blockquote><blockquote>project_name=PROJECT_ID, bucket=gcs_bucket_name, blob=pdf_filename, loader_func=load_pdf</blockquote><blockquote>)</blockquote><blockquote>documents = loader.load()</blockquote><blockquote>text_splitter = RecursiveCharacterTextSplitter(</blockquote><blockquote>chunk_size=10000,</blockquote><blockquote>chunk_overlap=100,</blockquote><blockquote>separators=[‚Äú\n\n‚Äù, ‚Äú\n‚Äù, ‚Äú.‚Äù, ‚Äú!‚Äù, ‚Äú?‚Äù, ‚Äú,‚Äù, ‚Äú ‚Äú,¬†‚Äú‚Äù],</blockquote><blockquote>)</blockquote><blockquote>doc_splits = text_splitter.split_documents(documents)</blockquote><blockquote>for idx, split in enumerate(doc_splits):</blockquote><blockquote>split.metadata[‚Äúchunk‚Äù] =¬†idx</blockquote><blockquote>print(f‚Äù# of documents = {len(doc_splits)}‚Äù)</blockquote><p>Once you have chunked your PDF data, now its time to ingest it into BigQuery vector¬†search.</p><p>Define your dataset (created in the first step) and table name. The table will be created at run time. Next, create an object <strong>BigQueryVectorSearch </strong>and use this object to invoke the <strong>add_documents</strong> method.</p><blockquote>DATASET = ‚Äúbq_vectordb‚Äù</blockquote><blockquote>TABLE = ‚Äúbq_vectors‚Äù # You can come up with a more innovative name¬†here</blockquote><blockquote>bq_object = BigQueryVectorSearch(</blockquote><blockquote>project_id=PROJECT_ID,</blockquote><blockquote>dataset_name=DATASET,</blockquote><blockquote>table_name=TABLE,</blockquote><blockquote>location=‚ÄùUS‚Äù,</blockquote><blockquote>embedding=embedding,</blockquote><blockquote>)</blockquote><blockquote>bq_object.add_documents(doc_splits)</blockquote><p>You can execute the entire <strong>bq_ingest_data.py </strong>script<strong> </strong>as a single python¬†script.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ck70GOSwdEcUkUsXMHZHSA.png" /></figure><p>Once the execution is complete, you can head back to Bigquery and refresh your schema. You should see a table <strong>bq_vectors </strong>with the below columns and data. This means your embeddings have been created and are now stored in a BigQuery¬†table.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iBOif5duBgOT-W3BcKMaPw.png" /></figure><h4>Step-4: Retrieve embeddings &amp; use langchain with OpenAI to chat with your¬†data</h4><p>Most of the below code is self-explanatory. We import the necessary libraries and use langchains <strong>ConversationBufferMemory</strong> which will retain the history of the chat in the subsequent messages which, is quite important if you are building a¬†chatbot.</p><p>Make sure to use the actual values in the below script before executing it.</p><blockquote>from langchain_community.vectorstores import BigQueryVectorSearch<br>from langchain_google_vertexai import VertexAIEmbeddings<br>from langchain_google_vertexai import VertexAI<br>from langchain.chains import RetrievalQA<br>from langchain.chains import ConversationalRetrievalChain<br>from langchain.memory import ConversationBufferMemory<br>from langchain.chat_models import ChatOpenAI<br>import pandas as pd <br>import¬†os</blockquote><blockquote>api_key = ‚Äúyour-openai-api-key‚Äù<br>os.environ[‚ÄòGOOGLE_APPLICATION_CREDENTIALS‚Äô] = ‚Äòjson-filename.json‚Äô</blockquote><blockquote>DATASET = ‚Äúbq_vectordb‚Äù<br>TABLE = ‚Äúbq_vectors‚Äù<br>PROJECT_ID = ‚Äúproject-id‚Äù</blockquote><blockquote>embedding = VertexAIEmbeddings(<br> model_name=‚Äùtextembedding-gecko@latest‚Äù, project=PROJECT_ID<br>)</blockquote><blockquote>memory = ConversationBufferMemory(memory_key=‚Äùchat_history‚Äù, return_messages=True,output_key=‚Äôanswer‚Äô)</blockquote><blockquote>bq_object = BigQueryVectorSearch(<br> project_id=PROJECT_ID,<br> dataset_name=DATASET,<br> table_name=TABLE,<br> location=‚ÄùUS‚Äù,<br> embedding=embedding,<br>)</blockquote><p>You can execute this code inside a jupyter notebook.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KaswBhP0_5ocyFZi7GdrOA.png" /></figure><p>We now define our <strong>llm model </strong>and create a <strong>retriever</strong> object which will point to the embeddings stored in the bigquery¬†table.</p><blockquote>llm_openai = ChatOpenAI(model=‚Äùgpt-4-turbo-2024‚Äì04‚Äì09&quot;,api_key=api_key)<br>retriever = bq_object.as_retriever()</blockquote><blockquote>conversational_retrieval = ConversationalRetrievalChain.from_llm(<br> llm=llm_openai,retriever=retriever, memory=memory,verbose=False<br>)</blockquote><p>Define a function which will simply accept a user query and return the answer from the bigquery vector¬†table.</p><blockquote>def QaWithMemory(query):<br> return conversational_retrieval.invoke(query)[‚Äúanswer‚Äù]</blockquote><p>Now lets ask a question¬†: ‚Äú <strong>What was the rate of growth in AI research publications from 2010 to 2021,<br> and which type of AI publication saw the most significant increase in this¬†period?</strong>‚Äù</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*m3ZA7ojEsmKHbYq11NL84Q.png" /></figure><p>You can see the response. Its quite accurate if you read the PDF content. You can ask a followup question now without giving too many details, such as <strong>‚Äúand how might this growth impact the future of AI research priorities?‚Äù</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Zivc9_q_VhoaA9U154SFUg.png" /></figure><p>Alright, that was it for this tutorial. Hope you enjoyed it¬†:-)¬†. Stay tuned for more.¬†Cheers</p><p><strong>Full Source code: </strong><a href="https://github.com/sidoncloud/gcp-use-cases/tree/main/langchain-bq-openai">https://github.com/sidoncloud/gcp-use-cases/tree/main/langchain-bq-openai</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4eac63140211" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Billed for unauthorized requests? Google Cloud Storage vs. AWS S3]]></title>
            <link>https://medium.com/google-cloud/billed-for-unauthorized-requests-google-cloud-storage-vs-aws-s3-8d4d6551fe72?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/8d4d6551fe72</guid>
            <category><![CDATA[aws]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[cloud-storage]]></category>
            <category><![CDATA[billing]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Marcos Manuel Ortega]]></dc:creator>
            <pubDate>Sat, 11 May 2024 04:44:46 GMT</pubDate>
            <atom:updated>2024-05-11T04:44:46.629Z</atom:updated>
            <content:encoded><![CDATA[<p>A recent story highlighted how unauthorized access to an empty AWS S3 bucket can result in surprising charges. This raised the question: can unauthorized access to Google Cloud Storage (GCS) also lead to unexpected bills?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*7wvAYoDth1XHDtqOA_BJJQ.jpeg" /></figure><h3>Catchup: what happened?</h3><p>A bit over a week ago, an interesting Medium story popped-up into many social¬†feeds:</p><p><a href="https://medium.com/@maciej.pocwierz/how-an-empty-s3-bucket-can-make-your-aws-bill-explode-934a383cb8b1">How an empty S3 bucket can make your AWS bill explode</a></p><p>It even made into <a href="https://news.ycombinator.com/item?id=40203126">Hacker News frontpage</a>.</p><p>To sum up: a user found that unauthorized 403 requests to upload files to his AWS S3 empty bucket by other users were billed to his account, as his bucket ID was used as a default value into a popular open-source solution and thousands of deployments tried by mistake to upload backup data into his¬†bucket.</p><p><strong>This was quite worrisome</strong>, as could be exploited by a malicious actor to bring anyone down just knowing any of our S3 bucket IDs: not by DoS but by <a href="https://academic.oup.com/cybersecurity/article/10/1/tyae004/7634012">Denial-of-Wallet attacks</a>.</p><p>This bore the question‚Ää‚Äî‚Ää<em>would this also happen on Google Cloud¬†Storage?</em></p><p>Spoiler alert: <strong><em>no, 403 PUT requests are not billed in¬†GCS.</em></strong></p><h3>Considerations</h3><h4>Base scenario</h4><p>The original story stated that 403 non-authorized requests to AWS S3 were¬†billed.</p><p>We wanted to also verify whether in Google Cloud Storage 403 non-authorized PUT requests to private GCS buckets are billed or¬†not.</p><h4>Number of¬†requests</h4><p>As per the <a href="https://cloud.google.com/storage/pricing#operations-by-class">GCS pricing docs</a>, listing and uploading objects are billed as <em>Class A operations</em> and downloading/reading objects as <em>Class B operations</em>. Pricing is per 1000 operations, so we aimed for a adequate number of requests to make sure we would notice the¬†charges.</p><h4>Requester pays¬†disabled</h4><p>In GCS and S3 you can opt-in for ‚Äúrequester pays‚Äù, where the user making the request is paying for it instead of the bucket‚Äôs owner, which is intended for other scenarios e.g. sharing public data, rather than e.g. public website static date. Therefore, it was intentionally disabled, as is the usual configuration.</p><h4>‚ÄúAlways free‚Äù tier and billing¬†exports</h4><p>In Google Cloud, there is a <a href="https://cloud.google.com/free/docs/free-cloud-features#storage">free tier for Cloud Storage</a> for some amount of Class A and B operations, so it would mask the costs of our requests.</p><p>Therefore, <a href="https://cloud.google.com/billing/docs/how-to/export-data-bigquery">detailed usage cost data</a> was exported to BigQuery and analyzed using SQL, where we could see the charges for operations even if they would fall in this free tier or be subjected to credits, discounts and promotions.</p><p>This cost data is exported to BigQuery with some latency (usually several hours, depending on service and SKU), so results were checked after 24¬†hours.</p><h3>Simulating unauthorized access: the experiment</h3><p>2 Google Cloud projects were¬†created:</p><ol><li><em>gcs-unauthorized-requests</em></li><li><em>gcs-requesting-instance</em></li></ol><p><em>Note: Usually you would want to keep your project IDs private for security.</em></p><h4>Billing</h4><p>Billing was configured for both projects with an active, good-standing billing account, with detailed usage cost data exported to BigQuery.</p><h4>Details for gcs-unauthorized-requests project</h4><ul><li>Required roles: Cloud Storage¬†Admin</li><li>Enabled APIs: Cloud Storage (<em>storage.googleapis.com</em>), BigQuery (<em>bigquery.googleapis.com</em>, to analyze exported billing data, could also be run in another¬†project)</li><li>Cloud Storage resources: location‚Ää‚Äî‚Ää<em>regional</em>, <em>europe-southwest1</em>, storage class‚Ää‚Äî‚Ää<em>standard</em></li><li>Private bucket: bucket-name-redacted, label bucket-private</li></ul><h4>Details for gcs-requesting-instance project</h4><ul><li>Required roles: Compute Engine Admin, Service Account¬†Admin</li><li>Enabled APIs: Compute Engine (<em>compute.googleapis.com</em>)</li><li>Networking‚Ää‚Äî‚ÄäVPC &amp; subnet:¬†<em>default</em></li><li>Networking‚Ää‚Äî‚ÄäFW rules: <em>default </em>VPC default rules ‚Üí SSH ingress enabled, all egress¬†enabled</li><li>VM instance: <em>requesting-instance</em></li><li>Region &amp; zone: <em>europe-southwest1-a</em> (Madrid)</li><li>Machine type: <em>e2-standard-16</em> (max. egress bandwidth of 16¬†Gbps)</li></ul><blockquote>(Would have been less expensive and recommended to use a spot VM instead of a regular one, but forgot to check the option‚Ä¶¬†<em>oops</em>)</blockquote><ul><li>VM instance OS: Debian 12 (bookworm) boot disk from the GCP public image¬†family</li><li>VM instance has GCP Cloud SDK already installed, as by default in (most) GCE¬†images</li><li>VM instance with an <em>ephemeral external¬†IP</em></li><li>User-managed service account assigned to the VM <strong>without any roles/permissions assigned</strong>, to force a 403¬†response</li></ul><h3>The test</h3><p>Test consisted in SSHing to requesting-instance VM and running this Bash command executing a 403 PUT request to the GCS bucket 2100 times (<em>more than the 1000 needed, just in¬†case</em>):</p><pre>for i in {1..2100};<br>do<br>  printf &quot;\niter $i\n&quot; &amp;&amp; \<br>  gsutil cp private-bucket-file.txt \<br>    gs://gcs-unauthorized-request-private;<br>done</pre><p>BigQuery SQL query for checking results (after 24 hours, given the expected delay for the billing¬†export):</p><pre>SELECT<br>  project.id as project_id,<br>  service.description as service_description,<br>  sku.description as sku_description,<br>  usage_start_time,<br>  usage_end_time,<br>  project.id,<br>  labels,<br>  location.region,<br>  resource.global_name,<br>  usage.amount,<br>  usage.unit,<br>  usage.amount_in_pricing_units,<br>  usage.pricing_unit<br>FROM<br>  `REDACTED.billing_export.gcp_billing_export_resource_v1_REDACTED`<br>WHERE<br>  usage_start_time &gt;= TIMESTAMP(&quot;2024-05-03&quot;)<br>  AND usage_start_time &lt; TIMESTAMP(&quot;2024-05-08&quot;)<br>  AND (project.id = &#39;gcs-unauthorized-requests&#39;<br>    OR project.id = &#39;gcs-requesting-instance&#39;)<br>  AND service.description = &#39;Cloud Storage&#39;<br>ORDER BY<br>  usage_start_time</pre><h3>Experiment results</h3><p>Remember: we wanted to check if 403 PUT requests to a private GCS bucket are¬†billed.</p><p>After executing the previous bash¬†command:</p><ul><li>Response status code:¬†403</li><li>Response: ‚ÄúAccessDeniedException: 403 <a href="mailto:requesting-instance@gcs-requesting-instance.iam.gserviceaccount.com">REDACTED@gcs-requesting-instance.iam.gserviceaccount.com</a> does not have storage.objects.create access to the Google Cloud Storage object.<br>Permission ‚Äòstorage.objects.create‚Äô denied on resource (or it may not¬†exist).‚Äù</li><li>Number of requests: 2100</li><li>Start: 24/05/07 08:26¬†CEST</li><li>End: 24/05/07 09:19¬†CEST</li></ul><p>After waiting +24 hours, we ran the query in the BigQuery billing¬†dataset.</p><blockquote><strong>SQL query results: </strong>No ‚ÄúRegional Standard Class A Operations‚Äù were shown for these projects and¬†dates</blockquote><h3>Verdict: No charges for unauthorized access</h3><p>The good news: Google Cloud Storage doesn‚Äôt bill for unauthorized PUT requests, neither to the private bucket owner nor the unauthorized requester.</p><h4>What This Means for¬†You</h4><p>Unlike AWS S3, Google Cloud Storage protects you from this ‚ÄúDenial-of-Wallet‚Äù attack where unauthorized access can inflate your bill. This adds another layer of security and cost control for your Cloud Storage¬†buckets.</p><h3>Further Exploration</h3><p>While unauthorized PUT requests are free, we plan to investigate billing for other access scenarios, including unauthorized GET requests and public bucket access attempts. In particular,</p><ol><li>Check if 403 GET requests to a private GCS bucket existing file are¬†billed</li><li>Check if 403 GET requests to a private GCS bucket non-existing file are¬†billed</li><li>Check if 403 PUT requests to a public GCS bucket are¬†billed</li><li>Check if 403 GET requests to a public GCS bucket non-existing file are¬†billed</li></ol><h4>Special thanks¬†to</h4><ul><li><a href="https://cloud.google.com/innovators/champions">Google Cloud Champions Innovator program</a>, for their members‚Äô support and feedback.</li><li><a href="https://cloud.google.com/innovators/innovatorsplus">Google Cloud Innovators Plus program</a>, for their annual free GCP credits which were used for this (and many others)¬†project.</li></ul><blockquote>This article was jointly written by Marcos Manuel Ortega (<a href="https://www.linkedin.com/in/marcosmanuelortega/">LinkedIn</a>), director at Indavelopers, and Julio Quinteros P. (<a href="https://twitter.com/jquinterosp">@jquinterosp</a>), Data &amp; AI/ML manager/practice lead at¬†Axmos</blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8d4d6551fe72" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/billed-for-unauthorized-requests-google-cloud-storage-vs-aws-s3-8d4d6551fe72">Billed for unauthorized requests? Google Cloud Storage vs. AWS S3</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud SLO demystified: Uncovering metrics behind predefined SLOs]]></title>
            <link>https://medium.com/google-cloud/google-cloud-slo-demystified-uncovering-metrics-behind-predefined-slos-40b153970479?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/40b153970479</guid>
            <category><![CDATA[observability]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[sre]]></category>
            <category><![CDATA[gcp-security-operations]]></category>
            <category><![CDATA[reliability]]></category>
            <dc:creator><![CDATA[minherz]]></dc:creator>
            <pubDate>Sat, 11 May 2024 04:43:14 GMT</pubDate>
            <atom:updated>2024-05-11T04:43:14.226Z</atom:updated>
            <content:encoded><![CDATA[<p><em>This post is mirrored from my personal website </em><a href="https://leoy.blog"><em>leoy.blog</em></a><em>. See reader-friendly </em><a href="https://leoy.blog/posts/google-cloud-slo-demystified/"><em>original</em></a><em>. This material was prepared with help from </em><a href="https://www.linkedin.com/in/ethantruong/"><em>Ethan¬†Truong</em></a><em>.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/799/1*MHwc3EM1GjxKpsDXL3YCTQ.png" /></figure><p>Google Cloud supports <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring"><strong>service monitoring</strong></a> by defining and tracking <a href="https://en.wikipedia.org/wiki/Service-level_objective"><strong>SLO</strong></a> of the services based on their metrics that are ingested to Google Cloud. This support greatly simplifies implementing SRE practices for services that are deployed to Google Cloud or that store telemetry data there. To make it even more simple to developers, the service monitoring is able to automatically detect many types of managed services and supports predefined <em>availability</em> and <em>latency</em> <a href="https://en.wikipedia.org/wiki/Service_level_indicator"><strong>SLI</strong></a> definitions for them.<br>When you define a new SLO you are prompted to select a predefined SLI or to define your¬†own.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/757/0*78QiebDtoB3PG1KF.png" /></figure><p>While it is convenient to use predefined SLIs you aren‚Äôt provided with information about SLI definitions. If you already defined an SLO using one of predefined SLIs, you can get its detailed description using the <a href="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives/get"><strong>services.serviceLevelObjectives.get</strong></a> API. For example, the following command returns JSON payload that describes the SLO named availability_slo of the frontend service that was defined using the predefined availability SLI:</p><pre>curl -X GET \<br>https://monitoring.googleapis.com/v3/projects/${GOOGLE_CLOUD_PROJECT}/\<br>services/frontend/\<br>serviceLevelObjectives/frontend-availability-slo?view=EXPLICIT \<br>-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>-H &quot;Content-Type: application/json; charset=utf-8&quot;</pre><p>This example uses gcloud <a href="https://cloud.google.com/sdk/gcloud"><strong>CLI</strong></a> and the environment variable GOOGLE_CLOUD_PROJECT. This variable is automatically set by <a href="https://cloud.google.com/shell/docs/launching-cloud-shell"><strong>Cloud Shell</strong></a>. However, if you run this command in your shell, you will need to <a href="https://cloud.google.com/sdk/docs/install-sdk"><strong>install</strong></a> gcloud, then to authenticate vs Google Cloud and to set up the GOOGLE_CLOUD_PROJECT environment variable to the project ID where your SLO is defined. The following paragraphs describe predefined SLIs to enable you to make an educated choice next time you use¬†them.</p><h3>Managed (auto-detected) services</h3><p>The <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/api/using-api#managing-services"><strong>managed services</strong></a> generate metrics that are further used to calculate predefined SLIs. The service monitoring automatically detects these managed services or the user can provision them manually. As long as the service is defined as one of the basic services and uses one of the following types, it can leverage the predefined SLIs.</p><ul><li>APP_ENGINE ‚Äì Every <a href="https://cloud.google.com/appengine/docs/standard/testing-and-deploying-your-app"><strong>deployed application</strong></a> is considered a service. This service can be monitored using multiple ways including SLOs.</li><li>ISTIO_CANONICAL_SERVICE ‚Äì When vanilla Istio is configured to ingest its metrics to Cloud Monitoring, its services are auto-detected as Istio canonical services. GKE workloads that leverage the managed version of Istio (aka <a href="https://cloud.google.com/service-mesh"><strong>ASM</strong></a>) do not use this¬†type.</li><li>CLUSTER_ISTIO ‚Äì Services that run on GKE with ASM are identified as Cluster Istio services. Note that since ASM supports Kubernetes clusters on Azure and AWS you can implement service monitoring for these clusters as¬†well.</li><li>CLOUD_RUN ‚Äì Cloud Run <a href="https://cloud.google.com/run/docs/overview/what-is-cloud-run#services"><strong>services</strong></a> are auto-detected using this type. Important to remember that service monitoring does not support Cloud Run¬†jobs.</li></ul><blockquote><strong>NOTE:</strong> Not all services that Service monitoring automatically detects, have predefined SLIs. For example, services of types GKE_SERVICE or GKE_WORKLOAD do not have predefined SLIs (because they do not ingest any metrics to Google¬†Cloud).</blockquote><h3>Predefined SLIs</h3><p>All predefined SLIs are measured in % and calculated based on the well known formula¬†of</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/321/0*SFvP6lTuqqsst9Qp.jpg" /></figure><p>The predefined SLIs use a <strong>resource type</strong> and a <strong>metric</strong> to get the event data. The <strong>availability metric</strong> is used to get availability events and the <strong>latency metric</strong> is used to get latency events. From all metric data the good events are derived using a <strong>good service filter</strong> and valid events‚Ää‚Äî‚Ääusing the union of <strong>good service filter</strong> <em>AND</em> <strong>bad service filter</strong>. The filters are built using metric labels and leverage the fact that both availability and latency metrics have the same set of labels. You can use the following MQL query to see the <em>valid¬†events</em>:</p><pre>fetch %%resource%%<br>| metric &#39;%%metric%%&#39;<br>| filter (%%good_service_filter%%) || (%%bad_service_filter%%)</pre><p>And this MQL query to see the <em>good¬†events</em>:</p><pre>fetch %%resource%%<br>| metric &#39;%%metric%%&#39;<br>| filter %%good_service_filter%%</pre><blockquote><strong>NOTE:</strong> You will have to replace values enclosed with double percentage (%%) with the actual values of the predefined SLIs.</blockquote><h3>AppEngine service</h3><p>Detects both standard and flexible AppEngine services.</p><p><strong>Resource type</strong>:¬†gae_app</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#appengine/http/server/response_count"><strong>appengine.googleapis.com/http/server/response_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#appengine/http/server/response_latencies"><strong>appengine.googleapis.com/http/server/response_latencies</strong></a></p><p><strong>Good service¬†filter</strong>:</p><pre>resource.module_id = &#39;MODULE_ID&#39; &amp;&amp;<br>metric.response_code &gt;= 200 &amp;&amp; metric.response_code &lt;= 299</pre><blockquote><strong>NOTE:</strong> MODULE_ID should be replaced with the actual AppEngine service/module name. See <a href="https://cloud.google.com/monitoring/api/resources#tag_gae_app"><strong>gae_app</strong></a> resource description for more information.</blockquote><p><strong>Bad service¬†filter</strong>:</p><pre>resource.module_id = &#39;MODULE_ID&#39; &amp;&amp;<br>metric.response_code &gt;= 500 &amp;&amp; metric.response_code &lt;= 599</pre><h3>Canonical Istio¬†service</h3><p>Detects Istio services provisioned by OSS¬†Istio.</p><p><strong>Resource type</strong>: istio_canonical_service</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/request_count"><strong>istio.io/service/server/request_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/response_latencies"><strong>istio.io/service/server/response_latencies</strong></a></p><p><strong>Good service¬†filter</strong>:</p><pre>resource.mesh_uid = &#39;MESH_ID&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>resource.canonical_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 200 &amp;&amp; metric.response_code &lt;= 299</pre><blockquote><strong>NOTE:</strong> MESH_ID should be replaced with the identifier for an Istio service mesh. K8S_NAMESPACE should be replaced with the namespace where the service is manifested. K8S_SERVICE_NAME should be replaced with the name of the <a href="https://cloud.google.com/service-mesh/docs/canonical-service"><strong>canonical service</strong></a>. See <a href="https://cloud.google.com/monitoring/api/resources#tag_istio_canonical_service"><strong>istio_canonical_service</strong></a> resource description for more information.</blockquote><p><strong>Bad service¬†filter</strong>:</p><pre>resource.mesh_uid = &#39;MESH_ID&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>resource.canonical_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 500 &amp;&amp; metric.response_code &lt;= 599</pre><h3>ASM service</h3><p>Detects the managed flavor of the Istio service. Used with Istio services provisioned by ASM, hence uses a different resource¬†type.</p><p><strong>Resource type</strong>: k8s_container</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/request_count"><strong>istio.io/service/server/request_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/response_latencies"><strong>istio.io/service/server/response_latencies</strong></a></p><p><strong>Good service¬†filter</strong>:</p><pre>resource.cluster_name = &#39;CLUSTER_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_namespace = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 200 &amp;&amp; metric.response_code &lt;= 299</pre><blockquote><strong>NOTE:</strong> CLUSTER_NAME should be replaced with the name of the cluster running the service. LOCATION should be replaced with the location (either zone or region) of the service. K8S_NAMESPACE should be replaced with the namespace where the service is manifested. K8S_SERVICE_NAME should be replaced with the name of the service. See <a href="https://cloud.google.com/monitoring/api/resources#tag_k8s_container"><strong>k8s_container</strong></a> and the <a href="https://cloud.google.com/monitoring/api/metrics_istio#istio/service/server/request_count"><strong>metric</strong></a> descriptions for more information.</blockquote><p><strong>Bad service¬†filter</strong>:</p><pre>resource.cluster_name = &#39;CLUSTER_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>resource.namespace_name = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_namespace = &#39;K8S_NAMESPACE&#39; &amp;&amp;<br>metric.destination_service_name = &#39;K8S_SERVICE_NAME&#39; &amp;&amp;<br>metric.response_code &gt;= 500 &amp;&amp; metric.response_code &lt;= 599</pre><h3>Cloud Run¬†service</h3><p>Detects a service deployed at Cloud¬†Run.</p><p><strong>Resource type</strong>: cloud_run_revision</p><p><strong>Availability metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#run/request_count"><strong>run.googleapis.com/request_count</strong></a></p><p><strong>Latency metric</strong>: <a href="https://cloud.google.com/monitoring/api/metrics_gcp#run/request_latencies"><strong>run.googleapis.com/request_latencies</strong></a></p><p><strong>Good service¬†filter</strong>:</p><pre>resource.service_name = &#39;SERVICE_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>metric.response_code_class = &#39;5xx&#39;</pre><blockquote><strong>NOTE:</strong> SERVICE_NAME should be replaced with the name of the Cloud Run service. LOCATION should be replaced with the region where the service is deployed. See <a href="https://cloud.google.com/monitoring/api/resources#tag_cloud_run_revision"><strong>cloud_run_revision</strong></a> for more information.</blockquote><p><strong>Bad service¬†filter</strong>:</p><pre>resource.service_name = &#39;SERVICE_NAME&#39; &amp;&amp;<br>resource.location = &#39;LOCATION&#39; &amp;&amp;<br>metric.response_code_class = &#39;5xx&#39;</pre><h3>Afterword</h3><p>The predefined SLIs are used with the ‚Äú<a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#slo-type-request"><strong>request-based</strong></a>‚Äù SLOs. This is because the auto-detected services communicate using requests. If you use the services differently or, in your case the metrics used in the predefined SLIs do not reflect good and valid events correctly, use <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/ui/create-slo#svcmon-sli-other"><strong>custom SLI</strong></a> for your availability and latency¬†SLOs.</p><p>The shown filters values use syntax that is compatible with MQL. If you plan to reuse them in custom SLIs with <a href="https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/monitoring_slo"><strong>Terraform</strong></a> or by calling <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/api/api-structures"><strong>API</strong></a>, you should reformat it. The main changes include replacing &amp;&amp; with AND, changing the keys of the labels by adding label suffix (e.g. metric.response_code will become metric.label.response_code). Look into documentation for more guidelines.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=40b153970479" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/google-cloud-slo-demystified-uncovering-metrics-behind-predefined-slos-40b153970479">Google Cloud SLO demystified: Uncovering metrics behind predefined SLOs</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GCP Cross-region internal application load balancer¬†: why and how]]></title>
            <link>https://medium.com/google-cloud/gcp-cross-region-internal-application-load-balancer-why-and-how-f3a33226d690?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/f3a33226d690</guid>
            <category><![CDATA[load-balancer]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[networking]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Gauravmadan]]></dc:creator>
            <pubDate>Sat, 11 May 2024 04:42:52 GMT</pubDate>
            <atom:updated>2024-05-11T04:42:52.833Z</atom:updated>
            <content:encoded><![CDATA[<h3>GCP Cross-region internal application load balancer¬†: why and¬†how</h3><p>Context¬†:</p><p>A Google Cloud<strong><em> internal Application Load Balancer</em></strong> is a proxy-based layer 7 load balancer that enables you to run and scale your services behind a single internal IP address. The internal Application Load Balancer distributes HTTP and HTTPS traffic to backends hosted on a variety of Google Cloud platforms such as Compute Engine, Google Kubernetes Engine (GKE), and Cloud Run. This load balancer is available in 2 flavors¬†:</p><ol><li>Regional internal application load¬†balancer</li><li>Cross-regional internal application load¬†balancer</li></ol><p>The cross-region internal application load balancer enables you to load balance traffic to backend services that are globally distributed, including traffic management that ensures traffic is directed to the closest¬†backend.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/911/0*1RXP-_RyrxR_ngJ6" /></figure><p>The idea of this blog post is to see how cross-regional internal application load balancer is deployed and its use-cases under steady state and failover situations.</p><p>Failover may fall in 2 categories¬†: (a) failover of a regional backend (b) failover of iLB¬†frontend</p><ul><li>Failover of Frontend: When the proxy tasks in a region (where the forwarding rule is configured) fail. Failover to a forwarding rule in a different region is done using a DNS failover¬†policy.</li><li>Failover of Backend: When all of the backends (e.g. VM instances) in a region fail, the load balancer automatically chooses healthy, available backends in the next closest¬†region.</li></ul><h3>Topology</h3><p>In the test topology used in this blog¬†, I had one VPC with subnets in 2 regions¬†: asia-south1 ( Mumbai) and asia-south2 ( Delhi )¬†. The CIDR used in each subnet is shown in the topology diagram below¬†. These 2 subnets host the instance groups where web application servers are¬†hosted.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*XsF-O-_jjjgd-dLW" /></figure><p>As a next step ( and a mandatory step ) for setting up cross regional internal application load balancer¬†, it is necessary to reserve PROXY subnet for each region in context¬†. In my example¬†, I have reserved 2 proxy subnets ( one each for asia-south1 and asis-south2 )</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/742/0*jCHDHuwtb6bpguLO" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tZyjIMJGJNJCbZ9h" /></figure><p>Setup Load balancer¬†: Lets break this in 3 simple¬†parts</p><ol><li>Front end¬†setup</li><li>Backend setup</li><li>Routing rules</li></ol><p><strong>Front End¬†:</strong> Create one front end per region¬†. You may choose the protocol as HTTP or HTTPs for the front end¬†. In our case¬†, we had following configured for the front¬†end</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oRK2qM9x-wbnZ3K4" /></figure><p><strong>Backend</strong></p><p>Created one backend¬†service</p><p>In this backend service ‚Üí I created 2 backends¬†. One for each regional instance group.In my case¬†, each backend consist of instance groups running web¬†server.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fx1m5yQriei4epnU" /></figure><p>Sample output of load balancer backend¬†service</p><pre>$ gcloud compute backend-services describe cross-region-bserv-for-ilb --global<br>affinityCookieTtlSec: 0<br>backends:<br>- balancingMode: UTILIZATION<br>  capacityScaler: 1.0<br>  group: https://www.googleapis.com/compute/v1/projects/gm-test-337806/zones/asia-south1-c/instanceGroups/lb-test-mumbai-umig<br>  maxRatePerInstance: 100.0<br>  maxUtilization: 0.8<br>- balancingMode: UTILIZATION<br>  capacityScaler: 1.0<br>  group: https://www.googleapis.com/compute/v1/projects/gm-test-337806/zones/asia-south2-a/instanceGroups/lb-test-delhi-01<br>  maxRatePerInstance: 100.0<br>  maxUtilization: 0.8<br>connectionDraining:<br>  drainingTimeoutSec: 300<br>creationTimestamp: &#39;2024-04-27T21:04:05.231-07:00&#39;<br>description: &#39;&#39;<br>fingerprint: WRypjLxKN0M=<br>healthChecks:<br>- https://www.googleapis.com/compute/v1/projects/gm-test-337806/global/healthChecks/auto-health<br>id: &#39;1322192009993155674&#39;<br>kind: compute#backendService<br>loadBalancingScheme: INTERNAL_MANAGED<br>localityLbPolicy: ROUND_ROBIN<br>logConfig:<br>  enable: false<br>  optionalMode: EXCLUDE_ALL_OPTIONAL<br>name: cross-region-bserv-for-ilb<br>port: 80<br>portName: lb-test-mumbai-http<br>protocol: HTTP<br>selfLink: https://www.googleapis.com/compute/v1/projects/gm-test-337806/global/backendServices/cross-region-bserv-for-ilb<br>sessionAffinity: NONE<br>timeoutSec: 30<br>usedBy:<br>- reference: https://www.googleapis.com/compute/v1/projects/gm-test-337806/global/urlMaps/cross-regional-internal-layer7</pre><h3>TEST #¬†1</h3><p>Client sending traffic to regional front-end¬†:In this test¬†, we assumed that regional clients will send traffic to respective iLB front end¬†; i.e Mumbai client sent traffic to ilB front end in Mumbai (asia-south1) and Delhi client sent requests to Delhi (asia-south2) front¬†end</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JQ_eqgNPJ7FLA4Km" /></figure><p>Test initiated from TEST-CLIENT-2 (Delhi¬†region)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nsf8Ldi_ft8FSE8Ard0sSA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wNpJFYX8rVZbXdpRiAJxVA.png" /></figure><p>This worked as expected¬†. Customer relied on making separate entries in DNS for each front end and this was used by respective clients in their application access method. The load balancer served traffic from closest healthy backend available.</p><h3>TEST #¬†2</h3><p>Client sending traffic to cross-regional front-end¬†: This is continuation of test case 1¬†, where the requirement is to access a workload in a cross-region¬†; i.e. client in asia-south1 (Mumbai) trying to access workload in asia-south2 by accessing front end name / IP of layer7 iLB in asia-south2 region</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5xlcWKelOS-SYrhF" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4gFU4GGLPR868OnKlK-chg.png" /></figure><p>Hence¬†, once the requests land on a iLB frontend¬†, it tries to route it to a healthy backend nearest to¬†it</p><h3>TEST #¬†3</h3><p>Failover Scenario¬†: In this case¬†, let‚Äôs assume service in asia-south2 failed ( Or all VMs down )¬†. For simulation in my setup¬†, I removed the backend in Delhi region from the iLB backend service configuration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5V3H5pYuX7hmvtDc" /></figure><p>Lets see the¬†results</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NPgfHr0OtxKuAAAPfwG98Q.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zz7jqMMh8ezDuhxHai22CQ.png" /></figure><p>Hence¬†, as expected¬†, in event of no healthy backend¬†, the iLB will redirect the traffic to healthy backends in next closest¬†region.</p><h3>TEST #¬†4</h3><p>Combining Cross region ILB functionality with GCP CLOUD DNS GEO routing¬†POLICY</p><p>This is most interesting usecase¬†. Here¬†; instead of using separate domain names for application hosted in asia-south1 / asia-south2, the customer created a domain name called ‚Äú<a href="http://application.myapp.com">application.myapp.com</a>‚Äù. In addition to this¬†, customer created a GEO routing policy specifying region as ‚Äòasia-south1‚Äô resolve to 10.10.152.10 [ load balancer front end in asia-south1 ] and asia-south2 resolve ot 10.10.151.11 [ load balancer front end in asia-south1 ]¬†. Sample DNS configuration is as follows¬†-</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*u81MR6Lacpz10SnG" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*eNvPTqsf_g-1OAKE" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*A-AcPldmLJwaN4B4R8RFHQ.png" /></figure><p>When client attempted to access the app behind<strong> </strong><a href="http://application.myapp.com">application.myapp.com</a><strong>¬†</strong>; the response came from the nearby¬†region</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*w1m9uOolhhvormxUGHsUXg.png" /></figure><p>BACKEND FAILOVER ( no healthy backend in one of region¬†)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QST9fnimdqhQMzGi" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0hChkmK94Q9SMUc8Qdhq9w.png" /></figure><p>Again¬†, as expected¬†, no matter which frontend IP the query lands¬†, iLB send the request to nearby healthy backend. For example¬†, if client in Delhi region sends request to front-end in Delhi-region of iLB¬†, this will be routed to next near healthy backend ( which is Mumbai in my setup¬†)</p><h3>TEST #¬†5</h3><p>This test has to do with failover of proxy task in a given region¬†. This is where Failover Policy of Cloud DNS can come¬†handy.</p><p>In this setup¬†, customer needs to serve everything from Asia-south1 application instance and if this is not available / reachable¬†; asia-south2 instance shall be considered¬†.</p><p>Lets look at Cloud DNS Failover¬†Policy</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7fuPsVppPUXHl6cxa31kaA.png" /></figure><p>This simply means that clients will resolve to iLB frontend IP address in Asia-south1 ( 10.10.152.10 ) and if this isnt reachable¬†; clients in both regions will resolve app1.myapp.com to iLB front-end IP in asia-south2 ( 10.10.151.12 )</p><p>Hence in steady state the setup will work like¬†follows</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sQ5ubMo1PNetYpbPngKuUQ.png" /></figure><p>However in case of Front end fail¬†, the setup will work like following</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93E6z75MsUUVRDhMInYsRw.png" /></figure><p>Useful URLs</p><p><a href="https://cloud.google.com/load-balancing/docs/l7-internal/setting-up-l7-cross-reg-internal">Setup cross-regional internal L7 load¬†balancer</a></p><p><strong>Disclaimer</strong>: This is to inform readers that the views, thoughts, and opinions expressed in the text belong solely to the author, and not necessarily to the author‚Äôs employer, organization, committee or other group or individual.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f3a33226d690" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/gcp-cross-region-internal-application-load-balancer-why-and-how-f3a33226d690">GCP Cross-region internal application load balancer¬†: why and how</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[API to Chatbot in less than 5 minutes | Vertex AI Extensions]]></title>
            <link>https://medium.com/google-cloud/api-to-chatbot-in-less-than-5-minutes-27ad6c4a063d?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/27ad6c4a063d</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[chatbots]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <dc:creator><![CDATA[Vaibhav Malpani]]></dc:creator>
            <pubDate>Fri, 10 May 2024 10:17:51 GMT</pubDate>
            <atom:updated>2024-05-10T10:17:51.650Z</atom:updated>
            <content:encoded><![CDATA[<p>Learn how to quickly create a chatbot over your API using Vertex AI Extensions. No coding or training required. Interact with your API seamlessly through a chat interface. Read more to explore the possibilities!</p><h3>What is¬†API?</h3><p>An API (Application Programming Interface) serves as a bridge between various software programs. It defines a set of rules and requirements that apps may follow to interact with one¬†another.</p><h3>What is¬†Chatbot?</h3><p>A software program that simulates human-user conversations. Chatbots are frequently used in messaging apps, websites, and customer service to offer information or do¬†tasks.</p><p><strong>Traditionally</strong>, a Chatbot is trained on intents to understand which data to be fetched, what all parameters are required and how to extract them from the prompt given by user and at the end format the API response for easy reading. This will improve readability for the¬†user.</p><h4><strong>Problem Statement:</strong></h4><p>If any changes are made to API, like taking in a new parameter, change in the response from API, in that case, we will have to make the changes in the chatbot as well to handle the new changes. Similarly, if a new API is added, then the whole process of creating intent, extracting parameters, prettifying API response.</p><h4><strong>Solution:</strong></h4><p>Using Extension in Vertex AI, to build Chatbots over your API. How to do it?? Just follow the steps¬†below!!</p><blockquote><strong>Disclaimer: Extensions in Vertex AI is still in PREVIEW. It </strong>Is under development and gaining functionalities.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NzBLlx8ssBksQoAYY9f9Yg.jpeg" /><figcaption>AI Generated by SDXL¬†1.0</figcaption></figure><h4><strong>How to Build a Chatbot over your¬†API?</strong></h4><ol><li>Create a sample <a href="https://swagger.io/docs/specification/basic-structure/"><strong><em>OpenAPI specification file</em></strong></a><strong><em> </em></strong>(only yaml file supported till now). I have used a sample file from <a href="https://petstore.swagger.io/#/"><strong>swagger</strong></a><strong> </strong>and trimmed down the file to only include API related to¬†pets.</li><li>Login to Google cloud console, and navigate to <a href="https://console.cloud.google.com/vertex-ai/extensions"><strong>Extensions in Vertex AI</strong></a>. Click on ‚ÄúCreate Extension‚Äù</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/967/1*XhAbnI0WjAi2LwiGO9NqRg.png" /></figure><p>3. Enter the name that you want to give to the extension, we have given name as ‚Äúpet_store‚Äù. Then enter any description that you want and in the Extension Type, Click on <strong>‚ÄúCustom‚Äù. </strong><em>(For other 2 Extension Types stay tuned and follow, I am going to soon write about¬†them)</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/1*n4HJxa0_rimgxzJN0EoN5w.png" /></figure><p>4. Once you select Custom Extension Type, you will get a below form, just enter details according to your API. In the OpenAPI Spec file section, upload you YAML file created in STEP¬†1.</p><p>If there are no error, you would get the below confirmation as ‚ÄúNo errors detected‚Äù. Along with that it will also list down all the APIs that are present in the YAML file. (as seen in the below image). For this demo, with will select ‚ÄúNo Authentication‚Äù, but if your application requires authentication you can set that up in the ‚ÄúAuthentication‚Äù dropdown.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/620/1*a1AYbiYTqah3RI1vpg6tGg.png" /></figure><p>5. Once the extension is created you would see a screen like shown below. This shows that your Chatbot is ready and you can query it in the below text box. Let us try some examples and¬†check.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VTDjZCc7YoZULPsMZN79Cw.png" /></figure><h4>Testing the¬†Chatbot:</h4><ol><li>Lets try to create a new pet in our pet store. I just put the query as ‚Äúadd a pet‚Äù and it was quickly able to understand what are the required parameters to create a pet. So the bot asked me one by one to enter the name and then the id for our new pet. Once it got all the required information, it created the¬†pet.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Xj_zCG3FaOXULUAbx4gNlw.jpeg" /></figure><p>2. Let‚Äôs try to get the information for our newly created pet. When I ask ‚Äúget pet‚Äù, it understood that there is not API to get all pets and hence asked me for an id. Once the id is entered, the chatbot was able to fetch the information for the pet we just now¬†created.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aY_yRDGdq3PUsFbJ7COFOw.jpeg" /></figure><p>3. Let‚Äôs add one more pet. When queried for ‚Äúadd a pet‚Äù, it asked for the name, then id and then finally the status. Notice how it gave me the options for status (available, pending or sold). This is because these are the options mentioned in the YAML file for the status¬†field.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GAioFu8z194NXUT8eqYVnA.jpeg" /></figure><p>4. Let‚Äôs try to get the new pet that we created. The Chatbot gets the complete information when the id is¬†entered.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AqY7U5u3i0Fqz3Kolsnd9Q.jpeg" /></figure><p>5. Now let us try to update the pet and see if the bot is able to handle it. So when I query ‚Äúupdate pet‚Äù, it asks for the id, and then asks what field i need to update, is it name or status. So when I enter ‚Äústatus is sold‚Äù, it updates that and gives the response for the successful status¬†update.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vxeEJ46eOZUPZC9R70CPIg.jpeg" /></figure><p>6. To be actually sure if the value is updated in backend or not, I tried to get the pet infomation. But this time, in the first query itself i have the id and did a query as ‚Äúget pet 123‚Äù. This time the Chatbot got the id and did not ask again for the id. In the very next step, it has come back with the response saying the pet with id 123 is¬†sold.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1013/1*XnLPpJCCto9yMK2FT4YuXg.jpeg" /></figure><p>7. Finally, lets now try to delete the pet. When I query ‚Äúdelete pet‚Äù, the Chatbot asks me for the id, and when the id is entered it deletes that entry for pet with id¬†123.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1010/1*4opSTPvFIBWlGFsfxmjObg.jpeg" /></figure><p>As seen above, We were able to interact with our API, perform CRUD operations (Create, Read, Update, Delete) through the Chatbot which required no training, no¬†coding!!</p><h3>Opportunities for improvement<strong>:</strong></h3><ol><li>It only supports Content Type with ‚Äúapplication/JSON‚Äù for input and¬†output.</li><li>Currently it does not work with query parameters.</li><li>There should be an option to update the YAML file once uploaded. Currently to update the YAML, you would have to delete the old Extension and create a new¬†one.</li><li>Currently it only supports calling the Chatbot through REST API or through GCP console. It would be great to have it integrated in the Vertex AI¬†SDK.</li></ol><h3>Conclusions:</h3><ol><li>It have become very easy to create a Chatbot over your¬†APIs.</li><li>You won‚Äôt have to train the Chatbot again once new APIs are¬†added.</li><li>You won‚Äôt have to manage the context and parameter values given by the¬†user.</li></ol><h3>If you enjoyed this post, give it a clap! üëè¬†üëè</h3><h3>Interested in similar content? Follow me on <a href="https://medium.com/@IVaibhavMalpani">Medium</a>, <a href="https://twitter.com/IVaibhavMalpani">Twitter</a>, <a href="https://www.linkedin.com/in/ivaibhavmalpani/">LinkedIn</a> for¬†more!</h3><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=27ad6c4a063d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/api-to-chatbot-in-less-than-5-minutes-27ad6c4a063d">API to Chatbot in less than 5 minutes | Vertex AI Extensions</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>