<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Laurent Picard on Medium]]></title>
        <description><![CDATA[Stories by Laurent Picard on Medium]]></description>
        <link>https://medium.com/@PicardParis?source=rss-6be63961431c------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*OMNmXavx5hd7qUUZxNYLhA.jpeg</url>
            <title>Stories by Laurent Picard on Medium</title>
            <link>https://medium.com/@PicardParis?source=rss-6be63961431c------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 14 May 2024 18:06:06 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@PicardParis/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI]]></title>
            <link>https://medium.com/google-cloud/making-ai-more-open-and-accessible-to-cloud-developers-with-gemma-on-vertex-ai-4b0fc2a14851?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/4b0fc2a14851</guid>
            <category><![CDATA[gemma]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[llm]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Wed, 21 Feb 2024 18:04:07 GMT</pubDate>
            <atom:updated>2024-02-26T14:27:56.268Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UboIDVQkrG2Qy0mnW21vPw.png" /></figure><h3>Gemma just openedÂ ;)</h3><p><a href="https://blog.google/technology/developers/gemma-open-models"><strong>Gemma</strong></a> is a family of open, lightweight, and easy-to-use models developed by Google Deepmind. The Gemma models are built from the same research and technology used to createÂ Gemini.</p><p>This means that we (ML developers &amp; practitioners) now have additional versatile large language models (LLMs) in ourÂ toolbox!</p><p>If youâ€™d rather read code, you can jump straight to this Python notebook:<br>â†’ <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb">Finetune Gemma using KerasNLP and deploy to VertexÂ AI</a></p><h3>Availability</h3><p>Gemma is available today in Google Cloud and the machine learning (ML) ecosystem. Youâ€™ll probably find an ML platform youâ€™re already familiarÂ with:</p><ul><li>Gemma joins 130+ models in <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335">Vertex AI ModelÂ Garden</a></li><li>Gemma joins the <a href="https://www.kaggle.com/models/google/gemma">KaggleÂ Models</a></li><li>Gemma joins the <a href="http://huggingface.co/google">Hugging FaceÂ Models</a></li><li>Gemma joins <a href="https://ai.google.dev/gemma">Google AI for Developers</a></li></ul><p>Here are the <a href="https://www.kaggle.com/models/google/gemma/license/consent">Gemma Terms ofÂ Use</a>.</p><h3>Gemma models</h3><p>The Gemma family launches in two sizes, Gemma <strong>2B</strong> and <strong>7B</strong>, targeting two typical platformÂ types:</p><pre>| Model    | Parameters  | Platforms                           |<br>| -------- | ----------- | ----------------------------------- |<br>| Gemma 2B | 2.5 billion | Mobile devices and laptops          |<br>| Gemma 7B | 8.5 billion | Desktop computers and small servers |</pre><p>These models are text-to-text EnglishÂ LLMs:</p><ul><li><strong>Input</strong>: a text string, such as a question, a prompt, or a document.</li><li><strong>Output</strong>: generated English text, such as an answer or aÂ summary.</li></ul><p>They were trained on a diverse and massive dataset of <strong>8 trillion tokens</strong>, including web documents, source code, and mathematical text.</p><p>Each size is available in untuned and tuned versions:</p><ul><li><strong>Pretrained (untuned)</strong>: Models were trained on core training data, not using any specific tasks or instructions.</li><li><strong>Instruction-tuned</strong>: Models were additionally trained on human language interactions.</li></ul><p>That gives us four variants. As an example, here are the corresponding model IDs when usingÂ Keras:</p><pre>- `gemma_2b_en`<br>- `gemma_instruct_2b_en`<br>- `gemma_7b_en`<br>- `gemma_instruct_7b_en`</pre><h3>Use cases</h3><p>Google now offers two families of LLMs: Gemini and Gemma. Gemma is a complement to Gemini, is based on technologies developed for Gemini, and addresses different useÂ cases.</p><p>Examples of Gemini benefits:</p><ul><li><strong>Enterprise</strong> applications</li><li><strong>Multilingual</strong> tasks</li><li>Optimal <strong>qualitative results</strong> and <strong>complexÂ tasks</strong></li><li>State-of-the-art <strong>multimodality</strong> (text, image,Â video)</li><li><strong>Groundbreaking features</strong> (e.g. <a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-on-vertex-ai-expands">Gemini 1.5 Pro 1M token contextÂ window</a>)</li></ul><p>Examples of Gemma benefits:</p><ul><li><strong>Learning</strong>, <strong>research</strong>, or <strong>prototyping</strong> based on lightweight models</li><li>Focused tasks such as <strong>text generation</strong>, <strong>summarization</strong>, andÂ <strong>Q&amp;A</strong></li><li>Framework or cross-device <strong>interoperability</strong></li><li><strong>Offline</strong> or <strong>real-time</strong> text-only processings</li></ul><p>The Gemma model variants are useful in the following useÂ cases:</p><ul><li><strong>Pretrained (untuned)</strong>: Consider these models as a lightweight foundation and perform a custom finetuning to optimize them to your ownÂ needs.</li><li><strong>Instruction-tuned</strong>: You can use these models for conversational applications, such as a chatbot, or to customize them evenÂ further.</li></ul><h3>Interoperability</h3><p>As Gemma models are open, they are virtually interoperable with all ML platforms and frameworks.</p><p>For the launch, Gemma is supported by the following ML ecosystem players:</p><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/gemma-model-available-in-vertex-ai-and-via-gke">Google Cloud</a></li><li><a href="https://www.kaggle.com/models/google/gemma">Kaggle</a></li><li><a href="https://developers.googleblog.com/2024/02/gemma-models-in-keras.html">Keras</a>, which means that Gemma runs on <strong>JAX</strong>, <strong>PyTorch</strong>, and <strong>TensorFlow</strong></li><li><a href="https://huggingface.co/blog/gemma">Hugging Face</a></li><li><a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-revs-up-inference-for-google-gemma/">Nvidia</a></li></ul><p>And, of course, Gemma can <strong>run on GPUs andÂ TPUs</strong>.</p><h3>Requirements</h3><p>Gemma models are lightweight but, in the LLM world, this still means gigabytes.</p><p>In my tests, when running inferences in half precision (bfloat16), here are the minimum storage and GPU memory that were required:</p><pre>| Model    | Total parameters | Assets size | Min. GPU RAM to run |<br>| -------- | ---------------: | ----------: | ------------------: |<br>| Gemma 2B |    2,506,172,416 |     4.67 GB |              8.3 GB |<br>| Gemma 7B |    8,537,680,896 |    15.90 GB |             20.7 GB |</pre><p>To experiment with Gemma and Vertex AI, I used <strong>Colab Enterprise</strong> with a g2-standard-8 runtime, which comes with an <strong>NVIDIA L4</strong> GPU and 24 GB of GPU RAM. This is a cost-effective configuration to save time and avoid running out-of-memory when prototyping in a notebook.</p><h3>Finetuning Gemma</h3><p>Depending on your preferred frameworks, youâ€™ll find different ways to customize Gemma. Keras is one of them and provides everything youÂ need.</p><p>KerasNLP lets you load a Gemma model in a singleÂ line:</p><pre>import keras<br>import keras_nlp<br><br>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(&quot;gemma_instruct_2b_en&quot;)</pre><p>Then, you can directly generateÂ text:</p><pre>inputs = [<br>    &quot;What&#39;s the most famous painting by Monet?&quot;,<br>    &quot;Who engineered the Statue of Liberty?&quot;,<br>    &#39;Who were &quot;The LumiÃ¨res&quot;?&#39;,<br>]<br><br>for input in inputs:<br>    response = gemma_lm.generate(input, max_length=25)<br>    output = response[len(input) :]<br>    print(f&quot;{input!r}\n{output!r}\n&quot;)</pre><p>With the instruction-tuned version, Gemma gives you expected answers, as would a good LLM-based chatbot:</p><pre># With &quot;gemma_instruct_2b_en&quot;<br><br>&quot;What&#39;s the most famous painting by Monet?&quot;<br>&#39;\n\nThe most famous painting by Monet is &quot;Water Lilies,&quot; which&#39;<br><br>&#39;Who engineered the Statue of Liberty?&#39;<br>&#39;\n\nThe Statue of Liberty was built by French sculptor FrÃ©dÃ©ric Auguste Bartholdi between 1&#39;<br><br>&#39;Who were &quot;The LumiÃ¨res&quot;?&#39;<br>&#39;\n\nThe LumiÃ¨res were a group of French scientists and engineers who played a significant role&#39;</pre><p>Now, try the untunedÂ version:</p><pre>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(&quot;gemma_2b_en&quot;)</pre><p>Youâ€™ll get different results, which is expected as this version was not trained on any specific task and is designed to be finetuned to your ownÂ needs:</p><pre># With &quot;gemma_2b_en&quot;<br><br>&quot;What&#39;s the most famous painting by Monet?&quot;<br>&quot;\n\nWhat&#39;s the most famous painting by Van Gogh?\n\nWhat&quot;<br><br>&#39;Who engineered the Statue of Liberty?&#39;<br>&#39;\n\nA. George Washington\nB. Napoleon Bonaparte\nC. Robert Fulton\nD&#39;<br><br>&#39;Who were &quot;The LumiÃ¨res&quot;?&#39;<br>&#39; What did they invent?\n\nIn the following sentence, underline the correct modifier from the&#39;</pre><p>If youâ€™d like, for example, to build a Q&amp;A application, prompt engineering may fix some of these issues but, to be grounded on facts and return consistent results, untuned models need to be finetuned with trainingÂ data.</p><p>The Gemma models have billions of trainable parameters. The next step consists of using the <a href="https://arxiv.org/abs/2106.09685">Low Rank Adaptation</a> (LoRA) to greatly reduce the number of trainable parameters:</p><pre># Number of trainable parameters before enabling LoRA: 2.5B<br><br># Enable LoRA for the model and set the LoRA rank to 4<br>gemma_lm.backbone.enable_lora(rank=4)<br><br># Number of trainable parameters after enabling LoRA: 1.4M (1,800x less)</pre><p>A training can now be performed with reasonable time and GPU memory requirements.</p><p>For prototyping on a small training dataset, you can launch a local finetuning:</p><pre>training_data: list[str] = [...]<br><br># Reduce the input sequence length to limit memory usage<br>gemma_lm.preprocessor.sequence_length = 128<br><br># Use AdamW (a common optimizer for transformer models)<br>optimizer = keras.optimizers.AdamW(<br>    learning_rate=5e-5,<br>    weight_decay=0.01,<br>)<br><br># Exclude layernorm and bias terms from decay<br>optimizer.exclude_from_weight_decay(var_names=[&quot;bias&quot;, &quot;scale&quot;])<br><br>gemma_lm.compile(<br>    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),<br>    optimizer=optimizer,<br>    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],<br>)<br>gemma_lm.fit(training_data, epochs=1, batch_size=1)</pre><p>After a couple of minutes of training (on 1,000 examples) and using a structured prompt, the finetuned model can now answer questions based on your trainingÂ data:</p><pre># With &quot;gemma_2b_en&quot; before finetuning<br>&#39;Who were &quot;The LumiÃ¨res&quot;?&#39;<br>&#39; What did they invent?\n\nIn the following sentence, underline the correct modifier from the&#39;<br><br># With &quot;gemma_2b_en&quot; after finetuning<br>&quot;&quot;&quot;<br>Instruction:<br>Who were &quot;The LumiÃ¨res&quot;?<br><br>Response:<br>&quot;&quot;&quot;<br>&quot;The LumiÃ¨res were the inventors of the first motion picture camera. They were&quot;</pre><p>The prototyping stage is over. Before you proceed to finetuning models in production with large datasets, check out new specific tools to build a responsible Generative AI application.</p><h3>Responsible AI</h3><p>With great power comes great responsibility, right?</p><p>The <a href="https://ai.google.dev/responsible">Responsible Generative AI Toolkit</a> will help you build safer AI applications with Gemma. Youâ€™ll find expert guidance on safety strategies for the different aspects of your solution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W_SYa_3eYK4SxPbyLVrQvg.png" /></figure><p>Also, do not miss the <a href="https://pair-code.github.io/lit">Learning Interpretability Tool</a> (LIT) for visualizing and understanding the behavior of your Gemma models. Hereâ€™s the tool in action, investigating Gemmaâ€™s behavior:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/999/1*2qd_RFenWKZrm0t543Rosw.gif" /></figure><h3>Deploying Gemma</h3><p>Weâ€™re in <a href="https://cloud.google.com/vertex-ai/docs/start/introduction-mlops"><strong>MLOps</strong></a> territory here and youâ€™ll find different LLM-optimized serving frameworks running on GPUs orÂ TPUs.</p><p>Here are popular serving frameworks:</p><ul><li><a href="https://github.com/vllm-project/vllm">vLLM</a> (GPU)</li><li><a href="https://github.com/huggingface/text-generation-inference/blob/main/README.md">TGI: Text Generation Interface</a> (GPU)</li><li><a href="https://github.com/google/saxml">Saxml</a> (TPU orÂ GPU)</li><li><a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a> (NVIDIA TritonÂ GPU)</li></ul><p>These frameworks come with prebuilt container images that you can easily deploy to VertexÂ AI.</p><p>Here is a simple notebook to get youÂ started:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb">Finetune Gemma using KerasNLP and deploy to VertexÂ AI</a></li></ul><p>For production finetuning, you can use <a href="https://cloud.google.com/vertex-ai/docs/training/create-custom-job">Vertex AI custom training jobs</a>. Here are detailed notebooks:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb">Gemma Finetuning</a> (served by vLLM orÂ HexLLM)</li><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_gemma_peft_finetuning_hf.ipynb">Gemma Finetuning</a> (served byÂ TGI)</li></ul><p>Those notebooks focus on deployment andÂ serving:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_deployment_on_vertex.ipynb">Gemma Deployment</a> (served by vLLM orÂ HexLLM)</li><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_deployment_on_gke.ipynb">Gemma Deployment to GKE using TGI onÂ GPU</a></li></ul><p>For more details and updates, check out the documentation:</p><ul><li><a href="https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma">Vertex AI</a></li><li><a href="https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra">GKE</a></li></ul><h3>All the best to Gemini andÂ Gemma!</h3><p>After years of consolidation in machine learning hardware and software, itâ€™s exciting to see the pace at which the overall ML landscape now evolves and in particular with LLMs. LLMs are technologies still in their infancy, so we can expect to see more breakthroughs in the nearÂ future.</p><p>I very much look forward to seeing what the ML community is going to build withÂ Gemma!</p><p><strong>All the best to the Gemini and Gemma families!</strong></p><p><em>Follow me on </em><a href="https://twitter.com/PicardParis"><em>Twitter</em></a><em> or </em><a href="https://linkedin.com/in/PicardParis"><em>LinkedIn</em></a><em> for more cloud explorations</em>â€¦</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4b0fc2a14851" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/making-ai-more-open-and-accessible-to-cloud-developers-with-gemma-on-vertex-ai-4b0fc2a14851">Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Moderating text with the Natural Language API]]></title>
            <link>https://medium.com/google-cloud/moderating-text-with-the-natural-language-api-5d379727da2c?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/5d379727da2c</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[moderation]]></category>
            <category><![CDATA[nlp]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Fri, 16 Jun 2023 16:49:45 GMT</pubDate>
            <atom:updated>2023-09-12T09:40:16.984Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*k_L9ToCSSwqKeROq17wA5A.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/fr/@daninasplash">Da Nina</a> onÂ <a href="https://unsplash.com/photos/MBqwXZTfkdA">Unsplash</a></figcaption></figure><blockquote>2023â€“09â€“12: text moderation got Generally Available (GA) over the summer + added link to Sep. blogÂ post</blockquote><p>The <a href="https://cloud.google.com/natural-language/docs/">Natural Language API</a> lets you extract information from unstructured text using Google machine learning and provides a solution to the following problems:</p><ul><li>Sentiment analysis</li><li>Entity analysis</li><li>Entity sentiment analysis</li><li>Syntax analysis</li><li>Content classification</li><li><strong>Text moderation</strong></li></ul><h3>ğŸ” Moderation categories</h3><p>Text moderation lets you detect sensitive or harmful content. The first moderation category that comes to mind is â€œtoxicityâ€, but there can be many more topics of interest. A <a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model">PaLM 2</a>-based model powers the predictions and scores 16 categories:</p><pre>| ---------- | --------------------- | ----------------- | -------------- |<br>| Toxic      | Insult                | Public Safety     | War &amp; Conflict |<br>| Derogatory | Profanity             | Health            | Finance        |<br>| Violent    | Death, Harm &amp; Tragedy | Religion &amp; Belief | Politics       |<br>| Sexual     | Firearms &amp; Weapons    | Illicit Drugs     | Legal          |</pre><h3>âš¡ Moderating text</h3><p>Like always, you can call the API through the REST/RPC interfaces or with idiomatic client libraries.</p><p>Here is an example using the Python client library (<a href="https://cloud.google.com/python/docs/reference/language">google-cloud-language</a>) and the moderate_text method:</p><pre>from google.cloud import language<br><br>def moderate_text(text: str) -&gt; language.ModerateTextResponse:<br>    client = language.LanguageServiceClient()<br>    document = language.Document(<br>        content=text,<br>        type_=language.Document.Type.PLAIN_TEXT,<br>    )<br>    return client.moderate_text(document=document)<br><br>text = (<br>    &quot;I have to read Ulysses by James Joyce.\n&quot;<br>    &quot;I&#39;m a little over halfway through and I hate it.\n&quot;<br>    &quot;What a pile of garbage!&quot;<br>)<br>response = moderate_text(text)</pre><blockquote><em>ğŸš€ Itâ€™s fast! The model latency is very low, allowing real-time analyses.</em></blockquote><p>The response contains confidence scores for each moderation category. Letâ€™s sort themÂ out:</p><pre>import pandas as pd<br><br>def confidence(category: language.ClassificationCategory) -&gt; float:<br>    return category.confidence<br><br>columns = [&quot;category&quot;, &quot;confidence&quot;]<br>categories = sorted(<br>    response.moderation_categories,<br>    key=confidence,<br>    reverse=True,<br>)<br>data = ((category.name, category.confidence) for category in categories)<br>df = pd.DataFrame(columns=columns, data=data)<br><br>print(f&quot;Text analyzed:\n{text}\n&quot;)<br>print(f&quot;Moderation categories:\n{df}&quot;)</pre><p>You may typically ignore scores below 50% and calibrate your solution by defining upper limits (or buckets) for the confidence scores. In this example, depending on your thresholds, you may flag the text as disrespectful (toxic) and insulting:</p><pre>Text analyzed:<br>I have to read Ulysses by James Joyce.<br>I&#39;m a little over halfway through and I hate it.<br>What a pile of garbage!<br><br>Moderation categories:<br>                 category  confidence<br>0                   Toxic    0.680873<br>1                  Insult    0.609475<br>2               Profanity    0.482516<br>3                 Violent    0.333333<br>4                Politics    0.237705<br>5   Death, Harm &amp; Tragedy    0.189759<br>6                 Finance    0.176955<br>7       Religion &amp; Belief    0.151079<br>8                   Legal    0.100946<br>9                  Health    0.096305<br>10          Illicit Drugs    0.083333<br>11     Firearms &amp; Weapons    0.076923<br>12             Derogatory    0.073953<br>13         War &amp; Conflict    0.052632<br>14          Public Safety    0.051813<br>15                 Sexual    0.028222</pre><h3>ğŸ–– More</h3><ul><li>To try it, run this Colab notebook: <a href="https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb">Using the Natural LanguageÂ API</a></li><li>See the <a href="https://cloud.google.com/natural-language/docs/languages#text_moderation">supported languages</a></li><li>Read more about <a href="https://cloud.google.com/natural-language/docs/moderating-text">text moderation</a></li><li>See the latest blog post on <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-text-moderation">Improving Trust in AI and Online Communities</a></li><li>Follow me on <a href="https://twitter.com/PicardParis">Twitter</a> or <a href="https://linkedin.com/in/PicardParis">LinkedIn</a> for more cloud explorations</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5d379727da2c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/moderating-text-with-the-natural-language-api-5d379727da2c">Moderating text with the Natural Language API</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Better Way to Use Google Cloud from Colab]]></title>
            <link>https://medium.com/google-colab/a-better-way-to-use-google-cloud-from-colab-bb93f88b5021?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/bb93f88b5021</guid>
            <category><![CDATA[jupyter-notebook]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[google-colab]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[announcements]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Tue, 13 Jun 2023 18:23:54 GMT</pubDate>
            <atom:updated>2023-06-13T18:23:54.271Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SsBZTw8ePeH59hdH" /><figcaption>Photo by <a href="https://unsplash.com/@pablogamedev?utm_source=medium&amp;utm_medium=referral">Pablo Arroyo</a> onÂ <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>One of the best things about Colab is low friction. You open a notebook and Colab knows who you are based on your current Google accountâ€Šâ€”â€Šthereâ€™s no need to separately login to Colab (unless youâ€™re not already logged intoÂ Google).</p><p><strong>Problem</strong>: Google Cloud provides an amazingly powerful array of services, which work great in a Colab notebook, but you need to separately authenticate yourself to Google Cloud. This is not difficult but, till now, many developers resorted to using a service account, which adds a bit of complexity and, even worse, can lead to accidentally checking service account credentials into a repo, which is a serious securityÂ concern.</p><p><strong>Solution</strong>: We simplified the flow for you so that you can now use your Google Cloud project from a Colab notebook in a more straightforward way and hopefully without thinking about creating a serviceÂ account.</p><p><strong>How does it work?</strong> In order to understand whatâ€™s new, letâ€™s do a before-after comparison. Here is what can be found in some notebooks using a service account and downloading a privateÂ key:</p><pre># BEFORE<br>import os<br>from google.colab import auth<br>auth.authenticate_user()<br><br>PROJECT_ID = &quot;YOUR_PROJECT_ID&quot;<br>SA_NAME = &quot;YOUR_SERVICE_ACCOUNT_NAME&quot;<br>SA_EMAIL = f&quot;{SA_NAME}@{PROJECT_ID}.iam.gserviceaccount.com&quot;<br>!gcloud config set project $PROJECT_ID<br>!gcloud iam service-accounts create $SA_NAME<br>!gcloud iam service-accounts keys create ./key.json --iam-account $SA_EMAIL<br>os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;./key.json&quot;</pre><p>This is not necessary and there are two potential problemsÂ here:</p><ul><li>A service account is created. If youâ€™re not the owner of the project, you may not have the permissions forÂ this.</li><li>Service account credentials are manually downloaded at source code level. The private key can accidentally be pushed and madeÂ public.</li></ul><p>And hereâ€™s the simpler, service-account-less method:</p><pre># AFTER<br>from google.colab import auth<br>PROJECT_ID = &quot;YOUR_PROJECT_ID&quot;<br>auth.authenticate_user(project_id=PROJECT_ID)</pre><ul><li>You are authenticated for gcloud <a href="https://cloud.google.com/sdk/gcloud/reference">CLI commands</a> (likeÂ before).</li><li>You are also authenticated when using Google Cloud <a href="https://cloud.google.com/python/docs/reference">Python Client Libraries</a>.</li><li>Your default project isÂ set.</li></ul><p><strong>The new version is shorter, simpler, and less error-prone.</strong></p><blockquote><em>If youâ€™re new to Colab, would like to get more details or example notebooks, check out â€œ</em><a href="https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731?source=friends_link&amp;sk=1045f8cdd0b4b494fc2e1ef51edf1b6d"><em>Using Google Cloud fromÂ Colab</em></a><em>â€.</em></blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bb93f88b5021" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/a-better-way-to-use-google-cloud-from-colab-bb93f88b5021">A Better Way to Use Google Cloud from Colab</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Google Cloud from Colab]]></title>
            <link>https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/75691f4a731</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[google-colab]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Tue, 13 Jun 2023 16:31:33 GMT</pubDate>
            <atom:updated>2023-06-16T16:28:51.151Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="Screencast showing different steps of a notebook running in Colab" src="https://cdn-images-1.medium.com/max/1024/1*nLjcqvMkvZ5TRHF4pZjbHg.gif" /></figure><p>Google Colab is an amazing tool for Pythonistas. It can be used for a variety of tasks, from quickly experimenting with Python code to sharing extensive data processing notebooks with the world. Colab runs on Google Cloud, which gives you a boost when accessing cloud services because your code is running inside Googleâ€™s high performance network, and also offers a simple way to use Google Cloud services, letting you run powerful cloud workloads from yourÂ browser.</p><h3>ğŸ“¦ï¸ Install dependencies</h3><p>Colab comes with hundreds of preinstalled packages, including pandas, numpy, matplotlib, Flask, Pillow, tensorflow, and pytorch. You can install additional required dependencies asÂ needed.</p><p>For example, to analyze text with the power of machine learning using the <a href="https://cloud.google.com/natural-language/docs">Natural Language API</a>, install the google-cloud-language clientÂ library:</p><pre>!pip install google-cloud-language&gt;=2.9.1</pre><h3>ğŸ”‘ Authenticate</h3><p>To authenticate with your Google Cloud account within the Colab notebook, use the authenticate_user method. A new parameter lets you specify your projectÂ ID:</p><pre>from google.colab import auth<br><br>PROJECT_ID = &quot;&quot;  # @param {type:&quot;string&quot;}<br><br>auth.authenticate_user(project_id=PROJECT_ID)</pre><p>After thisÂ step:</p><ul><li>ğŸ‘ You are authenticated for gcloud <a href="https://cloud.google.com/sdk/gcloud/reference">CLI commands</a>.</li><li>ğŸ‘ You are also authenticated when using Google Cloud <a href="https://cloud.google.com/python/docs/reference">Python client libraries</a>. Note that <strong>you generally wonâ€™t need to create a serviceÂ account</strong>.</li><li>ğŸ‘ Your default project isÂ set.</li><li>ğŸ‘ Your notebook can also access your Google Drive, letting you ingest or generate your ownÂ files.</li></ul><h3>ğŸ”“ EnableÂ APIs</h3><p>Ensure required APIs are enabled. In our example, thatâ€™s the service language.googleapis.com:</p><pre>!gcloud services enable language.googleapis.com</pre><h3>ğŸ¤¯ Use Google CloudÂ services</h3><p>Thatâ€™s it! You can now directly use the service by calling its Python clientÂ library:</p><pre>from google.cloud import language<br><br>def analyze_text_sentiment(text: str) -&gt; language.AnalyzeSentimentResponse:<br>    client = language.LanguageServiceClient()<br>    document = language.Document(<br>        content=text,<br>        type_=language.Document.Type.PLAIN_TEXT,<br>    )<br>    return client.analyze_sentiment(document=document)<br><br># Input<br>text = &quot;Python is a very readable language, ...&quot;  # @param {type:&quot;string&quot;}<br><br># Send a request to the API<br>response = analyze_text_sentiment(text)<br><br># Use the results<br>print(response)</pre><h3>ğŸ“Š Benefit from notebook advancedÂ features</h3><p>Colab is hosting a Jupyter notebook. I personally depict Colab as the â€œ<em>Google Drive of notebooks</em>â€. As notebooks have additional superpowers, this lets you nicely process and visualize your data, such as tables, images, orÂ charts.</p><p>In our example, the function show_text_sentiment gathers results in a <em>pandas</em> DataFrame, which renders as aÂ table:</p><figure><img alt="Screencast showing successive text sentiment analyses with results displayed in a table" src="https://cdn-images-1.medium.com/max/1024/1*5WIc70V9irgXcGgSb_ayBw.gif" /></figure><h3>âœ¨ Check itÂ out</h3><p>In these examples, click the â€œ<em>Open in Colab</em>â€Â link:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20Google%20Cloud%20from%20Colab.ipynb">Using Google Cloud from Colab</a>Â (short)</li><li><a href="https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb">Using the Natural Language API with Python</a> (detailed)</li></ul><blockquote><em>ğŸ’¡ You can directly create a new notebook by opening the </em><a href="https://colab.new"><em>colab.new</em></a><em> URL.</em></blockquote><h3>ğŸ‘‹ HaveÂ fun!</h3><ul><li>Read the <a href="https://medium.com/google-colab">Google Colab</a> blog to learn more aboutÂ Colab.</li><li>Follow me (<a href="https://twitter.com/PicardParis">Twitter</a> / <a href="https://linkedin.com/in/PicardParis">LinkedIn</a>) for more cloud explorationsÂ ;)</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=75691f4a731" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731">Using Google Cloud from Colab</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From pixels to information with Document AI]]></title>
            <link>https://medium.com/google-cloud/from-pixels-to-information-with-document-ai-3476431c3010?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/3476431c3010</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[technology]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Wed, 15 Mar 2023 17:17:47 GMT</pubDate>
            <atom:updated>2023-03-15T17:17:47.561Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jj8xPtmBQlwTZ6K_qoiAEQ.png" /></figure><p>Weâ€™re seeing successively difficult problems getting solved thanks to machine learning (ML) models. For example, <a href="https://cloud.google.com/natural-language">Natural Language AI</a> and <a href="https://cloud.google.com/vision">Vision AI</a> extract insights from text and images, with human-like results. They solve problems central to the way we communicate:</p><ul><li>âœ… Natural language processing (NLP)</li><li>âœ… Optical character recognition (OCR)</li><li>âœ… Handwriting recognition (HWR)</li></ul><p>Whatâ€™s next? Well, itâ€™s already here, with <a href="https://cloud.google.com/document-ai">Document AI</a>, and keepsÂ growing:</p><ul><li>âœ… Document understanding (DU)</li></ul><p>Document AI builds on these foundations to let you extract information from documents, in manyÂ forms:</p><ul><li>Text</li><li>Structure</li><li>Semantic entities</li><li>Normalized values</li><li>Enrichments</li><li>Qualitative signals</li></ul><p>In this article, youâ€™ll see the following:</p><ul><li>An overview of DocumentÂ AI</li><li>Practical and visualÂ examples</li><li>A document processing demo (source code included)</li></ul><h3>Processing documents</h3><h4>Processor types</h4><p>There are as many document types as you can imagine so, to meet all needs, document processors are at the heart of Document AI. They can be of the following types:</p><ul><li><strong>General processors</strong> handle common tasks such as detecting document tables or parsingÂ forms.</li><li><strong>Specialized processors</strong> analyze specific documents such as invoices, receipts, pay slips, bank statements, identity documents, official forms,Â etc.</li><li><strong>Custom processors</strong> meet other specific needs, letting you automatically train private ML models based on your own documents, and perform tasks such as custom document classification or custom entity detection.</li></ul><h4>Processor locations</h4><p>When you create a processor, you specify its location. This helps control where the documents will be processed. Here are the current multi-region locations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*vf34kofWBNhim4b5uSHicA.png" /></figure><p>In addition, some processors are available in single-region locations; this lets you address local regulation requirements. If you regularly process documents in real-time or large batches of documents, this can also help get responses with even lower latencies. As an example, here are the current locations for the â€œDocument OCRâ€ general processor:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*08MKu92DdUiLOc4nIxP8hw.png" /></figure><p>Note: API endpoints are named according to the convention {location}-documentai.googleapis.com.</p><p>For more information, check out <a href="https://cloud.google.com/document-ai/docs/regions">Regional and multi-regional support</a>.</p><h4>Processor versions</h4><p>Processors can evolve over time, to offer more precise results, new features, or fixes. For example, here are the current versions available for the â€œExpense Parserâ€ specialized processor:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*5pGJPYaig4xPbKnc1K1t4g.png" /></figure><p>You may typically do the following:</p><ul><li>Use Stable version 1.3 in production.</li><li>Use Release Candidate version 1.4 to benefit from new features or test a futureÂ version.</li></ul><p>To stay updated, follow the <a href="https://cloud.google.com/document-ai/docs/release-notes">ReleaseÂ notes</a>.</p><h4>Interfaces</h4><p>Document AI is available to developers and practitioners through the usual interfaces:</p><ul><li><a href="https://cloud.google.com/document-ai/docs/reference/rest">REST API</a> (universal JSON-based interface)</li><li><a href="https://cloud.google.com/document-ai/docs/reference/rpc">RPC API</a> (low-latency gRPC interface, used by all Google services)</li><li><a href="https://cloud.google.com/document-ai/docs/quickstart-client-libraries#install_the_client_library">Client Libraries</a> (gRPC wrappers, to develop in your preferred programming language)</li><li><a href="https://console.cloud.google.com/ai/document-ai">Cloud Console</a> (web admin user interface)</li></ul><h4>Requests</h4><p>There are two ways you can process documents:</p><ul><li><strong>Synchronously</strong> with online requests, to analyze a single document and directly use theÂ results</li><li><strong>Asynchronously</strong> with batch requests, to launch a batch processing operation on multiple or larger documents</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*wj-Vap3gqQxdVYnEgJyH7w.png" /></figure><h3>Extracting information fromÂ pixels</h3><p>Letâ€™s start by analyzing this simple screenshot with the â€œDocument OCRâ€ general processor, and check how pixels become structured data.</p><p><strong>Input image</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*BVNXioR-3X9EYqgv.png" /></figure><p><strong>Document AIÂ response</strong></p><ul><li>The response contains a Document instance.</li><li>The entire text of the input, detected in natural reading order, is serialized under Document.text.</li><li>The structured data is returned on a per page basis (a single pageÂ here).</li><li>The â€œDocument OCRâ€ processor returns a structural skeleton common to all processors.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*YZll14-jI-lSUB-htfbE_w.png" /></figure><p>The examples in this article show snake-case field names (like mime_type), using the convention used by gRPC and programming languages like Python. For camel-case environments (like REST/JSON), there is a direct mapping between the two conventions:</p><ul><li>mime_type â†”Â mimeType</li><li>page_number â†” pageNumber</li><li>â€¦</li></ul><h3>Text levels</h3><p>For each page, four levels of text detection are returned:</p><ul><li>blocks</li><li>paragraphs</li><li>lines</li><li>tokens</li></ul><p>In these lists, each item exposes a layout including the item&#39;s position relative to the page in bounding_poly.normalized_vertices.</p><p>This lets us, for example, highlight the 22 detectedÂ tokens:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*A2GXdeserG9If_3v.gif" /></figure><p>Here is the lastÂ token:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*_vPlheSSbY2hqADWexImCQ.png" /></figure><p>Note: Float values are presented truncated for the sake of readability.</p><h3>Language support</h3><p>Documents are often written using one single language, but sometimes use multiple languages. You can retrieve the detected languages at different textÂ levels.</p><p>In our previous example, two blocks are detected ( pages[0].blocks[]). Let&#39;s highlight them:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*qcMudBjq4iYZOeSo.gif" /></figure><p>The left block is a mix of German, French, and English, while the right block is English only. Here is how the three languages are reported at the pageÂ level:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*F9luc3xz2V-HQAK90DBUaw.png" /></figure><p>Note: At this level, the language confidence ratios roughly correspond to the proportion of text detected in each language.</p><p>Now, letâ€™s highlight the five detected lines ( pages[0].lines[]):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*9MQdRYljnE7pyxH6.gif" /></figure><p>Each language is also reported at the lineÂ level:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*J3JpAXRPz8YvRMCNTP18uA.png" /></figure><p>If needed, you can get language info at the token level too. â€œQuestionâ€ is the same word in French and in English, and is adequately returned as an English token in thisÂ context:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*pnqEAxbUmMTutIJAZJ2Nmg.png" /></figure><p>In the screenshot, did you notice something peculiar in the leftÂ block?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*FhNl5BWWBb1DRzDj.gif" /></figure><p>Well, punctuation rules can be different between languages. French uses a typographical space for double punctuation marks (â€œdoubleâ€ as â€œwritten in two partsâ€, such as in &quot;!&quot;, &quot;?&quot;, &quot;&quot;&quot;,...). Punctuation is an important part of languages that can get &quot;lost in translation&quot;. Here, the space is preserved in the transcription of &quot;BienvenueÂ !&quot;. Nice touch Document AI or, should I say,Â touchÃ©!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*GoB_u-QIjw5v6kwdvnkKnw.png" /></figure><p>For more information, see <a href="https://cloud.google.com/document-ai/docs/languages">LanguageÂ support</a>.</p><h3>Handwriting detection</h3><p>Nowâ€Šâ€”â€Ša much harder problemâ€Šâ€”â€Šletâ€™s check how handwriting isÂ handled.</p><p>This example is a mix of printed and handwritten text, where I wrote both a question and an answer. Here are the detectedÂ tokens:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aNu-Z50vqkp0cBTy.gif" /></figure><p>I am pleasantly surprised to see my own handwriting transcribed:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*2fu6sl8TJ4KNRWpxStmfUg.png" /></figure><p>I asked my family (used to writing French) to do the same. Each unique handwriting sample also gets correctly detected:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*O_yCkRBxOKNxBMVA.gif" /></figure><p>â€¦ and transcribed:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*K9Gi_ZyVQ7eAuPjgJFM6Mw.png" /></figure><p>This can look magical but thatâ€™s one of the goals of ML models: return results as close as possible to human responses.</p><h3>Confidence scores</h3><p>We make mistakes, and so can ML models. To better appreciate the structured data you get, results include confidence scores:</p><ul><li>Confidence scores do not represent accuracy.</li><li>They represent how confident the model is with the extracted results.</li><li>They let youâ€Šâ€”â€Šand your usersâ€Šâ€”â€Šponder the modelâ€™s extractions.</li></ul><p>Letâ€™s overlay confidence scores on top of the previousÂ example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qKTXjDXeOnu77iIP.gif" /></figure><p>After grouping them in buckets, confidence scores appear to be generally high, with a few outliers:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/809/0*Qje3vHm13P04tlQ_.png" /></figure><p>The lowest confidence score here is 57%. It corresponds to a handwritten word (token) that is both short (less context given for confidence) and not particularly legibleÂ indeed:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*r80MhJ4EcgSFet63xuMD-w.png" /></figure><p>For best results, keep in mind these general rules ofÂ thumb:</p><ul><li>ML results are best guesses, which can be correct or incorrect.</li><li>Results with lower confidence scores are more likely to be incorrect guesses.</li><li>If we canâ€™t smoothly read a part of a document, or need to think twice about it, itâ€™s probably also harder for the MLÂ model.</li></ul><p>Although all text is correctly transcribed in the presented examples, this wonâ€™t always be the case depending on the input document. To build safer solutionsâ€Šâ€”â€Šespecially with critical business applicationsâ€Šâ€”â€Šyou may consider the following:</p><ul><li>Be clear with your users that results are auto-generated.</li><li>Communicate the scope and limitations of your solution.</li><li>Improve the user experience with interpretable information or filterable results.</li><li>Design your processes to trigger human intervention on lower confidence scores.</li><li>Monitor your solution to detect changes over time (drift of stable metrics, decline in user satisfaction, etc.).</li></ul><p>To learn more about AI principles and best practices, check out <a href="https://ai.google/responsibilities/responsible-ai-practices">Responsible AI practices</a>.</p><h3>Rotation, skew, distortion</h3><p>How many times did you scan a document upside down by mistake? Well, this shouldnâ€™t be a concern anymore. Text detection is very robust to rotation, skew, and other distortions.</p><p>In this example, the webcam input is not only upside down but also skewed, blurry, and with text in unusual orientations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X9bZmB1QIADOJWp1.jpg" /></figure><p>Before further analysis, Document AI considers the best reading orientation, at the page level, and preprocesses (deskews) each page if needed. This gives you results that can be used and visualized in a more natural way. Once processed by Document AI, the preceding example gets easier to read, without straining yourÂ neck:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OClQBbzZjDwd4182.gif" /></figure><p>In the results, each page has an image field by default. This represents the image - deskewed if needed - used by Document AI to extract information. All the page results and coordinates are relative to this image. When a page has been deskewed, a transforms element is present and contains the list of transformation matrices applied to theÂ image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*3bn1qDt-9cXmmbyM71On5Q.png" /></figure><p>Notes:</p><ul><li>Page images can be in different formats. For instance, if your input is a JPEG image, the response will include either the same JPEG or a deskewed PNG (as in the earlier example).</li><li>Deskewed images have larger dimensions (deskewing adds blank outerÂ areas).</li><li>If you donâ€™t need visual results in your solution, you can specify a field_mask in your request to receive lighter responses, with only your fields of interest.</li></ul><h3>Orientation</h3><p>Documents donâ€™t always have all of their text in a single orientation, in which case deskewing alone is not enough. In this example, the sentence is broken out in four different orientations. Each part gets properly recognized and processed in its natural orientation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hrnNBdXqI-FejECn.gif" /></figure><p>Orientations are reported in the layoutÂ field:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*0RhaOXSb8oXT1s_o99f4Sg.png" /></figure><p>Note: Orientations are returned at each OCR level ( blocks, paragraphs, lines, andÂ tokens).</p><h3>Noise</h3><p>When documents come from our analog world, you can expectÂ â€¦ the unexpected. As ML models are trained from real-world samplesâ€Šâ€”â€Šcontaining real-life noiseâ€Šâ€”â€Ša very interesting outcome is that Document AI is also significantly robust toÂ noise.</p><p>In this example with crumpled paper, the text starts to be difficult to read but still gets correctly transcribed by the OCRÂ model:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*P-SQC50hw62rcppH.gif" /></figure><p>Documents can also be dirty or stained. With the same sample, this keeps working after adding some layers ofÂ noise:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kH1AQmX_vGemiApv.gif" /></figure><p>In both cases, the exact same text is correctly detected:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*Mr9h8OrJ7fplwDx1ipDCww.png" /></figure><p>Youâ€™ve seen most core features. They are supported by the â€œDocument OCRâ€ general processor as well as the other processors, which leverage these features to focus on more specific document types and provide additional information. Letâ€™s check the next-level processor: the â€œForm Parserâ€ processor.</p><h3>Form fields</h3><p>The â€œForm Parserâ€ processor lets you detect form fields. A form field is the combination of a field name and a field value, also called a key-value pair.</p><p>In this example, printed and handwritten text is detected as seenÂ before:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*94XK4nbuOgLsFRUF.gif" /></figure><p>In addition, the form parser returns a list of form_fields:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*2jI2xxd09LPJbisXTzvqmQ.png" /></figure><p>Here is how the detected key-value pairs are returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*HSv255xZseQziHbCW72h5w.png" /></figure><p>And here are their detected boundingÂ boxes:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tCOInmMLqZwdOwqW.gif" /></figure><p>Note: Form fields can follow flexible layouts. In this example, keys and values are in a left-right order. Youâ€™ll see a right-left example next. Those are just simple arbitrary examples. It also works with vertical or free layouts where keys and values are logically (visually) related.</p><h3>Checkboxes</h3><p>The form parser also detects checkboxes. A checkbox is actually a particular form fieldÂ value.</p><p>This example is a French exam with affirmations that should be checked when exact. To test this, I used checkboxes of different kinds, printed or handmade. All form fields are detected, with the affirmations as field names and the corresponding checkboxes as fieldÂ values:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UJ_fx2dF22Ce1TW4.gif" /></figure><p>When a checkbox is detected, the form field contains an additional value_type field, which value is either unfilled_checkbox or filled_checkbox:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*UwE4Xvo8Pdy-H_qA7M6DVw.png" /></figure><p>Being able to analyze forms can lead to huge time savings, by consolidatingâ€Šâ€”â€Šor even autoprocessingâ€Šâ€”â€Šcontent for you. The preceding checkbox detection example was actually an evolution of a prior experiment to autocorrect my wifeâ€™s pile of exam copies. The proof of concept got better using checkboxes, but was already conclusive enough with True/False handwritten answers. Here is how it can autocorrect and autograde:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BEzESKpTcyWrdorH.gif" /></figure><h3>Tables</h3><p>The form parser can also detect another important structural element:Â tables.</p><p>In this example, words are presented in a tabular layout without any borders. The form parser finds a table very close to the (hidden) layout. Here are the detectedÂ cells:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fpueWsZxtFjy6MeE.gif" /></figure><p>In this other example, some cells are filled with text while others are blank. There are enough signals for the form parser to detect a tabular structure:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yDxJ4Zy33HcXOyj8.gif" /></figure><p>When tables are detected, the form parser returns a list of tables with their rows and cells. Here is how the table is returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*DwoO44-zi8jc3X6IWBa4-w.png" /></figure><p>And here is the firstÂ cell:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*yXZF0PBB6x5De_Fe4UEqxw.png" /></figure><h3>Specialized processors</h3><p>Specialized processors focus on domain-specific documents and extract <strong>entities</strong>. They cover many different document types that can currently be classified in the following families:</p><ul><li><strong>Procurement</strong>â€Šâ€”â€Šreceipts, invoices, utility bills, purchaseÂ orders,â€¦</li><li><strong>Lending</strong>â€Šâ€”â€Šbank statements, pay slips, officialÂ forms,â€¦</li><li><strong>Identity</strong>â€Šâ€”â€Šnational IDs, driver licenses, passports,â€¦</li><li><strong>Contract</strong>â€Šâ€”â€Šlegal agreements</li></ul><p>For example, procurement processors typically detect the total_amount and currency entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*FLNM6pGN0oRrzv_hzeEddg.png" /></figure><p>For more information, check out <a href="https://cloud.google.com/document-ai/docs/fields">Fields detected</a>.</p><h3>Expenses</h3><p>The â€œExpense Parserâ€ lets you process receipts of various types. Letâ€™s analyze this actual (French)Â receipt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9KGKV8_agwvJY3A7.gif" /></figure><p>A fewÂ remarks:</p><ul><li>All expected entities are extracted.</li><li>Ideally (to be picky), Iâ€™d like to get the tax rate too. If thereâ€™s customer demand for it, this may be a supported entity in a futureÂ version.</li><li>Due to the receiptâ€™s very thin paper, the text on the back side is visible by transparency (another type ofÂ noise).</li><li>The supplier name is wrong (itâ€™s actually mirrored text from the back side) but with a very low confidence (4%). Youâ€™d typically handle it with special care (itâ€™s shown differently here) or ignoreÂ it.</li><li>The actual supplier info is hidden (the top part of the receipt is folded) on purpose. Youâ€™ll see another example with supplier data a bitÂ later.</li><li>Receipts are often printed on single (sometimes on multiple) pages. The expense parser supports analyzing expense documents of up to 10Â pages.</li></ul><p>Procurement documents often list parts of the data in tabular layouts. Here, theyâ€™re returned as many line_item/* entities. When the entities are detected as part of a hierarchical structure, the results are nested in the properties field of a parent entity, providing an additional level of information. Here&#39;s anÂ excerpt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*_OdZdElj3-qYJHDOTMQw2A.png" /></figure><p>For more information, see the <a href="https://cloud.google.com/document-ai/docs/processors-list#processor_expense-parser">Expense Parser</a>Â details.</p><h3>Entity normalization</h3><p>Getting results is generally not enough. Results often need to be handled in a post-processing stage, which can be both time consuming and a source of errors. To address this, specialized processors also return normalized values when possible. This lets you directly use standard values consolidated from the context of the whole document.</p><p>Letâ€™s check it with this otherÂ receipt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kusJyoZehiKDL4M6.jpg" /></figure><p>First, the receipt currency is returned with its standard code under normalized_value:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*PPG-t22mNm8N_uAgfIFHww.png" /></figure><p>Then, the receipt is dated 11/12/2022. But is it Nov. 12 or Dec. 11? Document AI uses the context of the document (a French receipt) and provides a normalized value that removes all ambiguity:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*7Mth-IX8WGmLXKbvLR1HIg.png" /></figure><p>Likewise, the receipt contains a purchase time, written in a non-standard way. The result also includes a canonical value that avoids any interpretation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*Jyrs9cI4URn0upimWwtgVg.png" /></figure><p>Normalized values simplify the post-processing stage:</p><ul><li>They provide standard values that are straightforward to use (e.g. enable direct storage in a data warehouse).</li><li>They prevent bugs (esp. the recurring developer mistakes we make when converting data).</li><li>They remove ambiguity and avoid incorrect interpretations by using the context of the whole document.</li></ul><p>For more information, check out the <a href="https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#NormalizedValue">NormalizedValue</a> structure.</p><h3>Entity enrichment</h3><p>Did you notice there was more information in theÂ receipt?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CxkHjnhkR9wrOUK_.jpg" /></figure><ul><li><em>â€œMaison Jeanne dâ€™Arc, Place de Gaulleâ€</em> mentions a Joan of Arcâ€™s House and a place name. Nonetheless, there is no address, zip code, city, or evenÂ country.</li><li>This receipt comes most likely from a museum in France, but Joan of Arc lived in a few places (starting with her birthplace in the DomrÃ©my village).</li><li>So, which location does this correspond to?</li><li>A manual search should give hints to investigate, but can we keep this automated?</li></ul><p>Extracting the information behind the data requires extra knowledge or human investigation. An automated solution would generally ignore this partial data, but Document AI handles this in a unique way. To understand the worldâ€™s information, Google has been consistently analyzing the web for over 20 years. The result is a gigantic up-to-date knowledge base called the Knowledge Graph. Document AI leverages this knowledge graph to normalize and enrich entities.</p><p>First, the supplier is correctly detected and normalized with its usualÂ name:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*h6gfiTACcy8mLQD0XqVyGQ.png" /></figure><p>Then, the supplier city is also returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*HjNW6mcFw-CoKWlhvnD8WA.png" /></figure><p>Note: Joan of Arc spent some time in OrlÃ©ans in 1429, as she led the liberation of the besieged city at the age of 17 (but thatâ€™s anotherÂ story).</p><p>And finally, the complete and canonical supplier address is also part of the results, closing our caseÂ here:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*1paAmdvSYvQDQKYDgFRn6Q.png" /></figure><p>Enriched entities bring significant value:</p><ul><li>They are canonical results, which avoids information conflicts and discrepancies.</li><li>They can add information, which prevents ignoring usefulÂ data.</li><li>They can complete or fix partially correctÂ data.</li><li>In a nutshell, they provide information that is reliable, consistent, and comparable.</li></ul><p>Here is a recap of the expectedâ€Šâ€”â€Šas well as the non-obviousâ€Šâ€”â€Šentities detected in theÂ receipt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*STXit5_dgb8JVQ_5.jpg" /></figure><p>Note: The slightest characteristic element can be sufficient to detect an entity. Iâ€™ve been regularly surprised to get entities I wasnâ€™t expecting, to eventually realize I had missed clues in the document. For example, I recently wondered how the expense processor was able to identify a specific store. The receipt only specified a zip code and the retail chain has several stores in my neighborhood. Well, a public phone number (hidden in the footer) was enough to uniquely identify the store in question and provide its fullÂ address.</p><p>To learn more about the Knowledge Graph and possible enrichments, check out <a href="https://cloud.google.com/document-ai/docs/ekg-enrichment">Enrichment and normalization</a>.</p><h3>Invoices</h3><p>Invoices are the most elaborate type of procurement documents, often spreading over multipleÂ pages.</p><p>Hereâ€™s an example showing entities extracted by the â€œInvoiceÂ Parserâ€:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*31ffw3l1v8SEB03k.gif" /></figure><p>A fewÂ remarks:</p><ul><li>The relevant information is extracted (even the supplier tax ID is detected on the 90Â°-rotated leftÂ side).</li><li>As seen earlier with the expense parser, the invoice parser extracts many line_item/* entities but also supplier_* and receiver_* info.</li><li>This is a typical invoice (issued by the historical French energy provider) with many numbers on the firstÂ pages.</li><li>The original source is a PDF (digital) that was anonymized, printed, stained, and scanned as a PDF (raster).</li></ul><p>For more information, see the <a href="https://cloud.google.com/document-ai/docs/processors-list#processor_invoice-processor">Invoice Parser</a>Â details.</p><h3>Barcodes</h3><p>Barcode detection is enabled for some processors. In this example, the invoice parser detects the barcode (a manifestÂ number):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*goBikGmCIaE_7O5Y.gif" /></figure><p>Note: I didnâ€™t have an invoice with barcodes at hand, so I used a (slightly anonymized) packingÂ list.</p><p>Barcodes are returned, at the page level, likeÂ this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*acC_Pwcy2q4ml9rmoZtf4Q.png" /></figure><p>For more information, check out the <a href="https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#detectedbarcode">DetectedBarcode</a> structure.</p><h3>Identity documents</h3><p>Identity processors let you extract identity entities. In this US passport specimen (credit: <a href="https://twitter.com/travelgov/status/1537072860346318849">Bureau of Consular Affairs</a>), the expected fields can be automatically extracted:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BOEVjAwzrxgeNjNP.gif" /></figure><p>For more details about identity processors, you can read my previous article <a href="https://cloud.google.com/blog/topics/developers-practitioners/automate-identity-document-processing-document-ai">Automate identity document processing</a>.</p><h3>Document signals</h3><p>Some processors return document signalsâ€Šâ€”â€Šinformation relative to the documentÂ itself.</p><p>For example, the â€œDocument OCRâ€ processor returns quality scores for the document, estimating the defects that might impact the accuracy of theÂ results.</p><p>The preceding crumpled paper example gets a high quality score of 95%, with glare as a potential defect:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1W1bFYhuI8VejI1G.jpg" /></figure><p>The same example, at a 4x lower resolution, gets a lower quality score of 53%, with blurriness detected as the main potential issue:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BKBsDLI8Nms2cm4A.jpg" /></figure><p>Some additional remarks:</p><ul><li>A low score lets you flag documents that may need a manual review (or a new better capture).</li><li>Surprisingly, the exact same text is correctly extracted in both cases (I checked twice) but that might not always be theÂ case.</li><li>The quality scores are returned both as entities (with a quality_score parent entity) and in the image_quality_scores field at the page level (see <a href="https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#imagequalityscores">ImageQualityScores</a>).</li><li>This was tested with release candidate pretrained-ocr-v1.1-2022-09-12.</li></ul><p>Hereâ€™s how the image quality scores are returned at the pageÂ level:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*3ayVT7oPZ1It2EAJBQWwkg.png" /></figure><p>Likewise, the â€œIdentity Proofingâ€ processor gives you signals about the validity of ID documents.</p><p>First, this can help detect whether documents look like IDs. This preceding example is a random document analyzed as NOT_AN_ID:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*K07HHYahhA-5YlDH.jpg" /></figure><p>Here are the corresponding entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*CCXyY2lYJzn6wJlPQ-4ogA.png" /></figure><p>The preceding passport example does get detected as an ID but also triggers useful fraudÂ signals:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EGtPKZlJIdYIi0xO.gif" /></figure><p>The results include evidences that #1 itâ€™s a specimen, #2 which can be foundÂ online:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*1SQiOGsvSBJ4a4W4YP6evA.png" /></figure><p>As new use cases appear, itâ€™s likely that some processors will extract new document signals. Follow the <a href="https://cloud.google.com/document-ai/docs/release-notes">Release notes</a> to stayÂ updated.</p><h3>Pre-trained processors</h3><p>Document AI is already huge and keeps evolving. Here is a screencast showing the current processor gallery and how to create a processor from the CloudÂ Console:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SaQV-rhnbBI8mBqB.gif" /></figure><h3>Custom processors with Workbench</h3><p>If you have your own business-specific documents, you may wish to extract custom entities not covered by the existing processors. Document AI Workbench can help you solve this by creating your own custom processor, trained with your own documents, in twoÂ ways:</p><ul><li>With <strong>uptraining</strong>, you can <strong>extend an existing processor</strong>.</li><li>You can also create a <strong>new processor fromÂ scratch</strong>.</li></ul><p>For more information, watch this great introduction video made by my teammateÂ Holt:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FBkE69dM0G-w%3Flist%3DPLIivdWyY5sqIR88BxIK-3w14Vm-jTH1id&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DBkE69dM0G-w&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FBkE69dM0G-w%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/de47c3c4fffcd14ff4cbc0f2a569fc8e/href">https://medium.com/media/de47c3c4fffcd14ff4cbc0f2a569fc8e/href</a></iframe><h3>Performance</h3><p>To capture your documents, youâ€™ll probably useÂ either:</p><ul><li>A scanner</li><li>A camera</li></ul><p>With scanners, youâ€™ll need to choose a resolution in dots per inch (dpi). To optimize the performance (especially the accuracy and consistency of the results), you can keep in mind the following:</p><ul><li>300 dpi is often a niceÂ spot</li><li>200 dpi is generally a hardÂ minimum</li></ul><p>Here are the resolutions needed to capture a sheet ofÂ paper:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gmmWWHQNbc2fK_IX.png" /></figure><p>Note: Dimensions are presented in the horizontal orientation (image sensors generally have a landscape native resolution).</p><p>With cameras, the captured dots per inch depend on the camera resolution but also on the way you zoom on the document. Hereâ€™s an example with an A4 sheet ofÂ paper:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6bmm-VT6DzFHx_vO.png" /></figure><p>This translates into different dpi ranges. Here are indicative values:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*A40rNOXGuWPL2YSO.png" /></figure><p>To give you an idea, here is how a PNG image size evolves when capturing the same document at different resolutions. The total number of pixels is a surface. Though the PNG compression slightly limits the growth, the size increases almost quadratically:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7iDRnwpnuJz3cUez.png" /></figure><p>A few general observations:</p><ul><li>Document captures are usually in the 100â€“600 dpi range, depending on the useÂ case.</li><li>100â€“200 dpi may be enough for some specific end user solutions, for example to process small receipts captured from mobile phone cameras or laptopÂ webcams.</li><li>Scans of 300â€“600 dpi may provide better results but will generate larger files and make your processing pipelineÂ slower.</li><li>Scans above 600 dpi are less likely. They are useful for very small documents (such as slides or negative films) or documents that need to be upscaled (e.g. to print aÂ poster).</li><li>If your use case involves documents with small text, barcodes, or very noisy (e.g. blurry) inputs, youâ€™ll typically need to compensate with a higher capture resolution.</li><li>Capture devices have noise intrinsic to the technology they use. They generally have options that will produce different results. Also, image sensors are more noisy in low-light environments.</li><li>Every conversion step in your architecture is likely to introduce side effects and a potential loss of information.</li><li>Lossy image formats (such as JPEG) can nicely reduce image sizes. On the other hand, too high compression levels (a setting) can generate artifacts with a negative impact on the detection performance (causing a strong degradation of image quality and consequently of result accuracy).</li><li>Lossless formats (such as PNG) imply larger file sizes but may bring the benefit of using lower resolutions. They may also be more future-proof (e.g. for archival) by preserving the possibility of later conversions to other formats or to smaller resolutions without losing information.</li><li>Storage costs are rather low, so it may also be interesting to consider higher acceptable scanning resolutions, to optimize both the performance and the overall cost of your solution.</li><li>In a nutshell, choosing capture devices, resolutions, and formats depends on your use case and is a tradeoff between accuracy, speed, andÂ cost.</li></ul><p>To finetune your solution and get more accurate, faster, or consistent results, you may consider the following:</p><ul><li>Check the performance of your solution with worst-case documents at different resolutions.</li><li>Have a clear understanding of the different formats used in your processing pipeline.</li><li>Test your solution with actual documents (as close as possible to what youâ€™ll have in production). Also, as the ML models are trained on real documents, you may get better results when testing real documents.</li><li>Keep a lossless format upstream when possible; documents can be converted to a lossy format downstream.</li><li>Evaluate whether capturing at a higher resolution and then downscaling produces betterÂ results.</li><li>When using lossy formats, finetune the compression level. There is usually a threshold at which compression artifacts become negligible while maintaining a good compression ratio.</li><li>When using cameras, control your lighting environment and avoid low-light conditions.</li><li>Make your users aware that blurry captures may decrease the performance.</li></ul><p>For example:</p><ul><li>The examples in this article can all be analyzed using cameras with resolutions ranging from 100 to 220 dpi. However, results wonâ€™t always be consistent with low resolutions.</li><li>For practical reasons, I mostly use laptop, mobile phone, or dedicated cameras. There are excellent ones (called document cameras) that can sit on your desk and provide solid captures with a fast auto-focus.</li><li>For an enterprise-grade solution, Iâ€™d probably stick to 300 dpi scans. Multifunction printers generally have scanners that provide excellent PNG, PDF, or TIFF captures.</li><li>The barcode (contained in the preceding packing list sample) starts to be detected at 150 dpi with a scanner, and at 200 dpi with a webcam. After being photocopied (i.e. scanned + printed), itâ€™s still detected at 150 dpi with a scanner but needs to be zoomed in with theÂ webcam.</li><li>While capturing documents from a browser and a webcam, I checked the HTML canvas default format: itâ€™s PNG (great!) but in RGBA (with a transparency channel, unnecessary for document images). This made the data larger without any benefit. The optimization was a parameter away (canvas.getContext(&#39;2d&#39;) â†’ canvas.getContext(&#39;2d&#39;, { alpha: false })) and made the captures sent to the backend 9 to 18%Â smaller.</li></ul><h3>Sample demo</h3><p>I put myself in your shoes to see what it takes to build a document processing prototype and made the following choices:</p><ul><li>A frontend using vanilla JavaScript, responsible for managing user interactions and input documents</li><li>A backend using the Python client library, responsible for all document processings and renderings</li><li>Synchronous calls to Document AI, to process documents in real-time</li><li>No cloud storage, to avoid storing personally identifiable information (analyses are stored client-side)</li></ul><p>Here is the chosen software stack based on open-source Python projects:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WDZhCqNnLdYOPh7b.png" /></figure><p>â€¦ and one possible architecture to deploy it to production using CloudÂ Run:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kE8SL9syaPbfSkMm.png" /></figure><p>Here is the core function I use to process a document live inÂ Python:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*LyMVoCHRpo-xFR1T0Vy_kg.png" /></figure><p>To make sure that â€œwhat you see is what you getâ€, the sample images and animations in this article have been generated by theÂ demo:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*N3Gk5aQe91lR4owf.gif" /></figure><p>Note: To avoid making repeated and unnecessary calls to Document AI while developing the app, sample document analyses are cached (JSON serialization). This lets you check the returned structured documents and also explains why responses are immediate in this (real-time, not sped up) screencast.</p><p>It takes seconds to analyze a document. Here is an example with the user uploading a PDFÂ scan:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qRuSmdeL3ZPt5wWq.gif" /></figure><p>You can also take camera captures from the webÂ app:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xe3R6pX9-NOI6mkg.gif" /></figure><p>Check out the <a href="https://github.com/GoogleCloudPlatform/document-ai-samples/tree/main/web-app-pix2info-python">source code</a> and feel free to reuse it. Youâ€™ll also find instructions to deploy it as a serverless app.</p><h3>More?</h3><ul><li>Test documents? â†’ Try <a href="https://cloud.google.com/document-ai/docs/drag-and-drop">Document AI in yourÂ browser</a>.</li><li>Video series? â†’ Watch <a href="https://goo.gle/FutureOfDocuments">The future of documents</a>.</li><li>New to Google Cloud? â†’ Check out the <a href="https://cloud.google.com/free/docs/free-cloud-features">$300 free trialÂ offer</a>.</li><li>Learn and grow? â†’ Become a <a href="https://cloud.google.com/innovators">Google Cloud Innovator</a>.</li><li>Feedback or questions? â†’ Reach out on Twitter (<a href="https://twitter.com/PicardParis">@PicardParis</a>) or LinkedIn (<a href="https://linkedin.com/in/PicardParis">in/PicardParis</a>).</li></ul><p><em>Originally published onÂ </em><a href="https://github.com/GoogleCloudPlatform/document-ai-samples/blob/main/web-app-pix2info-python/README.md"><em>GitHub</em></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3476431c3010" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/from-pixels-to-information-with-document-ai-3476431c3010">From pixels to information with Document AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automate identity document processing with Document AI]]></title>
            <link>https://medium.com/google-cloud/automate-identity-document-processing-with-document-ai-912e1011e164?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/912e1011e164</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[document-management]]></category>
            <category><![CDATA[programming]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Thu, 02 Jun 2022 12:32:10 GMT</pubDate>
            <atom:updated>2022-06-02T12:34:24.591Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FYi9mbiyo0ldGxOwtQEAAw.png" /></figure><p>How many times have you filled out forms requesting personal information? Itâ€™s probably too many times to count. When online and signed in, you can save a lot of time thanks to your browserâ€™s autofill feature. In other cases, you often have to provide the same data manually, again and again. The first Document AI identity processors are now generally available and can help you solve thisÂ problem.</p><p>In this post, youâ€™ll see howÂ toâ€¦</p><ul><li>Process identity documents with DocumentÂ AI</li><li>Create your own identity form autofiller</li></ul><h3>Use cases</h3><p>Here are a few situations that youâ€™ve probably encountered:</p><ul><li><strong>Financial accounts</strong>: Companies need to validate the identity of individuals. When creating a customer account, you need to present a government-issued ID for manual validation.</li><li><strong>Transportation networks</strong>: To handle subscriptions, operators often manage fleets of custom identity-like cards. These cards are used for in-person validation, and they require an IDÂ photo.</li><li><strong>Identity gates</strong>: When crossing a border (or even when flying domestically), you need to pass an identity check. The main gates have streamlined processes and are generally well equipped to scale with the traffic. On the contrary, smaller gates along borders can have manual processesâ€Šâ€”â€Šsometimes on the way in and the way outâ€Šâ€”â€Šwhich can lead to long lines andÂ delays.</li><li><strong>Hotels</strong>: When traveling abroad and checking in, you often need to show your passport for a scan. Sometimes, you also need to fill out a longer paper form and write down the sameÂ data.</li><li><strong>Customer benefits</strong>: For benefit certificates or loyalty cards, you generally have to provide personal info, which can include a portraitÂ photo.</li></ul><p>In these examples, the requested infoâ€Šâ€”â€Šincluding the portrait photoâ€Šâ€”â€Šis already on your identity document. Moreover, an official authority has already validated it. Checking or retrieving the data directly from this source of truth would not only make processes faster and more effective, but also remove a lot of friction for endÂ users.</p><h3>Identity processors</h3><h4>Processor types</h4><p>Each Document AI identity processor is a machine learning model trained to extract information from a standard ID document suchÂ as:</p><ul><li>Driver license</li><li>National ID</li><li>Passport</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Frzqp_Q52oefAtYkaOOgpA.png" /></figure><blockquote>Note: an ID can have information on both sides, so identity processors support up to two pages per document.</blockquote><h4>Availability</h4><p><strong>Generally available</strong> as of June 2022, you can use two US identity processors in production:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*cobi6drwCVZcnVtKuZXHMQ.png" /></figure><p>Currently available inÂ <strong>Preview</strong>:</p><ul><li>The Identity Doc Fraud Detector, to check whether an ID document has been tamperedÂ with</li><li>Three French identity processors</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*TEZeOERZ2sXsWcy_8pTA4w.png" /></figure><p>Notes:</p><ul><li>More identity processors are in theÂ pipe.</li><li>To request access to processors in Preview, please fill out the <a href="https://docs.google.com/forms/d/e/1FAIpQLSc_6s8jsHLZWWE0aSX0bdmk24XDoPiE_oq5enDApLcp1VKJ-Q/viewform">Access RequestÂ Form</a>.</li></ul><h4>Processor creation</h4><p>You can create a processor:</p><ul><li><strong>Manually</strong> from Cloud Console (web adminÂ UI)</li><li><strong>Programmatically</strong> with theÂ API</li></ul><p>Processors are location-based. This helps guarantee where processing will occur for each processor.</p><p>Here are the current multi-region locations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*gTsVHyU5McPy9QArRU3oiQ.png" /></figure><p>Once youâ€™ve created a processor, you reference it with its ID (PROCESSOR_ID hereafter).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AjqSo2y2c7gpgCz3IbgUxQ.gif" /></figure><blockquote>Note: To manage processors programmatically, see the codelab <a href="https://codelabs.developers.google.com/codelabs/cloud-documentai-manage-processors-python">Managing Document AI processors withÂ Python</a>.</blockquote><h3>Document processing</h3><p>You can process documents in twoÂ ways:</p><ul><li>Synchronously with an <strong>online request</strong>, to analyze a single document and directly use theÂ results</li><li>Asynchronously with a <strong>batch request</strong>, to launch a batch processing operation on multiple or larger documents</li></ul><h4>Online requests</h4><p>Example of a REST onlineÂ request:</p><ul><li>The method is namedÂ process.</li><li>The input document here is a PNG image (base64 encoded).</li><li>This request is processed in the EuropeanÂ Union.</li><li>The response is returned synchronously.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*laxgKwcnPcXYA8o65Olx8g.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*HVJXyLrE-xvWK_dGkAvGdA.png" /></figure><h4>Batch requests</h4><p>Example of a REST batchÂ request:</p><ul><li>The method is named batchProcess.</li><li>The batchProcess method launches the batch processing of multiple documents.</li><li>This request is processed in the UnitedÂ States.</li><li>The response is returned asynchronously; output files will be stored under my-storage-bucket/output/.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*DSKr6L1XLzUKOR0PutZ4wg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Qn5Aa9y1LICaRCwevsnCXQ.png" /></figure><h4>Interfaces</h4><p>Document AI is available through the usual Google Cloud interfaces:</p><ul><li>The RPC API (low-latency gRPC)</li><li>The REST API (JSON requests and responses)</li><li>Client libraries (gRPC wrappers, currently available for Python, Node.js, andÂ Java)</li><li>Cloud Console (web adminÂ UI)</li></ul><blockquote>Note: With the client libraries, you can develop in your preferred programming language. Youâ€™ll see an example later in thisÂ post.</blockquote><h3>Identity fields</h3><p>A typical REST response looks like the following:</p><ul><li>The text and pages fields include the OCR data detected by the underlying ML models. This part is common to all Document AI processors.</li><li>The entities list contains the fields specifically detected by the identity processor.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Nqsohxiz_Y8xisRR2AcFkA.png" /></figure><p>Here are the detectable identityÂ fields:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*53J9QMAGacEIaqhQPM0m7w.png" /></figure><p>Please note that Address and MRZ Code are optional fields. For example, a US passport contains an MRZ but noÂ address.</p><h3>Fraud detection</h3><p>Available in preview, the <strong>Identity Doc Fraud Detector</strong> helps detect tampering attempts. Typically, when an identity document does not â€œpassâ€ the fraud detector, your automated process can block the attempt or trigger a human validation.</p><p>Here is an example of signals returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*k0ylMb9Ghac5MUO1tZCizQ.png" /></figure><h3>Sample demo</h3><p>You can process a document live with just a few lines ofÂ code.</p><p>Here is a PythonÂ example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*rCVvAfK1kp69IItzZZPVvg.png" /></figure><p>This function uses the Python clientÂ library:</p><ul><li>The input is a file (any format supported by the processor).</li><li>client is an API wrapper (configured for processing to take place in the desired location).</li><li>process_document calls the API process method, which returns results inÂ seconds.</li><li>The output is a structured Document.</li></ul><p>You can collect the detected fields by parsing the document entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*LiBNpP9sCXGUDO7OkCbjtw.png" /></figure><blockquote>Note: This function builds a mapping ready to be sent to a frontend. A similar function can be used for other specialized processors.</blockquote><p>Finalize yourÂ app:</p><ul><li>Define your user experience and architecture</li><li>Implement your backend and itsÂ API</li><li>Implement your frontend with a mix of HTML + CSS +Â JS</li><li>Add a couple of features: file uploads, document samples, or webcamÂ captures</li><li>Thatâ€™s it; youâ€™ve built an identity form autofiller</li></ul><p>Here is a sample web app inÂ action:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/546/1*yGnCNNuPUTYAvEBjRW40Xw.gif" /></figure><p>Here is the processing of a French national ID, dropping images from theÂ client:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/676/1*r3x3nkr3BJuuW3zxmUITlw.gif" /></figure><blockquote>Note: For documents with multiple pages, you can use a PDF or TIFF container. In this example, the two uploaded PNG images are merged by the backend and processed as a TIFFÂ file.</blockquote><p>And this is the processing of a US driver license, captured with a laptop 720pÂ webcam:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/546/1*_bnJS9-LyllaW1Qb6JxCEw.gif" /></figure><p>Notes:</p><ul><li>Did you notice that the webcam capture is skewed and the detected portrait image straight? Thatâ€™s because Document AI automatically deskews the input at the page level. Documents can even be upsideÂ down.</li><li>Some fields (such as the dates) are returned with their normalized values. This can make storing and processing these values a lot easierâ€Šâ€”â€Šand less error-proneâ€Šâ€”â€Šfor developers.</li></ul><p>The source code for this demo is available in our <a href="https://github.com/GoogleCloudPlatform/document-ai-samples/tree/main/community/identity-form-autofiller-python">Document AI sample repository</a>.</p><h3>More</h3><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-extends-document-ai-with-new-parsers-for-identity">Check out the official announcement</a></li><li><a href="https://cloud.google.com/document-ai/docs/drag-and-drop">Try Document AI in yourÂ browser</a></li><li><a href="https://cloud.google.com/document-ai/docs">Document AI documentation</a></li><li><a href="https://cloud.google.com/document-ai/docs/how-to">Document AI how-toÂ guides</a></li><li><a href="https://cloud.google.com/document-ai/docs/send-request">Sending a processing request</a></li><li><a href="https://cloud.google.com/document-ai/docs/processors-list">Full processor and detailÂ list</a></li><li><a href="https://cloud.google.com/document-ai/docs/release-notes">Release notes</a></li><li><a href="https://codelabs.developers.google.com/codelabs/docai-specialized-processors-python">Codelabâ€Šâ€”â€ŠSpecialized processors with DocumentÂ AI</a></li><li><a href="https://github.com/GoogleCloudPlatform/document-ai-samples">Codeâ€Šâ€”â€ŠDocument AIÂ samples</a></li><li>For more cloud content, follow me on Twitter (<a href="https://twitter.com/PicardParis">@PicardParis</a>) or LinkedIn (<a href="https://linkedin.com/in/PicardParis">in/PicardParis</a>), and feel free to get in touch with any feedback or questions.</li></ul><p>Stay tuned; the family of Document AI processors keeps growing andÂ growing.</p><p><em>Originally published on the </em><a href="https://cloud.google.com/blog/topics/developers-practitioners/automate-identity-document-processing-document-ai"><em>Google Cloud Blog</em></a><em> on June 1,Â 2022.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=912e1011e164" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/automate-identity-document-processing-with-document-ai-912e1011e164">Automate identity document processing with Document AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Deploy a coloring page generator in minutes with Cloud Run]]></title>
            <link>https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/bff59e59d890</guid>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[image-processing]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Thu, 07 Apr 2022 07:56:59 GMT</pubDate>
            <atom:updated>2022-04-07T10:18:20.736Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="Coloring page generated from Hokusaiâ€™s painting â€œThe Great Wave off Kanagawaâ€" src="https://cdn-images-1.medium.com/max/902/1*A0eoGye0p45FcR8oimVR3g.gif" /></figure><h3>ğŸ‘‹ Hello</h3><p>Have you ever written a script to transform an image? Did you share the script with others or did you run it on multiple computers? How many times did you need to update the script or the setup instructions? Did you end up making it a service or an online app? If your script is useful, youâ€™ll likely want to make it available to others. Deploying processing services is a recurring needâ€Šâ€”â€Šone that comes with its own set of challenges. Serverless technologies let you solve these challenges easily and efficiently.</p><p>In this post, youâ€™ll see howÂ toâ€¦</p><ul><li>Create an image processing service that generates coloringÂ pages</li><li>Make it available online using minimal resources</li></ul><p>â€¦and do it all in less than 200 lines of Python and JavaScript!</p><h3>ğŸ› ï¸ Tools</h3><p>To build and deploy a coloring page generator, youâ€™ll need a fewÂ tools:</p><ul><li>A library to processÂ images</li><li>A web application framework</li><li>A webÂ server</li><li>A serverless solution to make the demo available 24/7</li></ul><h3>ğŸ§± Architecture</h3><p>Here is one possible architecture for a coloring page generator using CloudÂ Run:</p><figure><img alt="Architecture serving a web app with Cloud Run" src="https://cdn-images-1.medium.com/max/1024/1*SecpJoaRTIPrlSsGv_dXKg.png" /></figure><p>And here is the workflow:</p><ul><li>1. The user opens the web app: the browser requests the mainÂ page.</li><li>2. Cloud Run serves the app HTMLÂ code.</li><li>3. The browser requests the additional needed resources.</li><li>4. Cloud Run serves the CSS, JavaScript, and other resources.</li><li>A. The user selects an image and the frontend sends the image to the /api/coloring-page endpoint.</li><li>B. The backend processes the input image and returns an output image, which the user can then visualize, download, or print via theÂ browser.</li></ul><h3>ğŸ SoftwareÂ stack</h3><p>Of course, there are many different software stacks that you could use to implement such an architecture.</p><p>Here is a good one based onÂ Python:</p><figure><img alt="schema" src="https://cdn-images-1.medium.com/max/1024/1*kbwf9VRqKl_9pRO9x9xG-w.png" /></figure><p>It includes:</p><ul><li><a href="https://pypi.org/project/gunicorn">Gunicorn</a>: A production-grade WSGI HTTPÂ server</li><li><a href="https://pypi.org/project/Flask">Flask</a>: A popular web app framework</li><li><a href="https://pypi.org/project/scikit-image">scikit-image</a>: An extensive image processing library</li></ul><p>Define these app dependencies in a file named requirements.txt:</p><figure><img alt="Code snippet with highlighted code. For the text version, please see the GitHub repo: https://github.com/PicardParis/cherry-on-py/tree/main/cr_image_processing" src="https://cdn-images-1.medium.com/max/659/1*p2_rOHvlbr3kOBJhviONYA.png" /></figure><h3>ğŸ¨ Image processing</h3><p>How do you remove colors from an image? One way is by detecting the object edges and removing everything but the edges in the result image. This can be done with a <a href="https://wikipedia.org/wiki/Sobel_operator">Sobel</a> filter, a convolution filter that detects the regions in which the image intensity changes theÂ most.</p><p>Create a Python file named main.py, define an image processing function, and within it use the Sobel filter and other functions from scikit-image:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*sC8g_4Sbe4JvkE3-IuoI2w.png" /></figure><blockquote><em>Note: The NumPy and Pillow libraries are automatically installed as dependencies of scikit-image.</em></blockquote><p>As an example, here is how the Cloud Run logo is processed at eachÂ step:</p><figure><img alt="Colored input transformed into edge-detected grayscale output" src="https://cdn-images-1.medium.com/max/1024/1*je2Em2eSHrVwCKcWqq3T3w.png" /></figure><h3>âœ¨ WebÂ app</h3><h4>Backend</h4><p>To expose both endpoints ( GET / and POST /api/coloring-page), add Flask routes inÂ main.py:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*HCEIvUafupEEKeg4tXtCdA.png" /></figure><h4>Frontend</h4><p>On the browser side, write a JavaScript function that calls the /api/coloring-page endpoint and receives the processed image:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*fp8qLIYKqTl1ANE3n-6ueQ.png" /></figure><p>The base of your app is there. Now you just need to add a mix of HTML + CSS + JS to complete the desired user experience.</p><h4>Local development</h4><p>To develop and test the app on your computer, once your environment is set up, make sure you have the needed dependencies:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*gW5jSS_m_DdrJ8vrAbYDug.png" /></figure><p>Add the following block to main.py. It will only execute when you run your app manually:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*KOVCp7A63RQja-oz2TNkyQ.png" /></figure><p>Run yourÂ app:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*ab-plcqk4A_u4LzN_CS7Qg.png" /></figure><p>Flask starts a local webÂ server:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*E04hdGqMC3AO-66BzG5fyQ.png" /></figure><blockquote><em>Note: In this mode, youâ€™re using a development web server (one that is not suited for production). Youâ€™ll next set up the deployment to serve your app with Gunicorn, a production-grade server.</em></blockquote><p>Youâ€™re all set. Open localhost:8080 in your browser, test, refine, andÂ iterate.</p><h3>ğŸš€ Deployment</h3><p>Once your app is ready for prime time, you can define how it will be served with this single line in a file named Procfile:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*b-UqCiIfH1xb4O5ycx7M-Q.png" /></figure><p>At this stage, here are the files found in a typicalÂ project:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*7S3XGplsZn7aCAaHu-Jdqg.png" /></figure><p>Thatâ€™s it, you can now deploy your app from the sourceÂ folder:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*-rElij1wP6lT0PtoNj928g.png" /></figure><h3>âš™ï¸ Under theÂ hood</h3><p>The command line output details all the different steps:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*YzsvbclKXnk2KFzwC7wNBg.png" /></figure><p>Cloud Build is indirectly called to containerize your app. One of its core components is Google Cloud <a href="https://github.com/GoogleCloudPlatform/buildpacks">Buildpacks</a>, which automatically builds a production-ready container image from your source code. Here are the mainÂ steps:</p><ul><li>Cloud Build fetches the sourceÂ code.</li><li>Buildpacks autodetects the app language (Python, in this case) and uses the corresponding secure baseÂ image.</li><li>Buildpacks installs the app dependencies (defined in requirements.txt forÂ Python).</li><li>Buildpacks configures the service entrypoint (defined in Procfile forÂ Python).</li><li>Cloud Build pushes the container image to <a href="https://cloud.google.com/artifact-registry">Artifact Registry</a>.</li><li>Cloud Run creates a new revision of the service based on this container image.</li><li>Cloud Run routes production traffic toÂ it.</li></ul><blockquote>Notes:<br>- Buildpacks currently supports the following runtimes: Go, Java,Â .NET, Node.js, and Python.<br>- The base image is actively maintained by Google, scanned for security vulnerabilities, and patched against known issues. This means that, when you deploy an update, your service is based on an image that is as secure as possible.<br>- If you need to build your own container image, for example with a custom runtime, you can add your own Dockerfile and Buildpacks will use itÂ instead.</blockquote><h3>ğŸ’« Updates</h3><p>More testing from real-life users shows someÂ issues.</p><p>First, the app does not handle pictures taken with digital cameras in non-native orientations. You can fix this using the EXIF orientation data:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*uynIAotvDG0qoOeBOTl_QA.png" /></figure><p>In addition, the app is too sensitive to details in the input image. Textures in paintings, or noise in pictures, can generate many edges in the processed image. You can improve the processing algorithm by adding a denoising stepÂ upfront:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*cH68EUUcrEwgqbbrxC6gIg.png" /></figure><p>This additional step makes the coloring page cleaner and reduces the quantity of ink used if you printÂ it:</p><figure><img alt="La nascita di Venere by Botticelli, with and without denoising" src="https://cdn-images-1.medium.com/max/1024/1*sUrm87Gbc6JXq-ESvdjvXw.png" /></figure><p>Redeploy, and the app is automatically updated:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*nZubLUqea4Tr6R9DAbwm5w.png" /></figure><h3>ğŸ‰ Itâ€™sÂ alive</h3><p>The app is visible as a service in CloudÂ Run:</p><figure><img alt="screenshot" src="https://cdn-images-1.medium.com/max/1024/1*SDcZzkB1a5ZJFeGQT-H__w.png" /></figure><p>The service dashboard gives you an overview of appÂ usage:</p><figure><img alt="screenshot" src="https://cdn-images-1.medium.com/max/1024/1*7kwCH5smK9AjLN9MmnNK7w.png" /></figure><p>Thatâ€™s it; your image processing app is in production!</p><figure><img alt="Animated Demo" src="https://cdn-images-1.medium.com/max/1024/1*FR6K-3gmfmoPH-JlrhD7vA.gif" /></figure><h3>ğŸ¤¯ Itâ€™s serverless</h3><p>There are many benefits to using Cloud Run in this architecture:</p><ul><li>Your app is available 24/7.</li><li>The environment is fully managed: you can focus on your code and not worry about the infrastructure.</li><li>Your app is automatically available throughÂ HTTPS.</li><li>You can map your app to a customÂ domain.</li><li>Cloud Run scales the number of instances automatically and the billing includes only the resources used when your codeÂ runs.</li><li>If your app is not used, Cloud Run scales down toÂ zero.</li><li>If your app gets more traffic (imagine it makes the news), Cloud Run scales up to the number of instances needed.</li><li>You can control performance and cost by fine-tuning many settings: CPU, memory, concurrency, minimum instances, maximum instances, andÂ more.</li><li>Every month, the <a href="https://cloud.google.com/run/pricing">free tier</a> offers the first 50 vCPU-hours, 100 GiB-hours, and 2 million requests for noÂ cost.</li></ul><h3>ğŸ’¾ SourceÂ code</h3><p>The project includes just seven files and less than 200 lines of Python + JavaScript code.</p><p>You can reuse this demo as a base to build your own image processing app:</p><ul><li>Check out the source code onÂ <a href="https://github.com/PicardParis/cherry-on-py/tree/main/cr_image_processing">GitHub</a>.</li><li>For step-by-step instructions on deploying the app yourself in a few minutes, see <a href="https://github.com/PicardParis/cherry-on-py/blob/main/cr_image_processing/DEPLOY.md">â€œDeploying from scratchâ€</a>.</li></ul><h3>ğŸ–– More</h3><ul><li><a href="https://coloring-page.lolo.dev/">Try the demo</a> and generate your own coloringÂ pages.</li><li><a href="https://cloud.google.com/run/docs">Learn more</a> about CloudÂ Run.</li><li>For more cloud content, follow me on Twitter (<a href="https://twitter.com/PicardParis">@PicardParis</a>) or LinkedIn (<a href="https://linkedin.com/in/PicardParis">in/PicardParis</a>), and feel free to get in touch with any feedback or questions.</li></ul><h3>ğŸ“œ Also in thisÂ series</h3><ol><li><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=friends_link&amp;sk=f94ff51885c51dd52539848ce04654ab">Summarizing videos</a></li><li><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=friends_link&amp;sk=c9602c33c77aa950a59282b6de5c0c57">Tracking videoÂ objects</a></li><li><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=friends_link&amp;sk=cc252ab86eab9ed2e8583963d0598661">Face detection and processing</a></li><li>Processing images</li></ol><p><em>Originally published on the </em><a href="https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run"><em>Google Cloud Blog</em></a><em> on April 5,Â 2022.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bff59e59d890" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890">Deploy a coloring page generator in minutes with Cloud Run</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Face detection and processing in 300 lines of code]]></title>
            <link>https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/38dc51a115d4</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[technology]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Tue, 29 Sep 2020 14:19:14 GMT</pubDate>
            <atom:updated>2022-04-07T10:18:08.300Z</atom:updated>
            <content:encoded><![CDATA[<h3>â³ 2021â€“10â€“08 update</h3><ul><li>Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection"><strong>GitHub version</strong></a> with latest library versions + Python 3.7 â†’Â 3.9</li></ul><h3>ğŸ‘‹ Hello!</h3><p>In this article, youâ€™ll see the following:</p><ul><li>how to detect faces in pictures,</li><li>how to automatically anonymize, crop,â€¦ a picture withÂ faces,</li><li>how to make this a serverless onlineÂ demo,</li><li>in less than 300 lines of PythonÂ code.</li></ul><p>Here is a famous face that has been automatically anonymized andÂ cropped.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/329/1*EhOakiFNwIzh6ES549zEQw.png" /><figcaption>Do you guess who thisÂ is?</figcaption></figure><blockquote>Note: Weâ€™re talking about face detection, not face recognition. Though technically possible, face recognition can have harmful applications. Responsible companies have established AI principles and avoid exposing such potentially harmful technologies (e.g. <a href="https://ai.google/principles">Google AI Principles</a>).</blockquote><h3>ğŸ› ï¸ Tools</h3><p>A few tools willÂ do:</p><ul><li>a machine learning model to analyzeÂ images,</li><li>a library to processÂ images,</li><li>a web application framework,</li><li>a serverless solution to keep the demo available 24/7 and at minimalÂ cost.</li></ul><h3>ğŸ§± Architecture</h3><p>Here is an architecture using 2 Google Cloud services (<a href="https://cloud.google.com/appengine/docs">App Engine</a> + <a href="https://cloud.google.com/vision/docs">VisionÂ API</a>):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5ms7LOeFClA5bU9BtpUMMw.png" /></figure><p>The workflow is the following:</p><ol><li>Open the demo: App Engine serves the homeÂ page.</li><li>Take a selfie: the frontend sends it to the /analyze-image endpoint.</li><li>The backend sends a request to the Vision API: the image is analyzed and the results (annotations) are returned.</li><li>The backend returns the annotations, in addition to the number of detected faces (to display the info directly in the webÂ page).</li><li>The frontend sends image, annotations, and processing options to the /process-image endpoint.</li><li>The backend processes the image with the given options and returns the resultÂ image.</li><li>Change the options: steps 5 and 6 are repeated.</li><li>Get the image with newÂ options.</li></ol><p>This is one of many possible architectures. The advantages of this one are the following:</p><ul><li>The web browser caches both the selfie and the annotations: no storage is involved and no private images are stored anywhere in theÂ cloud.</li><li>The Vision API is only called once perÂ image.</li></ul><h3>ğŸ Python libraries</h3><h4>Google CloudÂ Vision</h4><ul><li>The client library that wraps calls to the VisionÂ API.</li><li><a href="https://pypi.org/project/google-cloud-vision">https://pypi.org/project/google-cloud-vision</a></li></ul><h4>Pillow</h4><ul><li>A very popular imaging library, both extensive and easy toÂ use.</li><li><a href="https://pypi.org/project/Pillow">https://pypi.org/project/Pillow</a></li></ul><h4>Flask</h4><ul><li>One of the most popular web app frameworks.</li><li><a href="https://pypi.org/project/Flask">https://pypi.org/project/Flask</a></li></ul><h4>Dependencies</h4><p>Define the dependencies in the requirements.txt file:</p><pre><strong>google-cloud-vision</strong>==1.0.0</pre><pre><strong>Pillow</strong>==7.2.0</pre><pre><strong>Flask</strong>==1.1.2</pre><blockquote>Notes:<br>- As a best practice, also specify the dependency versions. This freezes your production environment in a known state and prevents newer versions from potentially breaking future deployments.<br>- App Engine will automatically deploy these dependencies.</blockquote><h3>ğŸ§  ImageÂ analysis</h3><h4>Vision API</h4><p>The Vision API gives access to state-of-the-art machine learning models for image analysis. One of the multiple features is face detection. Here is a way to detect faces in anÂ image:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1bc70fc4769e5468900fc4377aee3080/href">https://medium.com/media/1bc70fc4769e5468900fc4377aee3080/href</a></iframe><h4>Backend endpoint</h4><p>Exposing an API endpoint with Flask consists in wrapping a function with a route. Here is a possible POST endpoint:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9d9bff745ecaadfd4b8ae3175481e66c/href">https://medium.com/media/9d9bff745ecaadfd4b8ae3175481e66c/href</a></iframe><h4>Frontend request</h4><p>Here is a javascript function to call the API from the frontend:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/80380594bf727837b452b5a8d62a8242/href">https://medium.com/media/80380594bf727837b452b5a8d62a8242/href</a></iframe><h3>ğŸ¨ Image processing</h3><h4>Face bounding box and landmarks</h4><p>The Vision API provides the bounding box of the detected faces and the position of 30+ face landmarks (mouth, nose, eyes,â€¦). Here is a way to visualize them with PillowÂ (PIL):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9875815330881f6d59e489872744d4ab/href">https://medium.com/media/9875815330881f6d59e489872744d4ab/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/502/1*uMuj-PdEoeejIfrCh7zAMg.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><h4>Face anonymization</h4><p>Here is way to anonymize the faces thanks to the boundingÂ boxes:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8ed328faf40629175a2bd873aeda793a/href">https://medium.com/media/8ed328faf40629175a2bd873aeda793a/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/502/1*bD7vuRmQdJ-Vffi57rYiqA.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><h4>Face cropping</h4><p>Similarly, to focus on the detected faces, you can crop everything around theÂ faces:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9f72e084a4f94f76f573e4dc7f61c9a3/href">https://medium.com/media/9f72e084a4f94f76f573e4dc7f61c9a3/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/502/1*Iup-0wvY_vI10_EZ8dVG_w.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><h3>ğŸ’ Cherry on PyÂ ğŸ</h3><p>Now, the icing on the cake (or the â€œcherry on the pieâ€ as we say inÂ French):</p><ul><li>Having independent rendering functions lets you combine multiple options atÂ once.</li><li>Knowing the bounding box for all faces allows cropping the image to the minimal boundingÂ box.</li><li>Using the location of the nose and the mouth, you can add a moustache to everyone.</li><li>If your functions have parameters to render a single frame, you can generate animations with a few lines ofÂ code.</li><li>Once your Flask app works locally, you can deploy and keep it available 24/7 at minimalÂ cost.</li></ul><p>Here is whatâ€™s detected on famous photorealistic paintings:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/438/1*X4hw8gESYNENqBAtlmJB3w.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/310/1*A7BWyLQvJohvCMzJw7otug.png" /><figcaption>Girl with a Pearl Earring (<a href="https://commons.wikimedia.org/wiki/File:Meisje_met_de_parel.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/461/1*qyMK3DDzT68qLSF5y_shRg.png" /><figcaption>Shakespeare (<a href="https://commons.wikimedia.org/wiki/File:Sanders_portrait2.jpg">Wikimedia</a>)</figcaption></figure><p>Here are some animated versions:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*r7OUErW_bKnlWD0p8iYKhw.gif" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*7yqwaRfOK-EB23NrUz9kwQ.gif" /><figcaption>Girl with a Pearl Earring (<a href="https://commons.wikimedia.org/wiki/File:Meisje_met_de_parel.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*T762aj2zjz9onKAN1Gld-A.gif" /><figcaption>Shakespeare (<a href="https://commons.wikimedia.org/wiki/File:Sanders_portrait2.jpg">Wikimedia</a>)</figcaption></figure><blockquote>Note: animations are a bit degraded (GIF version) as Medium does not support animated PNGs. The demo below lets you generate them in GIF, PNG, andÂ WebP.</blockquote><p>And, of course, this works even better on real pictures:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*rDrfkj4apSdEkwI0smp3-w.png" /><figcaption>Personal pictures (aged from 2 toÂ 44)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*EVyxahDBrJyxwyf3N54QSQ.gif" /><figcaption>Yes, Iâ€™ve had a moustache for over 42 years, and my sister tooÂ ;)</figcaption></figure><p>And, finally, here is our famous anonymous from the beginning:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/329/1*VjKW1nSa0yMxKqlNLGR03w.gif" /><figcaption>Mona Lisa (<a href="https://commons.wikimedia.org/wiki/File:Mona_Lisa-LF-restoration-v2.jpg">Wikimedia</a>)</figcaption></figure><h3>ğŸš€ Source code and deployment</h3><h4>Source code</h4><ul><li>The Python code for the backend takes less than 300 lines ofÂ code.</li><li>See the <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection">source onÂ GitHub</a>.</li></ul><h4>Deployment</h4><ul><li>You can deploy this demo in 4Â minutes.</li><li>See â€œ<a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection/DEPLOY.md">Deploying from scratch</a>â€.</li></ul><h3>ğŸ‰ OnlineÂ demo</h3><p>Try the demo by yourself:<br>â¡ï¸ <a href="https://face-detection.lolo.dev/">https://face-detection.lolo.dev</a> â¬…ï¸</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/333/1*sXW8u2q5bpcS_S2nb3a72w.gif" /><figcaption><a href="https://face-detection.lolo.dev/">https://face-detection.lolo.dev</a></figcaption></figure><h3>ğŸ–– SeeÂ you</h3><p><a href="https://bit.ly/feedback-face-detection">Feedback, questions</a>? Iâ€™d love to read from you! <a href="https://twitter.com/PicardParis">Follow me on Twitter</a> forÂ moreâ€¦</p><h3>â³ Updates</h3><ul><li><strong>2021â€“10â€“08</strong>: Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection">GitHub version</a> with latest library versions + Python 3.7 â†’Â 3.9</li></ul><h3>ğŸ“œ Also in thisÂ series</h3><ol><li><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=friends_link&amp;sk=f94ff51885c51dd52539848ce04654ab">Summarizing videos</a></li><li><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=friends_link&amp;sk=c9602c33c77aa950a59282b6de5c0c57">Tracking videoÂ objects</a></li><li>Face detection and processing</li><li><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=friends_link&amp;sk=a3d6e22e7e77828e411592f46025531e">Processing images</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=38dc51a115d4" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4">Face detection and processing in 300 lines of code</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tracking video objects in 300 lines of code]]></title>
            <link>https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/18eb4227df34</guid>
            <category><![CDATA[python]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Thu, 25 Jun 2020 17:46:01 GMT</pubDate>
            <atom:updated>2022-04-07T10:17:26.867Z</atom:updated>
            <content:encoded><![CDATA[<h3>â³ 2021â€“10â€“08 update</h3><ul><li>Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking"><strong>GitHub version</strong></a> with latest library versions + Python 3.7 â†’Â 3.9</li></ul><h3>ğŸ‘‹ Hello!</h3><p>In this article, youâ€™ll see the following:</p><ul><li>how to track objects present in aÂ video,</li><li>with an automated processing pipeline,</li><li>in less than 300 lines of PythonÂ code.</li></ul><p>Here is an example of an auto-generated object summary for the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/animals.mp4">animals.mp4</a>&gt;:</p><figure><img alt="Tracked object summary for animals.mp4" src="https://cdn-images-1.medium.com/max/1024/0*U73HEF_TUvYuP1Ne.jpeg" /></figure><h3>ğŸ› ï¸ Tools</h3><p>A few tools willÂ do:</p><ul><li>Storage space for videos andÂ results</li><li>A serverless solution to run theÂ code</li><li>A machine learning model to analyzeÂ videos</li><li>A library to extract frames fromÂ videos</li><li>A library to render theÂ objects</li></ul><h3>ğŸ§± Architecture</h3><p>Here is a possible architecture using 3 Google Cloud services (<a href="https://cloud.google.com/storage/docs">Cloud Storage</a>, <a href="https://cloud.google.com/functions/docs">Cloud Functions</a>, and the <a href="https://cloud.google.com/video-intelligence/docs">Video Intelligence API</a>):</p><figure><img alt="Architecture" src="https://cdn-images-1.medium.com/max/1024/0*-mUux73HcpRY0Uue.png" /></figure><p>The processing pipeline follows theseÂ steps:</p><ol><li>You upload aÂ video</li><li>The upload event automatically triggers the trackingÂ function</li><li>The function sends a request to the Video Intelligence API</li><li>The Video Intelligence API analyzes the video and uploads the results (annotations)</li><li>The upload event triggers the rendering function</li><li>The function downloads both annotation and videoÂ files</li><li>The function renders and uploads theÂ objects</li><li>You know which objects are present in yourÂ video!</li></ol><h3>ğŸ Python libraries</h3><h4>Video Intelligence API</h4><ul><li>To analyzeÂ videos</li><li><a href="https://pypi.org/project/google-cloud-videointelligence">https://pypi.org/project/google-cloud-videointelligence</a></li></ul><h4>Cloud Storage</h4><ul><li>To manage downloads andÂ uploads</li><li><a href="https://pypi.org/project/google-cloud-storage">https://pypi.org/project/google-cloud-storage</a></li></ul><h4>OpenCV</h4><ul><li>To extract videoÂ frames</li><li>OpenCV offers a headless version (without GUI features, ideal for aÂ service)</li><li><a href="https://pypi.org/project/opencv-python-headless">https://pypi.org/project/opencv-python-headless</a></li></ul><h4>Pillow</h4><ul><li>To render and annotate objectÂ images</li><li>Pillow is a very popular imaging library, both extensive and easy toÂ use</li><li><a href="https://pypi.org/project/Pillow">https://pypi.org/project/Pillow</a></li></ul><h3>ğŸ§  VideoÂ analysis</h3><h4>Video Intelligence API</h4><p>The Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of its multiple features is detecting and tracking objects. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the OBJECT_TRACKING feature:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8a64a3c78d7f90be9bdea2a2525f0189/href">https://medium.com/media/8a64a3c78d7f90be9bdea2a2525f0189/href</a></iframe><h4>Cloud Function entryÂ point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/49ed5269b70727a1fa83a5f30fcdce89/href">https://medium.com/media/49ed5269b70727a1fa83a5f30fcdce89/href</a></iframe><blockquote>Notes:<br>â€¢ This function will be called when a video is uploaded to the bucket defined as a trigger.<br>â€¢ Using an environment variable makes the code more portable and lets you deploy the exact same code with different trigger and outputÂ buckets.</blockquote><h3>ğŸ¨ Object rendering</h3><h4>Code structure</h4><p>Itâ€™s interesting to split the code into 2 mainÂ classes:</p><ul><li>StorageHelper for managing local files and cloud storageÂ objects</li><li>VideoProcessor for all graphical processings</li></ul><p>Here is a possible core function for the 2nd Cloud Function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/02a54ddd6ee288746d69f59c26a48b07/href">https://medium.com/media/02a54ddd6ee288746d69f59c26a48b07/href</a></iframe><h4>Cloud Function entryÂ point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/43f8e86165e95433fed65f48610591b0/href">https://medium.com/media/43f8e86165e95433fed65f48610591b0/href</a></iframe><blockquote>Note: This function will be called when an annotation file is uploaded to the bucket defined as aÂ trigger.</blockquote><h4>Frame rendering</h4><p>OpenCV and Pillow easily let you extract video frames and compose overÂ them:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c5f67819bddc6217f60fc87480e76a37/href">https://medium.com/media/c5f67819bddc6217f60fc87480e76a37/href</a></iframe><blockquote>Note: It would probably be possible to only use OpenCV but I found it more productive developing with Pillow (code is more readable and intuitive).</blockquote><h3>ğŸ” Results</h3><p>Here are the main objects found in the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4">JaneGoodall.mp4</a>&gt;:</p><figure><img alt="Tracked object summary for JaneGoodall.mp4" src="https://cdn-images-1.medium.com/max/1024/0*v21gdT8bxvr_C_gP.jpeg" /></figure><blockquote>Notes:<br>â€¢ The machine learning model has correctly identified different wildlife species: those are â€œtrue positivesâ€. It has also incorrectly identified our planet as â€œpackaged goodsâ€: this is a â€œfalse positiveâ€. Machine learning models keep learning by being trained with new samples so, with time, their precision keeps increasing (resulting in less false positives).<br>â€¢ The current code filters out objects detected with a confidence below 70% or with less than 10 frames. Lower the thresholds to get moreÂ results.</blockquote><h3>ğŸ’ Cherry on PyÂ ğŸ</h3><p>Now, the icing on the cake (or the â€œcherry on the pieâ€ as we say in French), you can enrich the architecture to add new possibilities:</p><ul><li>Trigger the processing for videos from any bucket (including external publicÂ buckets)</li><li>Generate individual object animations (in parallel to object summaries)</li></ul><h4>Architecture (v2)</h4><figure><img alt="Architecture (v2)" src="https://cdn-images-1.medium.com/max/1024/0*kMM7XhzxfkkpdkTR.png" /></figure><ul><li>Aâ€Šâ€”â€ŠVideo object tracking can also be triggered manually with an HTTP GETÂ request</li><li>Bâ€Šâ€”â€ŠThe same rendering code is deployed in 2 sibling functions, differentiated with an environment variable</li><li>Câ€Šâ€”â€ŠObject summaries and animations are generated inÂ parallel</li></ul><h4>Cloud Function HTTP entryÂ point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/42072aef5f825d5b2298aab3e7ff778c/href">https://medium.com/media/42072aef5f825d5b2298aab3e7ff778c/href</a></iframe><blockquote>Note: This is the same code as gcf_track_objects() with the video URI parameter specified by the caller through a GETÂ request.</blockquote><h3>ğŸ‰ Results</h3><p>Here are some auto-generated trackings for the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/animals.mp4">animals.mp4</a>&gt;:</p><ul><li>The left elephant (a big objectÂ ;) is detected:</li></ul><figure><img alt="Elephant on the left" src="https://cdn-images-1.medium.com/max/1024/0*uKmLlOt_2qr6_mBA.gif" /></figure><ul><li>The right elephant is perfectly isolatedÂ too:</li></ul><figure><img alt="Elephant on the right" src="https://cdn-images-1.medium.com/max/1024/0*fMtzSrFPU6IJ19j3.gif" /></figure><ul><li>The veterinarian is correctly identified:</li></ul><figure><img alt="Person on the left" src="https://cdn-images-1.medium.com/max/1024/0*M-XTcd6nMar2bgKL.gif" /></figure><ul><li>The animal heâ€™s feedingÂ too:</li></ul><figure><img alt="Animal on the right" src="https://cdn-images-1.medium.com/max/1024/0*TMz8h9fIqyLNh6y3.gif" /></figure><p>Moving objects or static objects in moving shots are tracked too, as in &lt;beyond-the-map-rio.mp4&gt;:</p><ul><li>A building in a movingÂ shot:</li></ul><figure><img alt="Shot with buildings 1" src="https://cdn-images-1.medium.com/max/1024/0*X5Z9ORKF_o3ZSItY.gif" /></figure><ul><li>Neighbor buildings are trackedÂ too:</li></ul><figure><img alt="Shot with buildings 2" src="https://cdn-images-1.medium.com/max/1024/0*A1T0UfvHwrokdgWz.gif" /></figure><ul><li>Persons in a movingÂ shot:</li></ul><figure><img alt="Moving persons" src="https://cdn-images-1.medium.com/max/1024/0*YjdIZ6jHfJbhZJ1X.gif" /></figure><ul><li>A surfer crossing theÂ shot:</li></ul><figure><img alt="Surfer" src="https://cdn-images-1.medium.com/max/1024/0*b1da7YkFdsqqf6bV.gif" /></figure><p>Here are some others for the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4">JaneGoodall.mp4</a>&gt;:</p><ul><li>A butterfly (easy?):</li></ul><figure><img alt="Butterfly" src="https://cdn-images-1.medium.com/max/1024/0*GuhCti_2CpVZUn1n.gif" /></figure><ul><li>An insect, in larval stage, climbing a movingÂ twig:</li></ul><figure><img alt="Caterpillar on moving twig" src="https://cdn-images-1.medium.com/max/1024/0*20XPF-jSBu_pS5KO.gif" /></figure><ul><li>An ape in a tree far awayÂ (hard?):</li></ul><figure><img alt="Ape catching bugs in tree far away" src="https://cdn-images-1.medium.com/max/1024/0*mabEdHmI0VtkEUSJ.gif" /></figure><ul><li>A monkey jumping from the top of a tree (harder?):</li></ul><figure><img alt="Monkey jumping from tree top" src="https://cdn-images-1.medium.com/max/1024/0*uB8flZZRyxc7g8Cc.gif" /></figure><ul><li>Now, a trap! If we can be fooled, current machine learning state of the art canÂ too:</li></ul><figure><img alt="A flower or maybe not a flower" src="https://cdn-images-1.medium.com/max/1024/0*9KwNRtwax1eV2tg8.gif" /></figure><h3>ğŸš€ Source code and deployment</h3><h3>Source code</h3><ul><li>The Python source is less than 300 lines ofÂ code.</li><li>See the <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking">source onÂ GitHub</a>.</li></ul><h3>Deployment</h3><ul><li>You can deploy this architecture in less than 8Â minutes.</li><li>See â€œ<a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking/DEPLOY.md">Deploying from scratch</a>â€.</li></ul><h3>ğŸ–– SeeÂ you</h3><p>Do you want more, do you have questions? Iâ€™d love to read <a href="https://bit.ly/feedback-video-object-tracking">your feedback</a>. You can also <a href="https://twitter.com/PicardParis">follow me onÂ Twitter</a>.</p><h3>â³ Updates</h3><ul><li><strong>2021â€“10â€“08</strong>: Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking">GitHub version</a> with latest library versions + Python 3.7 â†’Â 3.9</li></ul><h3>ğŸ“œ Also in thisÂ series</h3><ol><li><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=friends_link&amp;sk=f94ff51885c51dd52539848ce04654ab">Summarizing videos</a></li><li>Tracking videoÂ objects</li><li><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=friends_link&amp;sk=cc252ab86eab9ed2e8583963d0598661">Face detection and processing</a></li><li><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=friends_link&amp;sk=a3d6e22e7e77828e411592f46025531e">Processing images</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=18eb4227df34" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34">Tracking video objects in 300 lines of code</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Summarizing videos in 300 lines of code]]></title>
            <link>https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/c2f261c8035c</guid>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Sat, 30 May 2020 13:54:30 GMT</pubDate>
            <atom:updated>2022-04-07T10:16:05.744Z</atom:updated>
            <content:encoded><![CDATA[<h3>â³ 2021â€“10â€“08 update</h3><ul><li>Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_video_summary"><strong>GitHub version</strong></a> with latest library versions + Python 3.7 â†’Â 3.9</li></ul><h3>ğŸ‘‹ Hello!</h3><p>Dear developers,</p><p>Do you like the adage <em>â€œa picture is worth a thousand wordsâ€</em>? I do! Letâ€™s check if it also works for <em>â€œa picture is worth a thousandÂ framesâ€</em>.</p><p>In this tutorial, youâ€™ll see the following:</p><ul><li>how to understand the content of a video in aÂ blink,</li><li>in less than 300 lines of Python (3.7)Â code.</li></ul><figure><img alt="Video summary example" src="https://cdn-images-1.medium.com/max/1024/0*sBJJr_f1TVd9Ocwl.jpeg" /><figcaption>A visual summary generated from a 2&#39;42&quot; video made of 35 sequences (shots). The summary is a grid where each cell is a frame representing a videoÂ shot.</figcaption></figure><h3>ğŸ”­ Objectives</h3><p>This tutorial has 2 objectives, 1 practical and 1 technical:</p><ol><li>Automatically generate visual summaries ofÂ videos</li><li>Build a processing pipeline with these properties:</li></ol><ul><li>managed (always ready and easy to setÂ up)</li><li>scalable (able to ingest several videos in parallel)</li><li>not costing anything when notÂ used</li></ul><h3>ğŸ› ï¸ Tools</h3><p>A few tools areÂ enough:</p><ul><li>Storage space for videos andÂ results</li><li>A serverless solution to run theÂ code</li><li>A machine learning model to analyzeÂ videos</li><li>A library to extract frames fromÂ videos</li><li>A library to generate the visual summaries</li></ul><h3>ğŸ§± Architecture</h3><p>Here is a possible architecture using 3 Google Cloud services (<a href="https://cloud.google.com/storage/docs">Cloud Storage</a>, <a href="https://cloud.google.com/functions/docs">Cloud Functions</a>, and <a href="https://cloud.google.com/video-intelligence/docs">Video Intelligence API</a>):</p><figure><img alt="Architecture" src="https://cdn-images-1.medium.com/max/1024/0*WSqvJEpDo9DVkCua.png" /></figure><p>The processing pipeline follows theseÂ steps:</p><ol><li>You upload a video to the 1st bucket (a bucket is a storage space in theÂ cloud)</li><li>The upload event automatically triggers the 1stÂ function</li><li>The function sends a request to the Video Intelligence API to detect theÂ shots</li><li>The Video Intelligence API analyzes the video and uploads the results (annotations) to the 2ndÂ bucket</li><li>The upload event triggers the 2ndÂ function</li><li>The function downloads both annotation and videoÂ files</li><li>The function renders and uploads the summary to the 3rdÂ bucket</li><li>The video summary isÂ ready!</li></ol><h3>ğŸ Python libraries</h3><p>Open source client libraries let you interface with Google Cloud services in idiomatic Python. Youâ€™ll use the following:</p><p>Cloud Storage</p><ul><li>To manage downloads andÂ uploads</li><li><a href="https://pypi.org/project/google-cloud-storage">https://pypi.org/project/google-cloud-storage</a></li></ul><p>Video Intelligence API</p><ul><li>To analyzeÂ videos</li><li><a href="https://pypi.org/project/google-cloud-videointelligence">https://pypi.org/project/google-cloud-videointelligence</a></li></ul><p>Here is a choice of 2 additional Python libraries for the graphical needs:</p><p>OpenCV</p><ul><li>To extract videoÂ frames</li><li>Thereâ€™s even a headless version (without GUI features), which is ideal for aÂ service</li><li><a href="https://pypi.org/project/opencv-python-headless">https://pypi.org/project/opencv-python-headless</a></li></ul><p>Pillow</p><ul><li>To generate the visual summaries</li><li>Pillow is a very popular imaging library, both extensive and easy toÂ use</li><li><a href="https://pypi.org/project/Pillow">https://pypi.org/project/Pillow</a></li></ul><h3>âš™ï¸ ProjectÂ setup</h3><p>Assuming you have a Google Cloud account, you can set up the architecture from Cloud Shell with the gcloud and gsutil commands. This lets you script everything from scratch in a reproducible way.</p><h4>Environment variables</h4><pre># Project<br><strong>PROJECT_NAME=&quot;Visual Summary&quot;<br>PROJECT_ID=&quot;visual-summary-REPLACE_WITH_UNIQUE_SUFFIX&quot;<br></strong># Cloud Storage region (https://cloud.google.com/storage/docs/locations)<br><strong>GCS_REGION=&quot;europe-west1&quot;<br></strong># Cloud Functions region (https://cloud.google.com/functions/docs/locations)<br><strong>GCF_REGION=&quot;europe-west1&quot;<br></strong># Source<br><strong>GIT_REPO=&quot;cherry-on-py&quot;<br>PROJECT_SRC=~/$PROJECT_ID/$GIT_REPO/gcf_video_summary<br><br></strong># Cloud Storage buckets (environment variables)<br><strong>export VIDEO_BUCKET=&quot;b1-videos_${PROJECT_ID}&quot;<br>export ANNOTATION_BUCKET=&quot;b2-annotations_${PROJECT_ID}&quot;<br>export SUMMARY_BUCKET=&quot;b3-summaries_${PROJECT_ID}&quot;</strong></pre><blockquote>Note: You can use your GitHub username as a uniqueÂ suffix.</blockquote><h4>New project</h4><pre><strong>gcloud projects create $PROJECT_ID \<br>  --name=&quot;$PROJECT_NAME&quot; \<br>  --set-as-default</strong></pre><pre>Create in progress for [https://cloudresourcemanager.googleapis.com/v1/projects/PROJECT_ID].<br>Waiting for [operations/cp...] to finish...done.<br>Enabling service [cloudapis.googleapis.com] on project [PROJECT_ID]...<br>Operation &quot;operations/acf...&quot; finished successfully.<br>Updated property [core/project] to [PROJECT_ID].</pre><h4>Billing account</h4><pre># Link project with billing account (single account)<br><strong>BILLING_ACCOUNT=$(gcloud beta billing accounts list \<br>    --format &#39;value(name)&#39;)</strong><br># Link project with billing account (specific one among multiple accounts)<br>BILLING_ACCOUNT=$(gcloud beta billing accounts list  \<br>    --format &#39;value(name)&#39; \<br>    --filter &quot;displayName=&#39;My Billing Account&#39;&quot;)<br><br><strong>gcloud beta billing projects link $PROJECT_ID --billing-account $BILLING_ACCOUNT</strong></pre><pre>billingAccountName: billingAccounts/XXXXXX-YYYYYY-ZZZZZZ<br>billingEnabled: true<br>name: projects/PROJECT_ID/billingInfo<br>projectId: PROJECT_ID</pre><h4>Buckets</h4><pre># Create buckets with uniform bucket-level access<br><strong>gsutil mb -b on -c regional -l $GCS_REGION gs://$VIDEO_BUCKET<br>gsutil mb -b on -c regional -l $GCS_REGION gs://$ANNOTATION_BUCKET<br>gsutil mb -b on -c regional -l $GCS_REGION gs://$SUMMARY_BUCKET</strong></pre><pre>Creating gs://VIDEO_BUCKET/...<br>Creating gs://ANNOTATION_BUCKET/...<br>Creating gs://SUMMARY_BUCKET/...</pre><p>You can check how it looks like in the <a href="https://console.cloud.google.com/storage/browser">CloudÂ Console</a>:</p><figure><img alt="Cloud Storage buckets" src="https://cdn-images-1.medium.com/max/1024/0*X1EtcCS5kT9NC0mX.png" /></figure><h4>Service account</h4><p>Create a service account. This is for development purposes only (not needed for production). This provides you with credentials to run your codeÂ locally.</p><pre><strong>mkdir ~/$PROJECT_ID<br>cd ~/$PROJECT_ID<br><br>SERVICE_ACCOUNT_NAME=&quot;dev-service-account&quot;<br>SERVICE_ACCOUNT=&quot;${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&quot;<br>gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME<br>gcloud iam service-accounts keys create ~/$PROJECT_ID/key.json --iam-account $SERVICE_ACCOUNT</strong></pre><pre>Created service account [SERVICE_ACCOUNT_NAME].<br>created key [...] of type [json] as [~/PROJECT_ID/key.json] for [SERVICE_ACCOUNT]</pre><p>Set the GOOGLE_APPLICATION_CREDENTIALS environment variable and check that it points to the service account key. When you run the application code in the current shell session, client libraries will use these credentials for authentication. If you open a new shell session, set the variableÂ again.</p><pre><strong>export GOOGLE_APPLICATION_CREDENTIALS=~/$PROJECT_ID/key.json<br>cat $GOOGLE_APPLICATION_CREDENTIALS</strong></pre><pre>{<br>  &quot;type&quot;: &quot;service_account&quot;,<br>  &quot;project_id&quot;: &quot;PROJECT_ID&quot;,<br>  &quot;private_key_id&quot;: &quot;...&quot;,<br>  &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\n...&quot;,<br>  &quot;client_email&quot;: &quot;SERVICE_ACCOUNT&quot;,<br>  ...<br>}</pre><p>Authorize the service account to access theÂ buckets:</p><pre><strong>IAM_BINDING=&quot;serviceAccount:${SERVICE_ACCOUNT}:roles/storage.objectAdmin&quot;<br>gsutil iam ch $IAM_BINDING gs://$VIDEO_BUCKET<br>gsutil iam ch $IAM_BINDING gs://$ANNOTATION_BUCKET<br>gsutil iam ch $IAM_BINDING gs://$SUMMARY_BUCKET</strong></pre><h4>APIs</h4><p>A few APIs are enabled byÂ default:</p><pre><strong>gcloud services list</strong></pre><pre>NAME                              TITLE<br>bigquery.googleapis.com           BigQuery API<br>bigquerystorage.googleapis.com    BigQuery Storage API<br>cloudapis.googleapis.com          Google Cloud APIs<br>clouddebugger.googleapis.com      Cloud Debugger API<br>cloudtrace.googleapis.com         Cloud Trace API<br>datastore.googleapis.com          Cloud Datastore API<br>logging.googleapis.com            Cloud Logging API<br>monitoring.googleapis.com         Cloud Monitoring API<br>servicemanagement.googleapis.com  Service Management API<br>serviceusage.googleapis.com       Service Usage API<br>sql-component.googleapis.com      Cloud SQL<br>storage-api.googleapis.com        Google Cloud Storage JSON API<br>storage-component.googleapis.com  Cloud Storage</pre><p>Enable the Video Intelligence and Cloud Functions APIs:</p><pre><strong>gcloud services enable \<br>  videointelligence.googleapis.com \<br>  cloudfunctions.googleapis.com</strong></pre><pre>Operation &quot;operations/acf...&quot; finished successfully.</pre><h4>Source code</h4><p>Retrieve the sourceÂ code:</p><pre><strong>cd ~/$PROJECT_ID<br>git clone </strong><a href="https://github.com/PicardParis/$GIT_REPO.git"><strong>https://github.com/PicardParis/$GIT_REPO.git</strong></a></pre><pre>Cloning into &#39;GIT_REPO&#39;...<br>...</pre><h3>ğŸ§  VideoÂ analysis</h3><h4>Video shot detection</h4><p>The Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of the multiple features is video shot detection. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the SHOT_CHANGE_DETECTION feature:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a149fabef417bf720977f28c1ca4afb0/href">https://medium.com/media/a149fabef417bf720977f28c1ca4afb0/href</a></iframe><h4>Local development andÂ tests</h4><p>Before deploying the function, you need to develop and test it. Create a Python 3 virtual environment and activateÂ it:</p><pre><strong>cd ~/$PROJECT_ID<br>python3 -m venv venv<br>source venv/bin/activate</strong></pre><p>Install the dependencies:</p><pre><strong>pip install -r $PROJECT_SRC/gcf1_detect_shots/requirements.txt</strong></pre><p>Check the dependencies:</p><pre><strong>pip list</strong></pre><pre>Package                        Version<br>------------------------------ ----------<br>...<br>google-cloud-storage           1.28.1<br>google-cloud-videointelligence 1.14.0<br>...</pre><p>You can use the main scope to test the function in scriptÂ mode:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a25fa778a61308c2584487ea157c1104/href">https://medium.com/media/a25fa778a61308c2584487ea157c1104/href</a></iframe><blockquote>Note: You have already exported the ANNOTATION_BUCKET environment variable earlier in the shell session; you will also define it later at deployment stage. This makes the code generic and lets you reuse it independently of the outputÂ bucket.</blockquote><p>Test the function:</p><pre><strong>VIDEO_PATH=&quot;cloud-samples-data/video</strong><strong>/gbikes_dinosaur.mp4&quot;<br>VIDEO_URI=&quot;gs://$VIDEO_PATH&quot;<br>python $PROJECT_SRC/gcf1_detect_shots/main.py $VIDEO_URI</strong></pre><pre>Launching shot detection for &lt;gs://cloud-samples-data/video/gbikes_dinosaur.mp4&gt;...</pre><blockquote>Note: The test video &lt;gbikes_dinosaur.mp4&gt; is located in an external bucket. This works because the video is publicly accessible.</blockquote><p>Wait a moment and check that the annotations have been generated:</p><pre><strong>gsutil ls -r gs://$ANNOTATION_BUCKET</strong></pre><pre>964  YYYY-MM-DDThh:mm:ssZ  gs://ANNOTATION_BUCKET/VIDEO_PATH.json<br>TOTAL: 1 objects, 964 bytes (964 B)</pre><p>Check the last 200 bytes of the annotation file:</p><pre><strong>gsutil cat -r -200 gs://$ANNOTATION_BUCKET/$VIDEO_PATH.json</strong></pre><pre>}<br>    }, {<br>      &quot;start_time_offset&quot;: {<br>        &quot;seconds&quot;: 28,<br>        &quot;nanos&quot;: 166666000<br>      },<br>      &quot;end_time_offset&quot;: {<br>        &quot;seconds&quot;: 42,<br>        &quot;nanos&quot;: 766666000<br>      }<br>    } ]<br>  } ]<br>}</pre><blockquote>Note: Those are the start and end positions of the last video shot. Everything seemsÂ fine.</blockquote><p>Clean up when youâ€™re finished:</p><pre><strong>gsutil rm gs://$ANNOTATION_BUCKET/$VIDEO_PATH.json<br><br>deactivate<br><br>rm -rf venv</strong></pre><h4>Function entryÂ point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/61fd9a41967bfac4ab496eda6b118c78/href">https://medium.com/media/61fd9a41967bfac4ab496eda6b118c78/href</a></iframe><blockquote>Note: This function will be called whenever a video is uploaded to the bucket defined as aÂ trigger.</blockquote><h4>Function deployment</h4><p>Deploy the 1st function:</p><pre><strong>GCF_NAME=&quot;gcf1_detect_shots&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf1_detect_shots&quot;<br>GCF_ENTRY_POINT=&quot;gcf_detect_shots&quot;<br>GCF_TRIGGER_BUCKET=&quot;$VIDEO_BUCKET&quot;<br>GCF_ENV_VARS=&quot;ANNOTATION_BUCKET=$ANNOTATION_BUCKET&quot;<br>GCF_MEMORY=&quot;128MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS \<br>  --trigger-bucket $GCF_TRIGGER_BUCKET \<br>  --region $GCF_REGION \<br>  --memory $GCF_MEMORY \<br>  --quiet</strong></pre><blockquote>Note: The default memory allocated for a Cloud Function is 256 MB (possible values are 128MB, 256MB, 512MB, 1024MB, and 2048MB). As the function has no memory or CPU needs (it sends a simple API request), the minimum memory setting isÂ enough.</blockquote><pre>Deploying function (may take a while - up to 2 minutes)...done.<br>availableMemoryMb: 128<br>entryPoint: gcf_detect_shots<br>environmentVariables:<br>  ANNOTATION_BUCKET: b2-annotations...<br>eventTrigger:<br>  eventType: google.storage.object.finalize<br>...<br>status: ACTIVE<br>timeout: 60s<br>updateTime: &#39;YYYY-MM-DDThh:mm:ss.mmmZ&#39;<br>versionId: &#39;1&#39;</pre><blockquote>Note: The ANNOTATION_BUCKET environment variable is defined with the --update-env-vars flag. Using an environment variable lets you deploy the exact same code with different trigger and outputÂ buckets.</blockquote><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">CloudÂ Console</a>:</p><figure><img alt="Cloud Functions" src="https://cdn-images-1.medium.com/max/1024/0*IPQ5AixblUiLf8G_.png" /></figure><h4>Production tests</h4><p>Make sure to test the function in production. Copy a video into the videoÂ bucket:</p><pre><strong>VIDEO_NAME=&quot;gbikes_dinosaur.mp4&quot;<br>SRC_URI=&quot;gs://cloud-samples-data/video</strong><strong>/$VIDEO_NAME&quot;<br>DST_URI=&quot;gs://$VIDEO_BUCKET/$VIDEO_NAME&quot;<br><br>gsutil cp $SRC_URI $DST_URI</strong></pre><pre>Copying gs://cloud-samples-data/video/gbikes_dinosaur.mp4 [Content-Type=video/mp4]...<br>- [1 files][ 62.0 MiB/ 62.0 MiB]<br>Operation completed over 1 objects/62.0 MiB.</pre><p>Query the logs to check that the function has been triggered:</p><pre><strong>gcloud functions logs read --region $GCF_REGION</strong></pre><pre>LEVEL  NAME               EXECUTION_ID  TIME_UTC  LOG<br>D      gcf1_detect_shots  ...           ...       Function execution started<br>I      gcf1_detect_shots  ...           ...       Launching shot detection for &lt;gs://VIDEO_BUCKET/VIDEO_NAME&gt;...<br>D      gcf1_detect_shots  ...           ...       Function execution took 874 ms, finished with status: &#39;ok&#39;</pre><p>Wait a moment and check the annotation bucket:</p><pre><strong>gsutil ls -r gs://$ANNOTATION_BUCKET</strong></pre><p>You should see the annotation file:</p><pre>gs://ANNOTATION_BUCKET/VIDEO_BUCKET/:<br>gs://ANNOTATION_BUCKET/VIDEO_BUCKET/VIDEO_NAME.json</pre><p>The 1st function is operational!</p><h3>ğŸï¸ VisualÂ Summary</h3><h4>Code structure</h4><p>Itâ€™s interesting to split the code into 2 mainÂ classes:</p><ul><li>StorageHelper for local file and cloud storage object management</li><li>VideoProcessor for graphical processings</li></ul><p>Here is a possible core function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a93998518c177c34a6c86d85593fdb53/href">https://medium.com/media/a93998518c177c34a6c86d85593fdb53/href</a></iframe><blockquote>Note: If exceptions are raised, itâ€™s handy to log them with logging.exception() to get a stack trace in production logs.</blockquote><h4>Class StorageHelper</h4><p>The class manages the following:</p><ul><li>The retrieval and parsing of video shot annotations</li><li>The download of sourceÂ videos</li><li>The upload of generated visual summaries</li><li>File names</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c45a6726678a903e043c6329077814d5/href">https://medium.com/media/c45a6726678a903e043c6329077814d5/href</a></iframe><p>The source video is handled in the with statement contextÂ manager:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/eef563f23b6bcfc6caf00d35528c7f27/href">https://medium.com/media/eef563f23b6bcfc6caf00d35528c7f27/href</a></iframe><blockquote>Note: Once downloaded, the video uses memory space in the /tmp RAM disk (the only writable space for the serverless function). It&#39;s best to delete temporary files when they&#39;re not needed anymore, to avoid potential out-of-memory errors on future invocations of the function.</blockquote><p>Annotations are retrieved with the methods storage.Blob.download_as_string() and json.loads():</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6af2853cabbc5c5c4429d00e1a87c8ad/href">https://medium.com/media/6af2853cabbc5c5c4429d00e1a87c8ad/href</a></iframe><p>The parsing is handled with this VideoShot helperÂ class:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d95d8f8b8a79dde2f0187f8edf4b94c4/href">https://medium.com/media/d95d8f8b8a79dde2f0187f8edf4b94c4/href</a></iframe><p>Video shot info can be exposed with a getter and a generator:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/32f9d146cee581b4cebc6f9abcb3c8f3/href">https://medium.com/media/32f9d146cee581b4cebc6f9abcb3c8f3/href</a></iframe><p>The naming convention was chosen to keep consistent object paths between the different buckets. This also lets you deduce the video path from the annotation URI:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/da802bd2fde9d19c901cd7e5ba4b4eae/href">https://medium.com/media/da802bd2fde9d19c901cd7e5ba4b4eae/href</a></iframe><p>The video is directly downloaded with storage.Blob.download_to_filename():</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/56207a18cb0eb2250511f6581ced3dce/href">https://medium.com/media/56207a18cb0eb2250511f6581ced3dce/href</a></iframe><p>On the opposite, results can be uploaded with storage.Blob.upload_from_string():</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bf5877b770412025188594002e5f61fa/href">https://medium.com/media/bf5877b770412025188594002e5f61fa/href</a></iframe><blockquote>Note: from_string means from_bytes here (Python 2 legacy). Pillow supports working with memory images, which avoids having to manage localÂ files.</blockquote><p>And finally, here is a possible naming convention for the summaryÂ files:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3248045d7d7ec64ef8794070a06cf3bc/href">https://medium.com/media/3248045d7d7ec64ef8794070a06cf3bc/href</a></iframe><h4>Class VideoProcessor</h4><p>The class manages the following:</p><ul><li>Video frame extraction</li><li>Visual summary generation</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a729b2940d13ac7e97016a08622b77b1/href">https://medium.com/media/a729b2940d13ac7e97016a08622b77b1/href</a></iframe><p>Opening and closing the video is handled in the with statement contextÂ manager:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/966117816753013e7647fc08971001d8/href">https://medium.com/media/966117816753013e7647fc08971001d8/href</a></iframe><p>The video summary is a grid of cells which can be rendered in a single loop with two generators:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/24491fd8f2a86539b574d4babf76100d/href">https://medium.com/media/24491fd8f2a86539b574d4babf76100d/href</a></iframe><blockquote>Note: shot_ratio is set to 0.5 by default to extract video shot middleÂ frames.</blockquote><p>The first generator yields cellÂ images:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/97dcbe6f13eeab56ee5c318c2f30fdab/href">https://medium.com/media/97dcbe6f13eeab56ee5c318c2f30fdab/href</a></iframe><p>The second generator yields cell positions:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8a04df38e6b81926cf63889b8303847f/href">https://medium.com/media/8a04df38e6b81926cf63889b8303847f/href</a></iframe><p>OpenCV easily allows extracting video frames at a given position:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/042c493a92fa43c8c4076bf48c752a1d/href">https://medium.com/media/042c493a92fa43c8c4076bf48c752a1d/href</a></iframe><p>Choosing the summary grid composition is arbitrary. Here is an example to compose a summary preserving the video proportions:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6c3961789a20e1e78337737448cd5daa/href">https://medium.com/media/6c3961789a20e1e78337737448cd5daa/href</a></iframe><p>Finally, Pillow gives full control on image serializations:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/028535063c6ee49659586b1ad95dd163/href">https://medium.com/media/028535063c6ee49659586b1ad95dd163/href</a></iframe><blockquote>Note: Working with in-memory images avoids managing local files and uses lessÂ memory.</blockquote><h4>Local development andÂ tests</h4><p>You can use the main scope to test the function in scriptÂ mode:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a8a7d0f626fcb78a072f9d4a5d66a752/href">https://medium.com/media/a8a7d0f626fcb78a072f9d4a5d66a752/href</a></iframe><p>Test the function:</p><pre><strong>cd ~/$PROJECT_ID<br>python3 -m venv venv<br>source venv/bin/activate<br><br>pip install -r $PROJECT_SRC/gcf2_generate_summary/requirements.txt<br><br>VIDEO_NAME=&quot;gbikes_dinosaur.mp4&quot;<br>ANNOTATION_URI=&quot;gs://$ANNOTATION_BUCKET/$VIDEO_BUCKET/$VIDEO_NAME.json&quot;<br><br>python $PROJECT_SRC/gcf2_generate_summary/main.py $ANNOTATION_URI</strong></pre><pre>Downloading -&gt; /tmp/SUMMARY_BUCKET/VIDEO_BUCKET/VIDEO_NAME<br>Generating summary...<br>Uploading -&gt; VIDEO_BUCKET/VIDEO_NAME.summary004.jpeg</pre><blockquote>Note: The uploaded video summary shows 4Â shots.</blockquote><p>Clean up:</p><pre><strong>deactivate<br>rm -rf venv</strong></pre><h4>Function entryÂ point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/631923f995b0f8bbd4edda7b6c837359/href">https://medium.com/media/631923f995b0f8bbd4edda7b6c837359/href</a></iframe><blockquote>Note: This function will be called whenever an annotation file is uploaded to the bucket defined as aÂ trigger.</blockquote><h4>Function deployment</h4><pre><strong>GCF_NAME=&quot;gcf2_generate_summary&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf2_generate_summary&quot;<br>GCF_ENTRY_POINT=&quot;gcf_generate_summary&quot;<br>GCF_TRIGGER_BUCKET=&quot;$ANNOTATION_BUCKET&quot;<br>GCF_ENV_VARS=&quot;SUMMARY_BUCKET=$SUMMARY_BUCKET&quot;<br>GCF_TIMEOUT=&quot;540s&quot;<br>GCF_MEMORY=&quot;512MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS \<br>  --trigger-bucket $GCF_TRIGGER_BUCKET \<br>  --region $GCF_REGION \<br>  --timeout $GCF_TIMEOUT \<br>  --memory $GCF_MEMORY \<br>  --quiet</strong></pre><p>Notes:</p><ul><li>The default timeout for a Cloud Function is 60 seconds. As youâ€™re deploying a background function with potentially long processings, set it to the maximum value (540 seconds = 9 minutes).</li><li>You also need to bump up the memory a little for the video and image processings. Depending on the size of your videos and the maximum resolution of your output summaries, or if you need to generate the summary faster (memory size and vCPU speed are correlated), you might use a higher value (1024MB orÂ 2048MB).</li></ul><pre>Deploying function (may take a while - up to 2 minutes)...done.<br>availableMemoryMb: 512<br>entryPoint: gcf_generate_summary<br>environmentVariables:<br>  SUMMARY_BUCKET: b3-summaries...<br>...<br>status: ACTIVE<br>timeout: 540s<br>updateTime: &#39;YYYY-MM-DDThh:mm:ss.mmmZ&#39;<br>versionId: &#39;1&#39;</pre><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">CloudÂ Console</a>:</p><figure><img alt="Cloud Functions 2" src="https://cdn-images-1.medium.com/max/1024/0*Ci683g0AQEorLvDE.png" /></figure><h4>Production tests</h4><p>Make sure to test the function in production. You can upload an annotation file in the 2ndÂ bucket:</p><pre><strong>VIDEO_NAME=&quot;gbikes_dinosaur.mp4&quot;<br>ANNOTATION_FILE=&quot;$VIDEO_NAME.json&quot;<br>ANNOTATION_URI=&quot;gs://$ANNOTATION_BUCKET/$VIDEO_BUCKET/$ANNOTATION_FILE&quot;<br>gsutil cp $ANNOTATION_URI .<br>gsutil cp $ANNOTATION_FILE $ANNOTATION_URI<br>rm $ANNOTATION_FILE</strong></pre><blockquote>Note: This reuses the previous local test annotation file and overwrites it. Overwriting a file in a bucket also triggers attached functions.</blockquote><p>Wait a few seconds and query the logs to check that the function has been triggered:</p><pre><strong>gcloud functions logs read --region $GCF_REGION</strong></pre><pre>LEVEL  NAME                   EXECUTION_ID  TIME_UTC  LOG<br>...<br>D      gcf2_generate_summary  ...           ...       Function execution started<br>I      gcf2_generate_summary  ...           ...       Downloading -&gt; /tmp/SUMMARY_BUCKET/VIDEO_BUCKET/VIDEO_NAME<br>I      gcf2_generate_summary  ...           ...       Generating summary...<br>I      gcf2_generate_summary  ...           ...       Uploading -&gt; VIDEO_BUCKET/VIDEO_NAME.summary004.jpeg<br>D      gcf2_generate_summary  ...           ...       Function execution took 11591 ms, finished with status: &#39;ok&#39;</pre><p>The 2nd function is operational and the pipeline is in place! You can now do end-to-end tests by copying new videos in the 1stÂ bucket.</p><h4>Results</h4><p>Download the generated summary on your computer:</p><pre><strong>cd ~/$PROJECT_ID<br>gsutil cp -r gs://$SUMMARY_BUCKET/**.jpeg .<br>cloudshell download *.jpeg</strong></pre><p>Here is the visual summary for gbikes_dinosaur.mp4 (4 detectedÂ shots):</p><figure><img alt="Visual summary for gbikes_dinosaur.mp4" src="https://cdn-images-1.medium.com/max/1024/0*5kElus_pB3EIoDxU.jpeg" /></figure><p>You can also directly preview the file from the <a href="https://console.cloud.google.com/storage/browser/">CloudÂ Console</a>:</p><figure><img alt="Video summary" src="https://cdn-images-1.medium.com/max/1024/0*3iMF9kMT7x7xgtWq.png" /></figure><h3>ğŸ’ Cherry on the PyÂ ğŸ</h3><p>Now, the icing on the cake (or the â€œcherry on the pieâ€ as we say inÂ French)â€¦</p><p>Based on the same architecture and code, you can add a few features:</p><ul><li>Trigger the processing for videos from otherÂ buckets</li><li>Generate summaries in multiple formats (such as JPEG, PNG,Â WEBP)</li><li>Generate animated summaries (also in multiple formats, such as GIF, PNG,Â WEBP)</li></ul><p>Enrich the architecture to duplicate 2Â items:</p><ul><li>The video shot detection function, to get it to run as an HTTPÂ endpoint</li><li>The summary generation function to handle animatedÂ images</li></ul><p>Adapt the code to support the new features:</p><ul><li>An animated parameter to generate still or animated summaries</li><li>Save and upload the results in multipleÂ formats</li></ul><h4>Architecture (v2)</h4><figure><img alt="Architecture (v2)" src="https://cdn-images-1.medium.com/max/1024/0*VQdimtG1UCuy4KkK.png" /></figure><ul><li>A. Video shot detection can also be triggered manually with an HTTP GETÂ request</li><li>B. Still and animated summaries are generated in 2 functions inÂ parallel</li><li>C. Summaries are uploaded in multiple imageÂ formats</li></ul><h4>HTTP entryÂ point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e99021dde44910cc73b1f9421ecd53de/href">https://medium.com/media/e99021dde44910cc73b1f9421ecd53de/href</a></iframe><blockquote>Note: This is the same code as gcf_detect_shots with the video URI parameter provided from a GETÂ request.</blockquote><h4>Function deployment</h4><pre>GCF_NAME=&quot;gcf1_detect_shots_http&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf1_detect_shots&quot;<br>GCF_ENTRY_POINT=&quot;gcf_detect_shots_http&quot;<br>GCF_TRIGGER_BUCKET=&quot;$VIDEO_BUCKET&quot;<br>GCF_ENV_VARS=&quot;ANNOTATION_BUCKET=$ANNOTATION_BUCKET&quot;<br>GCF_MEMORY=&quot;128MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS \<br>  --trigger-http \<br>  --region $GCF_REGION \<br>  --memory $GCF_MEMORY \<br>  --quiet</pre><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">CloudÂ Console</a>:</p><figure><img alt="Cloud Functions 3" src="https://cdn-images-1.medium.com/max/1024/0*4fPdiARXnY-_41-C.png" /></figure><h4>Animation support</h4><p>Add an animated option in the core function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d305f1c3c310735bc0198d0dd7f0e8ad/href">https://medium.com/media/d305f1c3c310735bc0198d0dd7f0e8ad/href</a></iframe><p>Define the formats youâ€™re interested in generating:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/246476ead5b3109bdfac7fb155a97b35/href">https://medium.com/media/246476ead5b3109bdfac7fb155a97b35/href</a></iframe><p>Add support to generate still and animated summaries in different formats:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a879c7b4ec0215fe7b142d8e4aac464f/href">https://medium.com/media/a879c7b4ec0215fe7b142d8e4aac464f/href</a></iframe><p>The serialization can still take place in a single function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c65fdf651e5db931dc3150e962575c6d/href">https://medium.com/media/c65fdf651e5db931dc3150e962575c6d/href</a></iframe><blockquote>Note: Pillow is both versatile and consistent, allowing for significant and clean code factorization.</blockquote><p>Add an animated optional parameter to the StorageHelper class:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c9ccce745ad8453ff30ff698dd9ea8f5/href">https://medium.com/media/c9ccce745ad8453ff30ff698dd9ea8f5/href</a></iframe><p>And finally, add an ANIMATED optional environment variable in the entryÂ point:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a81644f5baa8058317a7708f2b9c5f29/href">https://medium.com/media/a81644f5baa8058317a7708f2b9c5f29/href</a></iframe><h4>Function deployment</h4><p>Duplicate the 2nd function with the additional ANIMATED environment variable:</p><pre><strong>GCF_NAME=&quot;gcf2_generate_summary_animated&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf2_generate_summary&quot;<br>GCF_ENTRY_POINT=&quot;gcf_generate_summary&quot;<br>GCF_TRIGGER_BUCKET=&quot;$ANNOTATION_BUCKET&quot;<br>GCF_ENV_VARS1=&quot;SUMMARY_BUCKET=$SUMMARY_BUCKET&quot;<br>GCF_ENV_VARS2=&quot;ANIMATED=1&quot;<br>GCF_TIMEOUT=&quot;540s&quot;<br>GCF_MEMORY=&quot;2048MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS1 \<br>  --update-env-vars $GCF_ENV_VARS2 \<br>  --trigger-bucket $GCF_TRIGGER_BUCKET \<br>  --region $GCF_REGION \<br>  --timeout $GCF_TIMEOUT \<br>  --memory $GCF_MEMORY \<br>  --quiet</strong></pre><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">CloudÂ Console</a>:</p><figure><img alt="Cloud Functions 4" src="https://cdn-images-1.medium.com/max/1024/0*JgdtT1hHxExMKhg1.png" /></figure><h3>ğŸ‰ FinalÂ tests</h3><p>The HTTP endpoint lets you trigger the pipeline with a GETÂ request:</p><pre><strong>GCF_NAME=&quot;gcf1_detect_shots_http&quot;<br>VIDEO_URI=&quot;gs://cloud-samples-data/video</strong><strong>/visionapi.mp4&quot;<br>GCF_URL=&quot;https://$GCF_REGION-$PROJECT_ID.cloudfunctions.net/$GCF_NAME?video_uri=$VIDEO_URI&quot;<br><br>curl $GCF_URL -H &quot;Authorization: bearer $(gcloud auth print-identity-token)&quot;</strong></pre><pre>Launched shot detection for video_uri &lt;VIDEO_URI&gt;</pre><blockquote>Note: The test video &lt;visionapi.mp4&gt; is located in an external bucket but is publicly accessible.</blockquote><p>In addition, copy one or several videos into the video bucket. You can drag and dropÂ videos:</p><figure><img alt="Dragging files to a bucket" src="https://cdn-images-1.medium.com/max/1024/0*2piYMwPyLnx4sP61.gif" /></figure><p>The videos are then processed in parallel. Here are a fewÂ logs:</p><pre>LEVEL NAME                           EXECUTION_ID ... LOG<br>...<br>D     gcf2_generate_summary_animated f6n6tslsfwdu ... Function execution took 49293 ms, finished with status: &#39;ok&#39;<br>I     gcf2_generate_summary          yd1vqabafn17 ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_still.png<br>I     gcf2_generate_summary_animated qv9b03814jjk ... shot_ratio: 43%<br>I     gcf2_generate_summary          yd1vqabafn17 ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_still.webp<br>D     gcf2_generate_summary          yd1vqabafn17 ... Function execution took 54616 ms, finished with status: &#39;ok&#39;<br>I     gcf2_generate_summary_animated g4d2wrzxz2st ... shot_ratio: 71%<br>...<br>D     gcf2_generate_summary          amwmov1wk0gn ... Function execution took 65256 ms, finished with status: &#39;ok&#39;<br>I     gcf2_generate_summary_animated 7pp882fz0x84 ... shot_ratio: 57%<br>I     gcf2_generate_summary_animated i3u830hsjz4r ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_anim.png<br>I     gcf2_generate_summary_animated i3u830hsjz4r ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_anim.webp<br>D     gcf2_generate_summary_animated i3u830hsjz4r ... Function execution took 70862 ms, finished with status: &#39;ok&#39;<br>...</pre><p>In the 3rd bucket, youâ€™ll find all still and animated summaries:</p><figure><img alt="Video summary" src="https://cdn-images-1.medium.com/max/1024/0*NTDWw5uXmNWsWr6n.png" /></figure><p>Youâ€™ve already seen the still summary for &lt;JaneGoodall.mp4&gt; as an introduction to this tutorial. In the animated version, and in only 6 frames, you get an even better idea of what the <a href="https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4">whole video</a> isÂ about:</p><figure><img alt="Video summary" src="https://cdn-images-1.medium.com/max/1024/0*O18noPhqhyceU85Q.gif" /></figure><p>If you donâ€™t want to keep your project, you can deleteÂ it:</p><pre><strong>gcloud projects delete $PROJECT_ID</strong></pre><h3>â• One moreÂ thing</h3><pre><strong>first_line_after_licence=16<br>find $PROJECT_SRC -name &#39;*.py&#39; -exec tail -n +$first_line_after_licence {} \; | grep -v &quot;^$&quot; | wc -l</strong></pre><pre>289</pre><p>You did everything in under 300 lines of Python. Less lines, lessÂ bugs!</p><p><strong>ğŸ”¥ğŸ Mission accomplished! ğŸğŸ”¥</strong></p><h3>ğŸ–– SeeÂ you</h3><p>I hope you appreciated this tutorial and would love to read <a href="https://bit.ly/feedback-video-summary">your feedback</a>. You can also <a href="https://twitter.com/PicardParis">follow me onÂ Twitter</a>.</p><h3>â³ Updates</h3><ul><li><strong>2021â€“10â€“08</strong>: Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_video_summary">GitHub version</a> with latest library versions + Python 3.7 â†’Â 3.9</li></ul><h3>ğŸ“œ Also in thisÂ series</h3><ol><li>Summarizing videos</li><li><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=friends_link&amp;sk=c9602c33c77aa950a59282b6de5c0c57">Tracking videoÂ objects</a></li><li><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=friends_link&amp;sk=cc252ab86eab9ed2e8583963d0598661">Face detection and processing</a></li><li><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=friends_link&amp;sk=a3d6e22e7e77828e411592f46025531e">Processing images</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c2f261c8035c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c">Summarizing videos in 300 lines of code</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>